Starting parallel training at Mon May 12 01:33:27 UTC 2025
Command:
python parallel_train.py       --model_name meta-llama/Llama-2-7b-hf       --layer_indices  0 3 6 9 12 15 18 21       --weighter_types  token positional surprise       --weighting_modes  full query_only none       --data_path ./datasets/msmarco       --batch_size 16       --gradient_accumulation_steps 16       --learning_rate 1e-5       --weight_decay 0.01       --max_steps 500       --save_steps 5       --evaluation_steps 2       --max_length 512       --output_dir ./model_checkpoints/msmarco_rerankers       --token_weights_filepath token_frequency_data/llama2_token_freq_weights.pkl       --token_weight_type log_weights       --margin 0.05       --lambda_factor 0.1       --weight_normalization linear
Initializing parallel training script...
2025-05-12 01:33:30 - Arguments: Namespace(model_name='meta-llama/Llama-2-7b-hf', layer_indices=[0, 3, 6, 9, 12, 15, 18, 21], weighter_types=['token', 'positional', 'surprise'], weighting_modes=['full', 'query_only', 'none'], weight_normalization='linear', data_path='./datasets/msmarco', batch_size=16, gradient_accumulation_steps=16, learning_rate=1e-05, weight_decay=0.01, max_steps=500, save_steps=5, evaluation_steps=2, ce_score_margin=3.0, num_negs_per_system=5, token_weights_filepath='token_frequency_data/llama2_token_freq_weights.pkl', token_weight_type='log_weights', token_fallback_weight=21.5481, margin=0.05, lambda_factor=0.1, max_length=512, output_dir='./model_checkpoints/msmarco_rerankers')
2025-05-12 01:33:30 - Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid memory fragmentation
2025-05-12 01:33:30 - Starting parallel training for all weighter configurations...
2025-05-12 01:33:30 - Using weight_normalization=linear
2025-05-12 01:33:30 - Loading MSMARCO data from ./datasets/msmarco
2025-05-12 01:33:30 - Loading Corpus...
  0%|          | 0/8841823 [00:00<?, ?it/s]  0%|          | 18711/8841823 [00:00<00:47, 187092.78it/s]  0%|          | 39254/8841823 [00:00<00:44, 197864.93it/s]  1%|          | 59539/8841823 [00:00<00:43, 200135.72it/s]  1%|          | 80256/8841823 [00:00<00:43, 202909.46it/s]  1%|          | 100547/8841823 [00:00<00:43, 201457.47it/s]  1%|▏         | 121034/8841823 [00:00<00:43, 202609.82it/s]  2%|▏         | 141361/8841823 [00:00<00:42, 202758.70it/s]  2%|▏         | 161639/8841823 [00:00<00:42, 202602.30it/s]  2%|▏         | 181901/8841823 [00:00<00:43, 197154.21it/s]  2%|▏         | 202296/8841823 [00:01<00:43, 199206.85it/s]  3%|▎         | 222506/8841823 [00:01<00:43, 200075.64it/s]  3%|▎         | 242533/8841823 [00:01<00:43, 199891.22it/s]  3%|▎         | 262535/8841823 [00:01<00:42, 199792.25it/s]  3%|▎         | 282889/8841823 [00:01<00:42, 200915.45it/s]  3%|▎         | 303366/8841823 [00:01<00:42, 202069.96it/s]  4%|▎         | 323850/8841823 [00:01<00:41, 202898.33it/s]  4%|▍         | 344411/8841823 [00:01<00:41, 203707.89it/s]  4%|▍         | 364785/8841823 [00:01<00:43, 193777.18it/s]  4%|▍         | 385447/8841823 [00:01<00:42, 197492.64it/s]  5%|▍         | 406120/8841823 [00:02<00:42, 200192.52it/s]  5%|▍         | 426674/8841823 [00:02<00:41, 201765.18it/s]  5%|▌         | 446900/8841823 [00:02<00:41, 201465.19it/s]  5%|▌         | 467119/8841823 [00:02<00:41, 201677.64it/s]  6%|▌         | 487461/8841823 [00:02<00:41, 202192.68it/s]  6%|▌         | 507698/8841823 [00:02<00:41, 201845.46it/s]  6%|▌         | 528114/8841823 [00:02<00:41, 202531.73it/s]  6%|▌         | 548514/8841823 [00:02<00:40, 202968.88it/s]  6%|▋         | 568944/8841823 [00:02<00:40, 203362.95it/s]  7%|▋         | 589398/8841823 [00:02<00:40, 203713.05it/s]  7%|▋         | 609891/8841823 [00:03<00:40, 204075.42it/s]  7%|▋         | 630342/8841823 [00:03<00:40, 204200.78it/s]  7%|▋         | 650787/8841823 [00:03<00:40, 204270.30it/s]  8%|▊         | 671222/8841823 [00:03<00:39, 204289.44it/s]  8%|▊         | 691652/8841823 [00:03<00:40, 202676.25it/s]  8%|▊         | 711924/8841823 [00:03<00:44, 184087.52it/s]  8%|▊         | 732511/8841823 [00:03<00:42, 190161.39it/s]  9%|▊         | 752948/8841823 [00:03<00:41, 194207.99it/s]  9%|▊         | 773329/8841823 [00:03<00:40, 196981.85it/s]  9%|▉         | 793677/8841823 [00:03<00:40, 198878.94it/s]  9%|▉         | 813920/8841823 [00:04<00:40, 199921.03it/s]  9%|▉         | 834188/8841823 [00:04<00:39, 200735.86it/s] 10%|▉         | 854570/8841823 [00:04<00:39, 201649.01it/s] 10%|▉         | 874950/8841823 [00:04<00:39, 202284.90it/s] 10%|█         | 895369/8841823 [00:04<00:39, 202851.25it/s] 10%|█         | 915779/8841823 [00:04<00:39, 203221.74it/s] 11%|█         | 936149/8841823 [00:04<00:38, 203359.60it/s] 11%|█         | 956516/8841823 [00:04<00:38, 203446.53it/s] 11%|█         | 976897/8841823 [00:04<00:38, 203551.44it/s] 11%|█▏        | 997319/8841823 [00:04<00:38, 203749.00it/s] 12%|█▏        | 1017698/8841823 [00:05<00:38, 203511.56it/s] 12%|█▏        | 1038113/8841823 [00:05<00:38, 203698.83it/s] 12%|█▏        | 1058485/8841823 [00:05<00:38, 203606.59it/s] 12%|█▏        | 1078847/8841823 [00:05<00:38, 203508.00it/s] 12%|█▏        | 1099297/8841823 [00:05<00:37, 203801.13it/s] 13%|█▎        | 1119678/8841823 [00:05<00:37, 203628.62it/s] 13%|█▎        | 1140042/8841823 [00:05<00:37, 203507.19it/s] 13%|█▎        | 1160394/8841823 [00:05<00:37, 203356.95it/s] 13%|█▎        | 1180730/8841823 [00:05<00:37, 203131.87it/s] 14%|█▎        | 1201044/8841823 [00:05<00:37, 203131.32it/s] 14%|█▍        | 1221368/8841823 [00:06<00:37, 203159.73it/s] 14%|█▍        | 1241685/8841823 [00:06<00:37, 203035.76it/s] 14%|█▍        | 1262065/8841823 [00:06<00:37, 203260.49it/s] 15%|█▍        | 1282392/8841823 [00:06<00:37, 203235.22it/s] 15%|█▍        | 1302716/8841823 [00:06<00:37, 202837.84it/s] 15%|█▍        | 1323000/8841823 [00:06<00:37, 202557.99it/s] 15%|█▌        | 1343256/8841823 [00:06<00:37, 202101.00it/s] 15%|█▌        | 1363467/8841823 [00:06<00:37, 201864.83it/s] 16%|█▌        | 1383654/8841823 [00:06<00:37, 201548.95it/s] 16%|█▌        | 1403810/8841823 [00:07<00:45, 164942.13it/s] 16%|█▌        | 1424110/8841823 [00:07<00:42, 174798.72it/s] 16%|█▋        | 1444286/8841823 [00:07<00:40, 182077.70it/s] 17%|█▋        | 1464435/8841823 [00:07<00:39, 187479.67it/s] 17%|█▋        | 1484747/8841823 [00:07<00:38, 191927.70it/s] 17%|█▋        | 1505104/8841823 [00:07<00:37, 195290.58it/s] 17%|█▋        | 1525431/8841823 [00:07<00:37, 197619.39it/s] 17%|█▋        | 1545782/8841823 [00:07<00:36, 199350.80it/s] 18%|█▊        | 1566098/8841823 [00:07<00:36, 200474.50it/s] 18%|█▊        | 1586483/8841823 [00:07<00:36, 201473.03it/s] 18%|█▊        | 1606863/8841823 [00:08<00:35, 202163.56it/s] 18%|█▊        | 1627234/8841823 [00:08<00:35, 202621.28it/s] 19%|█▊        | 1647591/8841823 [00:08<00:35, 202901.87it/s] 19%|█▉        | 1667914/8841823 [00:08<00:35, 202998.12it/s] 19%|█▉        | 1688280/8841823 [00:08<00:35, 203194.11it/s] 19%|█▉        | 1708645/8841823 [00:08<00:35, 203327.73it/s] 20%|█▉        | 1729045/8841823 [00:08<00:34, 203524.66it/s] 20%|█▉        | 1749433/8841823 [00:08<00:34, 203628.06it/s] 20%|██        | 1769801/8841823 [00:08<00:34, 203300.66it/s] 20%|██        | 1790135/8841823 [00:08<00:34, 203293.19it/s] 20%|██        | 1810467/8841823 [00:09<00:34, 203214.46it/s] 21%|██        | 1830791/8841823 [00:09<00:34, 203062.72it/s] 21%|██        | 1851099/8841823 [00:09<00:34, 203052.44it/s] 21%|██        | 1871406/8841823 [00:09<00:34, 202662.26it/s] 21%|██▏       | 1891746/8841823 [00:09<00:34, 202878.70it/s] 22%|██▏       | 1912071/8841823 [00:09<00:34, 202986.08it/s] 22%|██▏       | 1932370/8841823 [00:09<00:34, 202887.86it/s] 22%|██▏       | 1952759/8841823 [00:09<00:33, 203185.63it/s] 22%|██▏       | 1973151/8841823 [00:09<00:33, 203403.94it/s] 23%|██▎       | 1993508/8841823 [00:09<00:33, 203450.30it/s] 23%|██▎       | 2013854/8841823 [00:10<00:33, 203389.28it/s] 23%|██▎       | 2034194/8841823 [00:10<00:33, 203376.13it/s] 23%|██▎       | 2054542/8841823 [00:10<00:33, 203403.26it/s] 23%|██▎       | 2074883/8841823 [00:10<00:33, 203307.36it/s] 24%|██▎       | 2095214/8841823 [00:10<00:33, 203117.56it/s] 24%|██▍       | 2115558/8841823 [00:10<00:33, 203211.05it/s] 24%|██▍       | 2135974/8841823 [00:10<00:32, 203492.97it/s] 24%|██▍       | 2156352/8841823 [00:10<00:32, 203575.16it/s] 25%|██▍       | 2176714/8841823 [00:10<00:32, 203582.55it/s] 25%|██▍       | 2197092/8841823 [00:10<00:32, 203639.35it/s] 25%|██▌       | 2217501/8841823 [00:11<00:32, 203771.23it/s] 25%|██▌       | 2237879/8841823 [00:11<00:32, 203617.38it/s] 26%|██▌       | 2258310/8841823 [00:11<00:32, 203820.86it/s] 26%|██▌       | 2278693/8841823 [00:11<00:32, 203803.41it/s] 26%|██▌       | 2299174/8841823 [00:11<00:32, 204100.51it/s] 26%|██▌       | 2319616/8841823 [00:11<00:31, 204192.74it/s] 26%|██▋       | 2340036/8841823 [00:11<00:31, 203741.86it/s] 27%|██▋       | 2360411/8841823 [00:11<00:31, 203417.69it/s] 27%|██▋       | 2380754/8841823 [00:11<00:31, 203270.72it/s] 27%|██▋       | 2401092/8841823 [00:11<00:31, 203299.87it/s] 27%|██▋       | 2421423/8841823 [00:12<00:31, 203124.67it/s] 28%|██▊       | 2441736/8841823 [00:12<00:31, 203010.36it/s] 28%|██▊       | 2462038/8841823 [00:12<00:31, 202476.42it/s] 28%|██▊       | 2482286/8841823 [00:12<00:31, 202339.03it/s] 28%|██▊       | 2502521/8841823 [00:12<00:31, 202170.44it/s] 29%|██▊       | 2522739/8841823 [00:12<00:31, 201865.11it/s] 29%|██▉       | 2542947/8841823 [00:12<00:31, 201927.58it/s] 29%|██▉       | 2563171/8841823 [00:12<00:31, 202018.45it/s] 29%|██▉       | 2583420/8841823 [00:12<00:30, 202157.61it/s] 29%|██▉       | 2603668/8841823 [00:12<00:30, 202249.38it/s] 30%|██▉       | 2623979/8841823 [00:13<00:30, 202504.12it/s] 30%|██▉       | 2644243/8841823 [00:13<00:30, 202541.16it/s] 30%|███       | 2664498/8841823 [00:13<00:30, 202450.07it/s] 30%|███       | 2684744/8841823 [00:13<00:30, 202411.58it/s] 31%|███       | 2704986/8841823 [00:13<00:30, 202386.67it/s] 31%|███       | 2725225/8841823 [00:13<00:30, 202053.61it/s] 31%|███       | 2745431/8841823 [00:13<00:30, 202020.96it/s] 31%|███▏      | 2765660/8841823 [00:13<00:30, 202097.73it/s] 32%|███▏      | 2785912/8841823 [00:13<00:29, 202220.46it/s] 32%|███▏      | 2806135/8841823 [00:14<00:43, 137560.71it/s] 32%|███▏      | 2826308/8841823 [00:14<00:39, 152036.96it/s] 32%|███▏      | 2846542/8841823 [00:14<00:36, 164295.78it/s] 32%|███▏      | 2866737/8841823 [00:14<00:34, 174018.71it/s] 33%|███▎      | 2887000/8841823 [00:14<00:32, 181728.62it/s] 33%|███▎      | 2907350/8841823 [00:14<00:31, 187777.80it/s] 33%|███▎      | 2927560/8841823 [00:14<00:30, 191845.59it/s] 33%|███▎      | 2947951/8841823 [00:14<00:30, 195325.69it/s] 34%|███▎      | 2968371/8841823 [00:14<00:29, 197914.27it/s] 34%|███▍      | 2988830/8841823 [00:15<00:29, 199876.54it/s] 34%|███▍      | 3009235/8841823 [00:15<00:29, 201110.30it/s] 34%|███▍      | 3029578/8841823 [00:15<00:28, 201796.82it/s] 34%|███▍      | 3049995/8841823 [00:15<00:28, 202502.47it/s] 35%|███▍      | 3070335/8841823 [00:15<00:28, 202766.37it/s] 35%|███▍      | 3090669/8841823 [00:15<00:28, 202790.06it/s] 35%|███▌      | 3111052/8841823 [00:15<00:28, 203099.35it/s] 35%|███▌      | 3131459/8841823 [00:15<00:28, 203386.41it/s] 36%|███▌      | 3151900/8841823 [00:15<00:27, 203690.94it/s] 36%|███▌      | 3172353/8841823 [00:15<00:27, 203938.83it/s] 36%|███▌      | 3192757/8841823 [00:16<00:27, 203763.72it/s] 36%|███▋      | 3213141/8841823 [00:16<00:27, 203226.79it/s] 37%|███▋      | 3233469/8841823 [00:16<00:27, 203190.01it/s] 37%|███▋      | 3253792/8841823 [00:16<00:27, 201990.73it/s] 37%|███▋      | 3274149/8841823 [00:16<00:27, 202456.56it/s] 37%|███▋      | 3294578/8841823 [00:16<00:27, 203000.99it/s] 37%|███▋      | 3315055/8841823 [00:16<00:27, 203527.94it/s] 38%|███▊      | 3335543/8841823 [00:16<00:27, 203928.45it/s] 38%|███▊      | 3355993/8841823 [00:16<00:26, 204095.65it/s] 38%|███▊      | 3376425/8841823 [00:16<00:26, 204160.03it/s] 38%|███▊      | 3396861/8841823 [00:17<00:26, 204217.12it/s] 39%|███▊      | 3417284/8841823 [00:17<00:26, 204189.95it/s] 39%|███▉      | 3437711/8841823 [00:17<00:26, 204210.65it/s] 39%|███▉      | 3458192/8841823 [00:17<00:26, 204388.26it/s] 39%|███▉      | 3478631/8841823 [00:17<00:26, 204179.62it/s] 40%|███▉      | 3499050/8841823 [00:17<00:26, 204095.13it/s] 40%|███▉      | 3519460/8841823 [00:17<00:26, 203915.52it/s] 40%|████      | 3539859/8841823 [00:17<00:25, 203935.86it/s] 40%|████      | 3560253/8841823 [00:17<00:25, 203860.96it/s] 40%|████      | 3580640/8841823 [00:17<00:25, 203700.11it/s] 41%|████      | 3601011/8841823 [00:18<00:25, 203584.31it/s] 41%|████      | 3621370/8841823 [00:18<00:25, 203446.26it/s] 41%|████      | 3641715/8841823 [00:18<00:25, 203214.61it/s] 41%|████▏     | 3662037/8841823 [00:18<00:25, 199582.86it/s] 42%|████▏     | 3682010/8841823 [00:18<00:25, 199041.39it/s] 42%|████▏     | 3702246/8841823 [00:18<00:25, 200021.67it/s] 42%|████▏     | 3722578/8841823 [00:18<00:25, 200998.49it/s] 42%|████▏     | 3742946/8841823 [00:18<00:25, 201796.60it/s] 43%|████▎     | 3763221/8841823 [00:18<00:25, 202077.92it/s] 43%|████▎     | 3783433/8841823 [00:18<00:25, 201970.14it/s] 43%|████▎     | 3803633/8841823 [00:19<00:24, 201817.71it/s] 43%|████▎     | 3823928/8841823 [00:19<00:24, 202153.22it/s] 43%|████▎     | 3844161/8841823 [00:19<00:24, 202202.19it/s] 44%|████▎     | 3864408/8841823 [00:19<00:24, 202279.61it/s] 44%|████▍     | 3884642/8841823 [00:19<00:24, 202294.30it/s] 44%|████▍     | 3904872/8841823 [00:19<00:24, 202215.12it/s] 44%|████▍     | 3925147/8841823 [00:19<00:24, 202370.47it/s] 45%|████▍     | 3945387/8841823 [00:19<00:24, 202376.78it/s] 45%|████▍     | 3965625/8841823 [00:19<00:24, 202344.09it/s] 45%|████▌     | 3985875/8841823 [00:19<00:23, 202385.84it/s] 45%|████▌     | 4006114/8841823 [00:20<00:23, 202337.37it/s] 46%|████▌     | 4026431/8841823 [00:20<00:23, 202584.52it/s] 46%|████▌     | 4046690/8841823 [00:20<00:23, 202342.42it/s] 46%|████▌     | 4066925/8841823 [00:20<00:23, 202323.79it/s] 46%|████▌     | 4087158/8841823 [00:20<00:23, 201747.81it/s] 46%|████▋     | 4107381/8841823 [00:20<00:23, 201889.42it/s] 47%|████▋     | 4127694/8841823 [00:20<00:23, 202258.96it/s] 47%|████▋     | 4147960/8841823 [00:20<00:23, 202376.86it/s] 47%|████▋     | 4168215/8841823 [00:20<00:23, 202424.04it/s] 47%|████▋     | 4188493/8841823 [00:20<00:22, 202527.49it/s] 48%|████▊     | 4208822/8841823 [00:21<00:22, 202754.02it/s] 48%|████▊     | 4229122/8841823 [00:21<00:22, 202819.73it/s] 48%|████▊     | 4249413/8841823 [00:21<00:22, 202844.10it/s] 48%|████▊     | 4269698/8841823 [00:21<00:22, 202828.09it/s] 49%|████▊     | 4289981/8841823 [00:21<00:22, 202672.92it/s] 49%|████▊     | 4310249/8841823 [00:21<00:22, 202649.89it/s] 49%|████▉     | 4330544/8841823 [00:21<00:22, 202736.46it/s] 49%|████▉     | 4350835/8841823 [00:21<00:22, 202785.80it/s] 49%|████▉     | 4371114/8841823 [00:21<00:22, 202728.26it/s] 50%|████▉     | 4391387/8841823 [00:21<00:21, 202557.23it/s] 50%|████▉     | 4411645/8841823 [00:22<00:21, 202561.51it/s] 50%|█████     | 4431902/8841823 [00:22<00:21, 202506.35it/s] 50%|█████     | 4452153/8841823 [00:22<00:21, 202338.29it/s] 51%|█████     | 4472387/8841823 [00:22<00:21, 202226.44it/s] 51%|█████     | 4492615/8841823 [00:22<00:21, 202240.24it/s] 51%|█████     | 4512848/8841823 [00:22<00:21, 202263.88it/s] 51%|█████▏    | 4533075/8841823 [00:22<00:21, 202090.25it/s] 51%|█████▏    | 4553285/8841823 [00:22<00:21, 202059.30it/s] 52%|█████▏    | 4573491/8841823 [00:22<00:21, 201881.67it/s] 52%|█████▏    | 4593684/8841823 [00:22<00:21, 201891.88it/s] 52%|█████▏    | 4613874/8841823 [00:23<00:20, 201562.35it/s] 52%|█████▏    | 4634066/8841823 [00:23<00:20, 201666.49it/s] 53%|█████▎    | 4654277/8841823 [00:23<00:20, 201795.98it/s] 53%|█████▎    | 4674457/8841823 [00:23<00:20, 201703.98it/s] 53%|█████▎    | 4694628/8841823 [00:23<00:20, 201409.07it/s] 53%|█████▎    | 4714864/8841823 [00:23<00:20, 201689.89it/s] 54%|█████▎    | 4735096/8841823 [00:23<00:20, 201874.72it/s] 54%|█████▍    | 4755314/8841823 [00:23<00:20, 201961.81it/s] 54%|█████▍    | 4775511/8841823 [00:23<00:20, 201730.43it/s] 54%|█████▍    | 4795752/8841823 [00:23<00:20, 201930.21it/s] 54%|█████▍    | 4815946/8841823 [00:24<00:19, 201842.76it/s] 55%|█████▍    | 4836159/8841823 [00:24<00:19, 201924.15it/s] 55%|█████▍    | 4856359/8841823 [00:24<00:19, 201941.75it/s] 55%|█████▌    | 4876554/8841823 [00:24<00:19, 201913.82it/s] 55%|█████▌    | 4896760/8841823 [00:24<00:19, 201953.67it/s] 56%|█████▌    | 4916984/8841823 [00:24<00:19, 202037.00it/s] 56%|█████▌    | 4937219/8841823 [00:24<00:19, 202126.74it/s] 56%|█████▌    | 4957432/8841823 [00:24<00:19, 201946.25it/s] 56%|█████▋    | 4977627/8841823 [00:24<00:19, 201940.65it/s] 57%|█████▋    | 4997822/8841823 [00:24<00:19, 201838.03it/s] 57%|█████▋    | 5018006/8841823 [00:25<00:18, 201801.61it/s] 57%|█████▋    | 5038209/8841823 [00:25<00:18, 201866.82it/s] 57%|█████▋    | 5058410/8841823 [00:25<00:18, 201906.78it/s] 57%|█████▋    | 5078601/8841823 [00:25<00:18, 201466.29it/s] 58%|█████▊    | 5098773/8841823 [00:25<00:18, 201538.15it/s] 58%|█████▊    | 5118927/8841823 [00:25<00:18, 200846.35it/s] 58%|█████▊    | 5139082/8841823 [00:25<00:18, 201054.66it/s] 58%|█████▊    | 5159237/8841823 [00:25<00:18, 201196.23it/s] 59%|█████▊    | 5179389/8841823 [00:25<00:18, 201289.81it/s] 59%|█████▉    | 5199519/8841823 [00:25<00:18, 201074.42it/s] 59%|█████▉    | 5219627/8841823 [00:26<00:18, 200821.59it/s] 59%|█████▉    | 5239735/8841823 [00:26<00:17, 200895.58it/s] 59%|█████▉    | 5259825/8841823 [00:26<00:17, 200715.32it/s] 60%|█████▉    | 5279897/8841823 [00:26<00:17, 200251.32it/s] 60%|█████▉    | 5299939/8841823 [00:26<00:17, 200298.70it/s] 60%|██████    | 5319970/8841823 [00:26<00:17, 200078.75it/s] 60%|██████    | 5340026/8841823 [00:26<00:17, 200215.16it/s] 61%|██████    | 5360131/8841823 [00:26<00:17, 200458.99it/s] 61%|██████    | 5380222/8841823 [00:26<00:17, 200591.62it/s] 61%|██████    | 5400305/8841823 [00:26<00:17, 200660.13it/s] 61%|██████▏   | 5420372/8841823 [00:27<00:17, 200645.07it/s] 62%|██████▏   | 5440460/8841823 [00:27<00:16, 200712.71it/s] 62%|██████▏   | 5460568/8841823 [00:27<00:16, 200820.91it/s] 62%|██████▏   | 5480651/8841823 [00:27<00:16, 200582.86it/s] 62%|██████▏   | 5500710/8841823 [00:27<00:16, 200531.18it/s] 62%|██████▏   | 5520764/8841823 [00:27<00:16, 200432.05it/s] 63%|██████▎   | 5540808/8841823 [00:27<00:16, 200323.98it/s] 63%|██████▎   | 5560841/8841823 [00:27<00:16, 200157.95it/s] 63%|██████▎   | 5580871/8841823 [00:27<00:16, 200196.56it/s] 63%|██████▎   | 5600891/8841823 [00:28<00:32, 99282.17it/s]  64%|██████▎   | 5620733/8841823 [00:28<00:27, 116644.46it/s] 64%|██████▍   | 5640760/8841823 [00:28<00:23, 133378.82it/s] 64%|██████▍   | 5660823/8841823 [00:28<00:21, 148328.31it/s] 64%|██████▍   | 5681005/8841823 [00:28<00:19, 161220.04it/s] 64%|██████▍   | 5701209/8841823 [00:28<00:18, 171673.03it/s] 65%|██████▍   | 5721466/8841823 [00:28<00:17, 179946.25it/s] 65%|██████▍   | 5741833/8841823 [00:28<00:16, 186509.61it/s] 65%|██████▌   | 5762035/8841823 [00:29<00:16, 190900.71it/s] 65%|██████▌   | 5782327/8841823 [00:29<00:15, 194359.93it/s] 66%|██████▌   | 5802674/8841823 [00:29<00:15, 196991.54it/s] 66%|██████▌   | 5822983/8841823 [00:29<00:15, 198782.43it/s] 66%|██████▌   | 5843321/8841823 [00:29<00:14, 200140.77it/s] 66%|██████▋   | 5863638/8841823 [00:29<00:14, 201038.66it/s] 67%|██████▋   | 5883960/8841823 [00:29<00:14, 201684.72it/s] 67%|██████▋   | 5904355/8841823 [00:29<00:14, 202354.72it/s] 67%|██████▋   | 5924667/8841823 [00:29<00:14, 202542.00it/s] 67%|██████▋   | 5945053/8841823 [00:29<00:14, 202932.97it/s] 67%|██████▋   | 5965427/8841823 [00:30<00:14, 203171.08it/s] 68%|██████▊   | 5985782/8841823 [00:30<00:14, 203282.49it/s] 68%|██████▊   | 6006129/8841823 [00:30<00:13, 203287.91it/s] 68%|██████▊   | 6026471/8841823 [00:30<00:13, 202752.15it/s] 68%|██████▊   | 6046765/8841823 [00:30<00:13, 202806.54it/s] 69%|██████▊   | 6067053/8841823 [00:30<00:13, 202530.28it/s] 69%|██████▉   | 6087411/8841823 [00:30<00:13, 202839.90it/s] 69%|██████▉   | 6107779/8841823 [00:30<00:13, 203087.81it/s] 69%|██████▉   | 6128202/8841823 [00:30<00:13, 203426.65it/s] 70%|██████▉   | 6148655/8841823 [00:30<00:13, 203754.09it/s] 70%|██████▉   | 6169032/8841823 [00:31<00:13, 200616.54it/s] 70%|██████▉   | 6189106/8841823 [00:31<00:13, 198622.06it/s] 70%|███████   | 6208979/8841823 [00:31<00:13, 197180.05it/s] 70%|███████   | 6228705/8841823 [00:31<00:13, 195784.41it/s] 71%|███████   | 6248290/8841823 [00:31<00:13, 195235.95it/s] 71%|███████   | 6267817/8841823 [00:31<00:13, 194355.21it/s] 71%|███████   | 6287255/8841823 [00:31<00:13, 193973.53it/s] 71%|███████▏  | 6306654/8841823 [00:31<00:13, 193345.40it/s] 72%|███████▏  | 6326259/8841823 [00:31<00:12, 194146.01it/s] 72%|███████▏  | 6346508/8841823 [00:31<00:12, 196629.43it/s] 72%|███████▏  | 6366716/8841823 [00:32<00:12, 198249.26it/s] 72%|███████▏  | 6387007/8841823 [00:32<00:12, 199640.36it/s] 72%|███████▏  | 6407330/8841823 [00:32<00:12, 200711.52it/s] 73%|███████▎  | 6427563/8841823 [00:32<00:11, 201194.03it/s] 73%|███████▎  | 6447843/8841823 [00:32<00:11, 201671.29it/s] 73%|███████▎  | 6468012/8841823 [00:32<00:11, 201444.59it/s] 73%|███████▎  | 6488313/8841823 [00:32<00:11, 201910.45it/s] 74%|███████▎  | 6508608/8841823 [00:32<00:11, 202216.87it/s] 74%|███████▍  | 6528897/8841823 [00:32<00:11, 202415.39it/s] 74%|███████▍  | 6549182/8841823 [00:32<00:11, 202541.57it/s] 74%|███████▍  | 6569469/8841823 [00:33<00:11, 202634.18it/s] 75%|███████▍  | 6589793/8841823 [00:33<00:11, 202812.16it/s] 75%|███████▍  | 6610173/8841823 [00:33<00:10, 203105.75it/s] 75%|███████▍  | 6630484/8841823 [00:33<00:10, 202740.99it/s] 75%|███████▌  | 6650759/8841823 [00:33<00:10, 201319.71it/s] 75%|███████▌  | 6670894/8841823 [00:33<00:10, 200048.75it/s] 76%|███████▌  | 6691126/8841823 [00:33<00:10, 200720.78it/s] 76%|███████▌  | 6711413/8841823 [00:33<00:10, 201359.28it/s] 76%|███████▌  | 6731762/8841823 [00:33<00:10, 201992.11it/s] 76%|███████▋  | 6752124/8841823 [00:34<00:10, 202476.19it/s] 77%|███████▋  | 6772423/8841823 [00:34<00:10, 202626.59it/s] 77%|███████▋  | 6792765/8841823 [00:34<00:10, 202860.99it/s] 77%|███████▋  | 6813080/8841823 [00:34<00:09, 202945.63it/s] 77%|███████▋  | 6833376/8841823 [00:34<00:09, 202761.60it/s] 78%|███████▊  | 6853733/8841823 [00:34<00:09, 202999.69it/s] 78%|███████▊  | 6874034/8841823 [00:34<00:09, 202900.03it/s] 78%|███████▊  | 6894337/8841823 [00:34<00:09, 202934.84it/s] 78%|███████▊  | 6914650/8841823 [00:34<00:09, 202991.81it/s] 78%|███████▊  | 6934966/8841823 [00:34<00:09, 203039.69it/s] 79%|███████▊  | 6955271/8841823 [00:35<00:09, 202990.88it/s] 79%|███████▉  | 6975571/8841823 [00:35<00:09, 202845.02it/s] 79%|███████▉  | 6995912/8841823 [00:35<00:09, 203010.23it/s] 79%|███████▉  | 7016284/8841823 [00:35<00:08, 203217.87it/s] 80%|███████▉  | 7036606/8841823 [00:35<00:08, 203146.38it/s] 80%|███████▉  | 7056954/8841823 [00:35<00:08, 203244.43it/s] 80%|████████  | 7077323/8841823 [00:35<00:08, 203375.64it/s] 80%|████████  | 7098112/8841823 [00:35<00:08, 204727.28it/s] 81%|████████  | 7118815/8841823 [00:35<00:08, 205413.90it/s] 81%|████████  | 7139534/8841823 [00:35<00:08, 205943.17it/s] 81%|████████  | 7160277/8841823 [00:36<00:08, 206384.65it/s] 81%|████████  | 7180935/8841823 [00:36<00:08, 206437.55it/s] 81%|████████▏ | 7201605/8841823 [00:36<00:07, 206513.24it/s] 82%|████████▏ | 7222359/8841823 [00:36<00:07, 206818.20it/s] 82%|████████▏ | 7243041/8841823 [00:36<00:07, 206709.25it/s] 82%|████████▏ | 7263712/8841823 [00:36<00:07, 206675.99it/s] 82%|████████▏ | 7284380/8841823 [00:36<00:07, 206348.70it/s] 83%|████████▎ | 7305015/8841823 [00:36<00:07, 206269.88it/s] 83%|████████▎ | 7325643/8841823 [00:36<00:07, 205591.21it/s] 83%|████████▎ | 7346203/8841823 [00:36<00:07, 204900.42it/s] 83%|████████▎ | 7366694/8841823 [00:37<00:07, 204379.64it/s] 84%|████████▎ | 7387133/8841823 [00:37<00:07, 204081.85it/s] 84%|████████▍ | 7407542/8841823 [00:37<00:07, 202799.94it/s] 84%|████████▍ | 7427936/8841823 [00:37<00:06, 203136.54it/s] 84%|████████▍ | 7448327/8841823 [00:37<00:06, 203365.52it/s] 84%|████████▍ | 7468808/8841823 [00:37<00:06, 203793.81it/s] 85%|████████▍ | 7489204/8841823 [00:37<00:06, 203836.32it/s] 85%|████████▍ | 7509654/8841823 [00:37<00:06, 204029.43it/s] 85%|████████▌ | 7530175/8841823 [00:37<00:06, 204380.42it/s] 85%|████████▌ | 7550654/8841823 [00:37<00:06, 204501.33it/s] 86%|████████▌ | 7571105/8841823 [00:38<00:06, 204444.66it/s] 86%|████████▌ | 7591550/8841823 [00:38<00:06, 204303.94it/s] 86%|████████▌ | 7611982/8841823 [00:38<00:06, 204306.91it/s] 86%|████████▋ | 7632501/8841823 [00:38<00:05, 204569.96it/s] 87%|████████▋ | 7652959/8841823 [00:38<00:05, 204387.75it/s] 87%|████████▋ | 7673451/8841823 [00:38<00:05, 204543.82it/s] 87%|████████▋ | 7693906/8841823 [00:38<00:05, 203938.47it/s] 87%|████████▋ | 7714370/8841823 [00:38<00:05, 204146.84it/s] 87%|████████▋ | 7734867/8841823 [00:38<00:05, 204387.13it/s] 88%|████████▊ | 7755406/8841823 [00:38<00:05, 204682.11it/s] 88%|████████▊ | 7775903/8841823 [00:39<00:05, 204765.38it/s] 88%|████████▊ | 7796393/8841823 [00:39<00:05, 204802.14it/s] 88%|████████▊ | 7816874/8841823 [00:39<00:05, 204520.02it/s] 89%|████████▊ | 7837335/8841823 [00:39<00:04, 204543.53it/s] 89%|████████▉ | 7857790/8841823 [00:39<00:04, 204445.60it/s] 89%|████████▉ | 7878235/8841823 [00:39<00:04, 204362.13it/s] 89%|████████▉ | 7898672/8841823 [00:39<00:04, 203812.01it/s] 90%|████████▉ | 7919160/8841823 [00:39<00:04, 204129.60it/s] 90%|████████▉ | 7939665/8841823 [00:39<00:04, 204401.34it/s] 90%|█████████ | 7960178/8841823 [00:39<00:04, 204583.18it/s] 90%|█████████ | 7980637/8841823 [00:40<00:04, 204522.15it/s] 90%|█████████ | 8001090/8841823 [00:40<00:04, 204232.97it/s] 91%|█████████ | 8021514/8841823 [00:40<00:04, 203126.25it/s] 91%|█████████ | 8042207/8841823 [00:40<00:03, 204259.01it/s] 91%|█████████ | 8062877/8841823 [00:40<00:03, 204984.14it/s] 91%|█████████▏| 8083559/8841823 [00:40<00:03, 205529.05it/s] 92%|█████████▏| 8104113/8841823 [00:40<00:03, 205349.41it/s] 92%|█████████▏| 8124722/8841823 [00:40<00:03, 205566.24it/s] 92%|█████████▏| 8145344/8841823 [00:40<00:03, 205758.06it/s] 92%|█████████▏| 8166005/8841823 [00:40<00:03, 206009.19it/s] 93%|█████████▎| 8186642/8841823 [00:41<00:03, 206114.18it/s] 93%|█████████▎| 8207303/8841823 [00:41<00:03, 206259.82it/s] 93%|█████████▎| 8227930/8841823 [00:41<00:02, 206219.73it/s] 93%|█████████▎| 8248553/8841823 [00:41<00:02, 205740.23it/s] 94%|█████████▎| 8269128/8841823 [00:41<00:02, 205077.23it/s] 94%|█████████▍| 8289637/8841823 [00:41<00:02, 204388.19it/s] 94%|█████████▍| 8310077/8841823 [00:41<00:02, 203568.50it/s] 94%|█████████▍| 8330435/8841823 [00:41<00:02, 203196.90it/s] 94%|█████████▍| 8350763/8841823 [00:41<00:02, 203218.71it/s] 95%|█████████▍| 8371212/8841823 [00:41<00:02, 203592.54it/s] 95%|█████████▍| 8391626/8841823 [00:42<00:02, 203751.73it/s] 95%|█████████▌| 8412044/8841823 [00:42<00:02, 203877.68it/s] 95%|█████████▌| 8432433/8841823 [00:42<00:02, 203869.35it/s] 96%|█████████▌| 8452862/8841823 [00:42<00:01, 203991.68it/s] 96%|█████████▌| 8473274/8841823 [00:42<00:01, 204028.46it/s] 96%|█████████▌| 8493710/8841823 [00:42<00:01, 204125.15it/s] 96%|█████████▋| 8514123/8841823 [00:42<00:01, 203686.56it/s] 97%|█████████▋| 8534536/8841823 [00:42<00:01, 203817.23it/s] 97%|█████████▋| 8554918/8841823 [00:42<00:01, 203617.68it/s] 97%|█████████▋| 8575329/8841823 [00:42<00:01, 203762.58it/s] 97%|█████████▋| 8595752/8841823 [00:43<00:01, 203900.24it/s] 97%|█████████▋| 8616163/8841823 [00:43<00:01, 203960.84it/s] 98%|█████████▊| 8636560/8841823 [00:43<00:01, 199000.71it/s] 98%|█████████▊| 8656871/8841823 [00:43<00:00, 200209.53it/s] 98%|█████████▊| 8677320/8841823 [00:43<00:00, 201475.45it/s] 98%|█████████▊| 8697750/8841823 [00:43<00:00, 202313.50it/s] 99%|█████████▊| 8718074/8841823 [00:43<00:00, 202584.77it/s] 99%|█████████▉| 8738544/8841823 [00:43<00:00, 203212.62it/s] 99%|█████████▉| 8759024/8841823 [00:43<00:00, 203685.56it/s] 99%|█████████▉| 8779570/8841823 [00:43<00:00, 204212.45it/s]100%|█████████▉| 8799995/8841823 [00:44<00:00, 204175.78it/s]100%|█████████▉| 8820457/8841823 [00:44<00:00, 204306.02it/s]100%|█████████▉| 8840890/8841823 [00:44<00:00, 204271.08it/s]100%|██████████| 8841823/8841823 [00:44<00:00, 199888.09it/s]
2025-05-12 01:34:42 - Loaded 8841823 TRAIN Documents.
2025-05-12 01:34:42 - Doc Example: {'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', 'title': ''}
2025-05-12 01:34:42 - Loading Queries...
2025-05-12 01:34:45 - Loaded 502939 TRAIN Queries.
2025-05-12 01:34:45 - Query Example: )what was the immediate impact of the success of the manhattan project?
2025-05-12 01:34:45 - Loaded 8841823 passages and 502939 queries
2025-05-12 01:34:45 - Processing hard negatives file...
Loading hard negatives:   0%|          | 0/502939 [00:00<?, ?it/s]Loading hard negatives:   0%|          | 267/502939 [00:00<03:08, 2669.73it/s]Loading hard negatives:   0%|          | 572/502939 [00:00<02:53, 2890.26it/s]Loading hard negatives:   0%|          | 869/502939 [00:00<02:51, 2925.62it/s]Loading hard negatives:   0%|          | 1178/502939 [00:00<02:47, 2989.37it/s]Loading hard negatives:   0%|          | 1486/502939 [00:00<02:46, 3020.66it/s]Loading hard negatives:   0%|          | 1789/502939 [00:00<02:49, 2960.02it/s]Loading hard negatives:   0%|          | 2096/502939 [00:00<02:47, 2995.32it/s]Loading hard negatives:   0%|          | 2402/502939 [00:00<02:46, 3014.60it/s]Loading hard negatives:   1%|          | 2709/502939 [00:00<02:45, 3029.36it/s]Loading hard negatives:   1%|          | 3021/502939 [00:01<02:43, 3055.30it/s]Loading hard negatives:   1%|          | 3327/502939 [00:01<02:47, 2978.11it/s]Loading hard negatives:   1%|          | 3636/502939 [00:01<02:45, 3009.06it/s]Loading hard negatives:   1%|          | 3939/502939 [00:01<02:45, 3013.49it/s]Loading hard negatives:   1%|          | 4247/502939 [00:01<02:44, 3033.24it/s]Loading hard negatives:   1%|          | 4554/502939 [00:01<02:46, 2985.94it/s]Loading hard negatives:   1%|          | 4859/502939 [00:01<02:45, 3003.63it/s]Loading hard negatives:   1%|          | 5162/502939 [00:01<02:45, 3009.05it/s]Loading hard negatives:   1%|          | 5469/502939 [00:01<02:44, 3026.33it/s]Loading hard negatives:   1%|          | 5772/502939 [00:02<04:41, 1763.43it/s]Loading hard negatives:   1%|          | 6071/502939 [00:02<04:09, 1991.29it/s]Loading hard negatives:   1%|▏         | 6375/502939 [00:02<03:43, 2220.53it/s]Loading hard negatives:   1%|▏         | 6677/502939 [00:02<03:25, 2411.20it/s]Loading hard negatives:   1%|▏         | 6980/502939 [00:02<03:13, 2567.64it/s]Loading hard negatives:   1%|▏         | 7284/502939 [00:02<03:04, 2693.30it/s]Loading hard negatives:   2%|▏         | 7589/502939 [00:02<02:57, 2789.95it/s]Loading hard negatives:   2%|▏         | 7885/502939 [00:02<02:56, 2802.85it/s]Loading hard negatives:   2%|▏         | 8182/502939 [00:02<02:53, 2849.22it/s]Loading hard negatives:   2%|▏         | 8488/502939 [00:03<02:50, 2908.07it/s]Loading hard negatives:   2%|▏         | 8795/502939 [00:03<02:47, 2954.94it/s]Loading hard negatives:   2%|▏         | 9100/502939 [00:03<02:45, 2981.55it/s]Loading hard negatives:   2%|▏         | 9402/502939 [00:03<02:47, 2948.20it/s]Loading hard negatives:   2%|▏         | 9710/502939 [00:03<02:45, 2986.00it/s]Loading hard negatives:   2%|▏         | 10011/502939 [00:03<02:44, 2989.38it/s]Loading hard negatives:   2%|▏         | 10318/502939 [00:03<02:43, 3011.54it/s]Loading hard negatives:   2%|▏         | 10620/502939 [00:03<02:44, 2989.15it/s]Loading hard negatives:   2%|▏         | 10920/502939 [00:03<02:46, 2949.97it/s]Loading hard negatives:   2%|▏         | 11235/502939 [00:03<02:43, 3007.92it/s]Loading hard negatives:   2%|▏         | 11544/502939 [00:04<02:42, 3031.28it/s]Loading hard negatives:   2%|▏         | 11850/502939 [00:04<02:41, 3039.73it/s]Loading hard negatives:   2%|▏         | 12155/502939 [00:04<02:45, 2973.53it/s]Loading hard negatives:   2%|▏         | 12469/502939 [00:04<02:42, 3021.11it/s]Loading hard negatives:   3%|▎         | 12772/502939 [00:04<02:42, 3017.24it/s]Loading hard negatives:   3%|▎         | 13079/502939 [00:04<02:41, 3030.21it/s]Loading hard negatives:   3%|▎         | 13392/502939 [00:04<02:40, 3057.83it/s]Loading hard negatives:   3%|▎         | 13698/502939 [00:04<02:43, 2998.43it/s]Loading hard negatives:   3%|▎         | 14003/502939 [00:04<02:42, 3011.73it/s]Loading hard negatives:   3%|▎         | 14310/502939 [00:05<02:41, 3027.01it/s]Loading hard negatives:   3%|▎         | 14623/502939 [00:05<02:39, 3055.33it/s]Loading hard negatives:   3%|▎         | 14929/502939 [00:05<02:40, 3049.45it/s]Loading hard negatives:   3%|▎         | 15235/502939 [00:05<02:44, 2973.19it/s]Loading hard negatives:   3%|▎         | 15545/502939 [00:05<02:41, 3008.94it/s]Loading hard negatives:   3%|▎         | 15855/502939 [00:05<02:40, 3033.34it/s]Loading hard negatives:   3%|▎         | 16162/502939 [00:05<02:39, 3044.07it/s]Loading hard negatives:   3%|▎         | 16467/502939 [00:05<02:43, 2979.35it/s]Loading hard negatives:   3%|▎         | 16770/502939 [00:05<02:42, 2993.96it/s]Loading hard negatives:   3%|▎         | 17074/502939 [00:05<02:41, 3005.12it/s]Loading hard negatives:   3%|▎         | 17385/502939 [00:06<02:40, 3033.57it/s]Loading hard negatives:   4%|▎         | 17689/502939 [00:06<02:40, 3018.53it/s]Loading hard negatives:   4%|▎         | 17992/502939 [00:06<02:43, 2961.66it/s]Loading hard negatives:   4%|▎         | 18294/502939 [00:06<02:42, 2978.11it/s]Loading hard negatives:   4%|▎         | 18599/502939 [00:06<02:41, 2998.48it/s]Loading hard negatives:   4%|▍         | 18905/502939 [00:06<02:40, 3014.47it/s]Loading hard negatives:   4%|▍         | 19211/502939 [00:06<02:39, 3025.47it/s]Loading hard negatives:   4%|▍         | 19514/502939 [00:06<02:43, 2955.08it/s]Loading hard negatives:   4%|▍         | 19811/502939 [00:06<02:43, 2957.55it/s]Loading hard negatives:   4%|▍         | 20116/502939 [00:06<02:41, 2983.83it/s]Loading hard negatives:   4%|▍         | 20417/502939 [00:07<02:41, 2991.21it/s]Loading hard negatives:   4%|▍         | 20719/502939 [00:07<02:40, 2998.29it/s]Loading hard negatives:   4%|▍         | 21019/502939 [00:07<02:43, 2945.57it/s]Loading hard negatives:   4%|▍         | 21323/502939 [00:07<02:42, 2972.40it/s]Loading hard negatives:   4%|▍         | 21628/502939 [00:07<02:40, 2994.92it/s]Loading hard negatives:   4%|▍         | 21931/502939 [00:07<02:40, 3005.08it/s]Loading hard negatives:   4%|▍         | 22236/502939 [00:07<02:39, 3016.84it/s]Loading hard negatives:   4%|▍         | 22538/502939 [00:07<02:42, 2958.08it/s]Loading hard negatives:   5%|▍         | 22841/502939 [00:07<02:41, 2977.13it/s]Loading hard negatives:   5%|▍         | 23146/502939 [00:07<02:40, 2996.82it/s]Loading hard negatives:   5%|▍         | 23449/502939 [00:08<02:39, 3004.77it/s]Loading hard negatives:   5%|▍         | 23753/502939 [00:08<02:39, 3012.39it/s]Loading hard negatives:   5%|▍         | 24055/502939 [00:08<02:41, 2965.48it/s]Loading hard negatives:   5%|▍         | 24352/502939 [00:08<02:41, 2957.74it/s]Loading hard negatives:   5%|▍         | 24648/502939 [00:08<02:41, 2956.02it/s]Loading hard negatives:   5%|▍         | 24950/502939 [00:08<02:40, 2972.58it/s]Loading hard negatives:   5%|▌         | 25253/502939 [00:08<02:39, 2988.33it/s]Loading hard negatives:   5%|▌         | 25554/502939 [00:08<02:42, 2930.56it/s]Loading hard negatives:   5%|▌         | 25863/502939 [00:08<02:40, 2975.87it/s]Loading hard negatives:   5%|▌         | 26164/502939 [00:08<02:39, 2983.95it/s]Loading hard negatives:   5%|▌         | 26468/502939 [00:09<02:38, 2998.11it/s]Loading hard negatives:   5%|▌         | 26768/502939 [00:09<02:39, 2984.74it/s]Loading hard negatives:   5%|▌         | 27074/502939 [00:09<02:41, 2942.03it/s]Loading hard negatives:   5%|▌         | 27374/502939 [00:09<02:40, 2958.93it/s]Loading hard negatives:   6%|▌         | 27677/502939 [00:09<02:39, 2978.91it/s]Loading hard negatives:   6%|▌         | 27981/502939 [00:09<02:38, 2996.49it/s]Loading hard negatives:   6%|▌         | 28284/502939 [00:09<02:37, 3005.85it/s]Loading hard negatives:   6%|▌         | 28586/502939 [00:09<02:37, 3009.18it/s]Loading hard negatives:   6%|▌         | 28887/502939 [00:09<02:41, 2940.90it/s]Loading hard negatives:   6%|▌         | 29190/502939 [00:09<02:39, 2965.39it/s]Loading hard negatives:   6%|▌         | 29495/502939 [00:10<02:38, 2989.95it/s]Loading hard negatives:   6%|▌         | 29795/502939 [00:10<02:38, 2992.48it/s]Loading hard negatives:   6%|▌         | 30098/502939 [00:10<02:37, 3003.49it/s]Loading hard negatives:   6%|▌         | 30399/502939 [00:10<02:40, 2940.45it/s]Loading hard negatives:   6%|▌         | 30703/502939 [00:10<02:39, 2968.82it/s]Loading hard negatives:   6%|▌         | 31008/502939 [00:10<02:37, 2991.30it/s]Loading hard negatives:   6%|▌         | 31310/502939 [00:10<02:37, 2998.58it/s]Loading hard negatives:   6%|▋         | 31611/502939 [00:11<04:50, 1620.69it/s]Loading hard negatives:   6%|▋         | 31887/502939 [00:11<04:16, 1833.32it/s]Loading hard negatives:   6%|▋         | 32191/502939 [00:11<03:45, 2086.74it/s]Loading hard negatives:   6%|▋         | 32501/502939 [00:11<03:22, 2320.12it/s]Loading hard negatives:   7%|▋         | 32804/502939 [00:11<03:08, 2495.40it/s]Loading hard negatives:   7%|▋         | 33107/502939 [00:11<02:58, 2635.23it/s]Loading hard negatives:   7%|▋         | 33401/502939 [00:11<02:55, 2682.32it/s]Loading hard negatives:   7%|▋         | 33705/502939 [00:11<02:48, 2780.00it/s]Loading hard negatives:   7%|▋         | 33999/502939 [00:11<02:45, 2825.21it/s]Loading hard negatives:   7%|▋         | 34303/502939 [00:11<02:42, 2884.66it/s]Loading hard negatives:   7%|▋         | 34613/502939 [00:12<02:38, 2946.09it/s]Loading hard negatives:   7%|▋         | 34916/502939 [00:12<02:37, 2968.63it/s]Loading hard negatives:   7%|▋         | 35217/502939 [00:12<02:40, 2917.68it/s]Loading hard negatives:   7%|▋         | 35515/502939 [00:12<02:39, 2935.70it/s]Loading hard negatives:   7%|▋         | 35820/502939 [00:12<02:37, 2968.50it/s]Loading hard negatives:   7%|▋         | 36126/502939 [00:12<02:35, 2995.18it/s]Loading hard negatives:   7%|▋         | 36427/502939 [00:12<02:35, 2993.15it/s]Loading hard negatives:   7%|▋         | 36728/502939 [00:12<02:39, 2921.03it/s]Loading hard negatives:   7%|▋         | 37032/502939 [00:12<02:37, 2954.39it/s]Loading hard negatives:   7%|▋         | 37342/502939 [00:13<02:35, 2994.58it/s]Loading hard negatives:   7%|▋         | 37644/502939 [00:13<02:35, 3000.91it/s]Loading hard negatives:   8%|▊         | 37946/502939 [00:13<02:34, 3005.73it/s]Loading hard negatives:   8%|▊         | 38247/502939 [00:13<02:37, 2947.38it/s]Loading hard negatives:   8%|▊         | 38553/502939 [00:13<02:35, 2978.18it/s]Loading hard negatives:   8%|▊         | 38854/502939 [00:13<02:35, 2986.29it/s]Loading hard negatives:   8%|▊         | 39159/502939 [00:13<02:34, 3003.82it/s]Loading hard negatives:   8%|▊         | 39464/502939 [00:13<02:33, 3016.31it/s]Loading hard negatives:   8%|▊         | 39766/502939 [00:13<02:36, 2958.15it/s]Loading hard negatives:   8%|▊         | 40068/502939 [00:13<02:35, 2975.28it/s]Loading hard negatives:   8%|▊         | 40372/502939 [00:14<02:34, 2993.08it/s]Loading hard negatives:   8%|▊         | 40683/502939 [00:14<02:32, 3026.26it/s]Loading hard negatives:   8%|▊         | 40986/502939 [00:14<02:33, 3011.40it/s]Loading hard negatives:   8%|▊         | 41288/502939 [00:14<02:35, 2965.35it/s]Loading hard negatives:   8%|▊         | 41590/502939 [00:14<02:34, 2980.76it/s]Loading hard negatives:   8%|▊         | 41895/502939 [00:14<02:33, 2999.02it/s]Loading hard negatives:   8%|▊         | 42199/502939 [00:14<02:33, 3009.87it/s]Loading hard negatives:   8%|▊         | 42501/502939 [00:14<02:33, 3007.51it/s]Loading hard negatives:   9%|▊         | 42802/502939 [00:14<02:35, 2954.61it/s]Loading hard negatives:   9%|▊         | 43106/502939 [00:14<02:34, 2977.43it/s]Loading hard negatives:   9%|▊         | 43404/502939 [00:15<02:34, 2970.33it/s]Loading hard negatives:   9%|▊         | 43709/502939 [00:15<02:33, 2991.38it/s]Loading hard negatives:   9%|▉         | 44019/502939 [00:15<02:31, 3022.76it/s]Loading hard negatives:   9%|▉         | 44322/502939 [00:15<02:34, 2963.00it/s]Loading hard negatives:   9%|▉         | 44626/502939 [00:15<02:33, 2983.37it/s]Loading hard negatives:   9%|▉         | 44930/502939 [00:15<02:32, 2999.82it/s]Loading hard negatives:   9%|▉         | 45235/502939 [00:15<02:31, 3013.15it/s]Loading hard negatives:   9%|▉         | 45538/502939 [00:15<02:31, 3017.31it/s]Loading hard negatives:   9%|▉         | 45840/502939 [00:15<02:35, 2945.37it/s]Loading hard negatives:   9%|▉         | 46151/502939 [00:15<02:32, 2992.47it/s]Loading hard negatives:   9%|▉         | 46451/502939 [00:16<02:58, 2556.94it/s]Loading hard negatives:   9%|▉         | 46718/502939 [00:16<03:47, 2008.69it/s]Loading hard negatives:   9%|▉         | 46947/502939 [00:16<03:40, 2071.96it/s]Loading hard negatives:   9%|▉         | 47252/502939 [00:16<03:17, 2311.35it/s]Loading hard negatives:   9%|▉         | 47555/502939 [00:16<03:02, 2496.92it/s]Loading hard negatives:  10%|▉         | 47837/502939 [00:16<02:56, 2584.02it/s]Loading hard negatives:  10%|▉         | 48140/502939 [00:16<02:47, 2708.15it/s]Loading hard negatives:  10%|▉         | 48447/502939 [00:16<02:41, 2810.70it/s]Loading hard negatives:  10%|▉         | 48754/502939 [00:17<02:37, 2883.98it/s]Loading hard negatives:  10%|▉         | 49059/502939 [00:17<02:34, 2931.70it/s]Loading hard negatives:  10%|▉         | 49356/502939 [00:17<02:35, 2907.83it/s]Loading hard negatives:  10%|▉         | 49659/502939 [00:17<02:34, 2943.20it/s]Loading hard negatives:  10%|▉         | 49963/502939 [00:17<02:32, 2971.21it/s]Loading hard negatives:  10%|▉         | 50268/502939 [00:17<02:31, 2992.25it/s]Loading hard negatives:  10%|█         | 50569/502939 [00:17<02:31, 2995.45it/s]Loading hard negatives:  10%|█         | 50870/502939 [00:17<02:33, 2944.55it/s]Loading hard negatives:  10%|█         | 51173/502939 [00:17<02:32, 2969.01it/s]Loading hard negatives:  10%|█         | 51485/502939 [00:17<02:29, 3011.67it/s]Loading hard negatives:  10%|█         | 51790/502939 [00:18<02:29, 3022.61it/s]Loading hard negatives:  10%|█         | 52096/502939 [00:18<02:28, 3031.57it/s]Loading hard negatives:  10%|█         | 52400/502939 [00:18<02:31, 2968.76it/s]Loading hard negatives:  10%|█         | 52703/502939 [00:18<02:30, 2984.86it/s]Loading hard negatives:  11%|█         | 53004/502939 [00:18<02:30, 2990.74it/s]Loading hard negatives:  11%|█         | 53307/502939 [00:18<02:29, 2999.76it/s]Loading hard negatives:  11%|█         | 53610/502939 [00:18<02:29, 3007.40it/s]Loading hard negatives:  11%|█         | 53911/502939 [00:18<02:32, 2948.02it/s]Loading hard negatives:  11%|█         | 54221/502939 [00:18<02:30, 2990.63it/s]Loading hard negatives:  11%|█         | 54523/502939 [00:18<02:29, 2998.65it/s]Loading hard negatives:  11%|█         | 54827/502939 [00:19<02:28, 3009.87it/s]Loading hard negatives:  11%|█         | 55130/502939 [00:19<02:28, 3013.87it/s]Loading hard negatives:  11%|█         | 55432/502939 [00:19<02:31, 2946.16it/s]Loading hard negatives:  11%|█         | 55736/502939 [00:19<02:30, 2972.79it/s]Loading hard negatives:  11%|█         | 56040/502939 [00:19<02:29, 2991.86it/s]Loading hard negatives:  11%|█         | 56344/502939 [00:19<02:28, 3004.74it/s]Loading hard negatives:  11%|█▏        | 56653/502939 [00:19<02:27, 3028.03it/s]Loading hard negatives:  11%|█▏        | 56956/502939 [00:19<02:30, 2954.37it/s]Loading hard negatives:  11%|█▏        | 57259/502939 [00:19<02:29, 2974.46it/s]Loading hard negatives:  11%|█▏        | 57559/502939 [00:19<02:29, 2979.86it/s]Loading hard negatives:  12%|█▏        | 57864/502939 [00:20<02:28, 2999.59it/s]Loading hard negatives:  12%|█▏        | 58167/502939 [00:20<02:27, 3007.44it/s]Loading hard negatives:  12%|█▏        | 58468/502939 [00:20<02:31, 2942.88it/s]Loading hard negatives:  12%|█▏        | 58772/502939 [00:20<02:29, 2969.98it/s]Loading hard negatives:  12%|█▏        | 59070/502939 [00:20<02:29, 2971.25it/s]Loading hard negatives:  12%|█▏        | 59382/502939 [00:20<02:27, 3013.37it/s]Loading hard negatives:  12%|█▏        | 59684/502939 [00:20<02:27, 3013.65it/s]Loading hard negatives:  12%|█▏        | 59986/502939 [00:20<02:29, 2953.56it/s]Loading hard negatives:  12%|█▏        | 60288/502939 [00:20<02:28, 2971.52it/s]Loading hard negatives:  12%|█▏        | 60592/502939 [00:20<02:27, 2989.62it/s]Loading hard negatives:  12%|█▏        | 60897/502939 [00:21<02:26, 3007.22it/s]Loading hard negatives:  12%|█▏        | 61201/502939 [00:21<02:26, 3015.88it/s]Loading hard negatives:  12%|█▏        | 61503/502939 [00:21<02:29, 2944.56it/s]Loading hard negatives:  12%|█▏        | 61805/502939 [00:21<02:28, 2964.97it/s]Loading hard negatives:  12%|█▏        | 62105/502939 [00:21<02:28, 2973.44it/s]Loading hard negatives:  12%|█▏        | 62403/502939 [00:21<05:04, 1449.09it/s]Loading hard negatives:  12%|█▏        | 62705/502939 [00:22<04:16, 1717.40it/s]Loading hard negatives:  13%|█▎        | 63010/502939 [00:22<03:42, 1978.36it/s]Loading hard negatives:  13%|█▎        | 63292/502939 [00:22<03:23, 2161.28it/s]Loading hard negatives:  13%|█▎        | 63585/502939 [00:22<03:07, 2344.25it/s]Loading hard negatives:  13%|█▎        | 63889/502939 [00:22<02:54, 2520.14it/s]Loading hard negatives:  13%|█▎        | 64193/502939 [00:22<02:45, 2657.97it/s]Loading hard negatives:  13%|█▎        | 64498/502939 [00:22<02:38, 2765.08it/s]Loading hard negatives:  13%|█▎        | 64793/502939 [00:22<02:38, 2760.07it/s]Loading hard negatives:  13%|█▎        | 65098/502939 [00:22<02:34, 2840.64it/s]Loading hard negatives:  13%|█▎        | 65398/502939 [00:22<02:31, 2885.43it/s]Loading hard negatives:  13%|█▎        | 65698/502939 [00:23<02:29, 2917.28it/s]Loading hard negatives:  13%|█▎        | 66000/502939 [00:23<02:28, 2946.31it/s]Loading hard negatives:  13%|█▎        | 66303/502939 [00:23<02:27, 2969.71it/s]Loading hard negatives:  13%|█▎        | 66603/502939 [00:23<02:28, 2930.26it/s]Loading hard negatives:  13%|█▎        | 66902/502939 [00:23<02:27, 2947.30it/s]Loading hard negatives:  13%|█▎        | 67202/502939 [00:23<02:27, 2961.84it/s]Loading hard negatives:  13%|█▎        | 67504/502939 [00:23<02:26, 2977.02it/s]Loading hard negatives:  13%|█▎        | 67809/502939 [00:23<02:25, 2996.16it/s]Loading hard negatives:  14%|█▎        | 68110/502939 [00:23<02:28, 2936.00it/s]Loading hard negatives:  14%|█▎        | 68413/502939 [00:23<02:26, 2963.62it/s]Loading hard negatives:  14%|█▎        | 68715/502939 [00:24<02:25, 2979.60it/s]Loading hard negatives:  14%|█▎        | 69021/502939 [00:24<02:24, 3001.70it/s]Loading hard negatives:  14%|█▍        | 69322/502939 [00:24<02:24, 2999.43it/s]Loading hard negatives:  14%|█▍        | 69623/502939 [00:24<02:27, 2938.43it/s]Loading hard negatives:  14%|█▍        | 69921/502939 [00:24<02:26, 2949.90it/s]Loading hard negatives:  14%|█▍        | 70221/502939 [00:24<02:26, 2962.74it/s]Loading hard negatives:  14%|█▍        | 70523/502939 [00:24<02:25, 2977.51it/s]Loading hard negatives:  14%|█▍        | 70829/502939 [00:24<02:24, 3000.31it/s]Loading hard negatives:  14%|█▍        | 71130/502939 [00:24<02:26, 2949.14it/s]Loading hard negatives:  14%|█▍        | 71436/502939 [00:24<02:24, 2981.08it/s]Loading hard negatives:  14%|█▍        | 71735/502939 [00:25<02:24, 2982.58it/s]Loading hard negatives:  14%|█▍        | 72041/502939 [00:25<02:23, 3004.34it/s]Loading hard negatives:  14%|█▍        | 72348/502939 [00:25<02:22, 3022.39it/s]Loading hard negatives:  14%|█▍        | 72651/502939 [00:25<02:25, 2949.93it/s]Loading hard negatives:  15%|█▍        | 72958/502939 [00:25<02:24, 2983.26it/s]Loading hard negatives:  15%|█▍        | 73270/502939 [00:25<02:22, 3022.54it/s]Loading hard negatives:  15%|█▍        | 73573/502939 [00:25<02:22, 3021.47it/s]Loading hard negatives:  15%|█▍        | 73876/502939 [00:25<02:22, 3016.30it/s]Loading hard negatives:  15%|█▍        | 74178/502939 [00:25<02:24, 2958.13it/s]Loading hard negatives:  15%|█▍        | 74484/502939 [00:25<02:23, 2986.76it/s]Loading hard negatives:  15%|█▍        | 74789/502939 [00:26<02:22, 3002.99it/s]Loading hard negatives:  15%|█▍        | 75091/502939 [00:26<02:22, 3007.20it/s]Loading hard negatives:  15%|█▍        | 75397/502939 [00:26<02:21, 3020.32it/s]Loading hard negatives:  15%|█▌        | 75700/502939 [00:26<02:24, 2964.35it/s]Loading hard negatives:  15%|█▌        | 76005/502939 [00:26<02:22, 2989.42it/s]Loading hard negatives:  15%|█▌        | 76305/502939 [00:26<02:22, 2989.82it/s]Loading hard negatives:  15%|█▌        | 76608/502939 [00:26<02:22, 3001.73it/s]Loading hard negatives:  15%|█▌        | 76915/502939 [00:26<02:21, 3020.15it/s]Loading hard negatives:  15%|█▌        | 77218/502939 [00:26<02:23, 2958.95it/s]Loading hard negatives:  15%|█▌        | 77521/502939 [00:27<02:22, 2979.74it/s]Loading hard negatives:  15%|█▌        | 77824/502939 [00:27<02:22, 2993.62it/s]Loading hard negatives:  16%|█▌        | 78129/502939 [00:27<02:21, 3009.15it/s]Loading hard negatives:  16%|█▌        | 78436/502939 [00:27<02:20, 3025.52it/s]Loading hard negatives:  16%|█▌        | 78739/502939 [00:27<02:23, 2953.02it/s]Loading hard negatives:  16%|█▌        | 79046/502939 [00:27<02:21, 2985.40it/s]Loading hard negatives:  16%|█▌        | 79351/502939 [00:27<02:20, 3004.20it/s]Loading hard negatives:  16%|█▌        | 79654/502939 [00:27<02:20, 3009.33it/s]Loading hard negatives:  16%|█▌        | 79956/502939 [00:27<02:20, 3010.96it/s]Loading hard negatives:  16%|█▌        | 80258/502939 [00:27<02:23, 2951.85it/s]Loading hard negatives:  16%|█▌        | 80561/502939 [00:28<02:22, 2973.53it/s]Loading hard negatives:  16%|█▌        | 80866/502939 [00:28<02:20, 2995.02it/s]Loading hard negatives:  16%|█▌        | 81166/502939 [00:28<02:21, 2991.26it/s]Loading hard negatives:  16%|█▌        | 81469/502939 [00:28<02:20, 3002.72it/s]Loading hard negatives:  16%|█▋        | 81770/502939 [00:28<02:22, 2952.50it/s]Loading hard negatives:  16%|█▋        | 82066/502939 [00:28<02:23, 2939.44it/s]Loading hard negatives:  16%|█▋        | 82368/502939 [00:28<02:21, 2961.82it/s]Loading hard negatives:  16%|█▋        | 82671/502939 [00:28<02:20, 2980.78it/s]Loading hard negatives:  16%|█▋        | 82976/502939 [00:28<02:19, 3000.00it/s]Loading hard negatives:  17%|█▋        | 83277/502939 [00:28<02:24, 2902.56it/s]Loading hard negatives:  17%|█▋        | 83580/502939 [00:29<02:22, 2938.61it/s]Loading hard negatives:  17%|█▋        | 83885/502939 [00:29<02:21, 2971.11it/s]Loading hard negatives:  17%|█▋        | 84188/502939 [00:29<02:20, 2986.38it/s]Loading hard negatives:  17%|█▋        | 84492/502939 [00:29<02:19, 2999.87it/s]Loading hard negatives:  17%|█▋        | 84793/502939 [00:29<02:22, 2943.60it/s]Loading hard negatives:  17%|█▋        | 85095/502939 [00:29<02:20, 2965.71it/s]Loading hard negatives:  17%|█▋        | 85398/502939 [00:29<02:19, 2983.66it/s]Loading hard negatives:  17%|█▋        | 85697/502939 [00:29<02:20, 2968.36it/s]Loading hard negatives:  17%|█▋        | 85998/502939 [00:29<02:19, 2978.99it/s]Loading hard negatives:  17%|█▋        | 86297/502939 [00:29<02:22, 2932.64it/s]Loading hard negatives:  17%|█▋        | 86602/502939 [00:30<02:20, 2964.71it/s]Loading hard negatives:  17%|█▋        | 86911/502939 [00:30<02:18, 3001.16it/s]Loading hard negatives:  17%|█▋        | 87214/502939 [00:30<02:18, 3007.20it/s]Loading hard negatives:  17%|█▋        | 87515/502939 [00:30<02:18, 3006.83it/s]Loading hard negatives:  17%|█▋        | 87816/502939 [00:30<02:20, 2958.80it/s]Loading hard negatives:  18%|█▊        | 88113/502939 [00:30<02:20, 2957.23it/s]Loading hard negatives:  18%|█▊        | 88417/502939 [00:30<02:19, 2979.98it/s]Loading hard negatives:  18%|█▊        | 88722/502939 [00:30<02:18, 2999.83it/s]Loading hard negatives:  18%|█▊        | 89030/502939 [00:30<02:16, 3023.00it/s]Loading hard negatives:  18%|█▊        | 89333/502939 [00:30<02:19, 2959.08it/s]Loading hard negatives:  18%|█▊        | 89635/502939 [00:31<02:18, 2976.06it/s]Loading hard negatives:  18%|█▊        | 89936/502939 [00:31<02:18, 2984.99it/s]Loading hard negatives:  18%|█▊        | 90235/502939 [00:31<02:18, 2977.11it/s]Loading hard negatives:  18%|█▊        | 90538/502939 [00:31<02:17, 2991.01it/s]Loading hard negatives:  18%|█▊        | 90842/502939 [00:31<02:17, 3002.97it/s]Loading hard negatives:  18%|█▊        | 91143/502939 [00:31<02:19, 2949.56it/s]Loading hard negatives:  18%|█▊        | 91445/502939 [00:31<02:18, 2970.14it/s]Loading hard negatives:  18%|█▊        | 91748/502939 [00:31<02:17, 2987.75it/s]Loading hard negatives:  18%|█▊        | 92050/502939 [00:31<02:17, 2996.25it/s]Loading hard negatives:  18%|█▊        | 92357/502939 [00:31<02:16, 3015.43it/s]Loading hard negatives:  18%|█▊        | 92659/502939 [00:32<02:19, 2946.30it/s]Loading hard negatives:  18%|█▊        | 92960/502939 [00:32<02:18, 2963.08it/s]Loading hard negatives:  19%|█▊        | 93266/502939 [00:32<02:17, 2989.21it/s]Loading hard negatives:  19%|█▊        | 93570/502939 [00:32<02:16, 3003.41it/s]Loading hard negatives:  19%|█▊        | 93873/502939 [00:32<02:15, 3009.40it/s]Loading hard negatives:  19%|█▊        | 94175/502939 [00:32<02:19, 2931.99it/s]Loading hard negatives:  19%|█▉        | 94482/502939 [00:32<02:17, 2972.38it/s]Loading hard negatives:  19%|█▉        | 94786/502939 [00:32<02:16, 2991.77it/s]Loading hard negatives:  19%|█▉        | 95086/502939 [00:32<02:16, 2993.87it/s]Loading hard negatives:  19%|█▉        | 95391/502939 [00:33<02:15, 3009.33it/s]Loading hard negatives:  19%|█▉        | 95693/502939 [00:33<02:17, 2971.14it/s]Loading hard negatives:  19%|█▉        | 95998/502939 [00:33<02:16, 2991.90it/s]Loading hard negatives:  19%|█▉        | 96305/502939 [00:33<02:14, 3014.97it/s]Loading hard negatives:  19%|█▉        | 96609/502939 [00:33<02:14, 3021.08it/s]Loading hard negatives:  19%|█▉        | 96914/502939 [00:33<02:14, 3026.84it/s]Loading hard negatives:  19%|█▉        | 97217/502939 [00:33<02:16, 2964.48it/s]Loading hard negatives:  19%|█▉        | 97518/502939 [00:33<02:16, 2977.39it/s]Loading hard negatives:  19%|█▉        | 97824/502939 [00:33<02:15, 3000.53it/s]Loading hard negatives:  20%|█▉        | 98130/502939 [00:33<02:14, 3015.91it/s]Loading hard negatives:  20%|█▉        | 98435/502939 [00:34<02:13, 3024.67it/s]Loading hard negatives:  20%|█▉        | 98738/502939 [00:34<02:15, 2973.28it/s]Loading hard negatives:  20%|█▉        | 99046/502939 [00:34<02:14, 3002.42it/s]Loading hard negatives:  20%|█▉        | 99352/502939 [00:34<02:13, 3018.36it/s]Loading hard negatives:  20%|█▉        | 99655/502939 [00:34<02:13, 3016.79it/s]Loading hard negatives:  20%|█▉        | 99957/502939 [00:34<05:04, 1322.27it/s]Loading hard negatives:  20%|█▉        | 100236/502939 [00:35<04:19, 1552.75it/s]Loading hard negatives:  20%|█▉        | 100538/502939 [00:35<03:40, 1821.61it/s]Loading hard negatives:  20%|██        | 100842/502939 [00:35<03:13, 2073.47it/s]Loading hard negatives:  20%|██        | 101146/502939 [00:35<02:55, 2294.37it/s]Loading hard negatives:  20%|██        | 101451/502939 [00:35<02:41, 2480.07it/s]Loading hard negatives:  20%|██        | 101741/502939 [00:35<02:36, 2571.52it/s]Loading hard negatives:  20%|██        | 102050/502939 [00:35<02:27, 2710.24it/s]Loading hard negatives:  20%|██        | 102347/502939 [00:35<02:24, 2780.45it/s]Loading hard negatives:  20%|██        | 102655/502939 [00:35<02:19, 2864.12it/s]Loading hard negatives:  20%|██        | 102962/502939 [00:35<02:16, 2921.64it/s]Loading hard negatives:  21%|██        | 103263/502939 [00:36<02:18, 2894.56it/s]Loading hard negatives:  21%|██        | 103568/502939 [00:36<02:15, 2938.85it/s]Loading hard negatives:  21%|██        | 103873/502939 [00:36<02:14, 2969.78it/s]Loading hard negatives:  21%|██        | 104176/502939 [00:36<02:13, 2986.61it/s]Loading hard negatives:  21%|██        | 104477/502939 [00:36<02:13, 2985.38it/s]Loading hard negatives:  21%|██        | 104778/502939 [00:36<02:16, 2909.89it/s]Loading hard negatives:  21%|██        | 105081/502939 [00:36<02:15, 2944.94it/s]Loading hard negatives:  21%|██        | 105388/502939 [00:36<02:13, 2979.13it/s]Loading hard negatives:  21%|██        | 105690/502939 [00:36<02:12, 2990.61it/s]Loading hard negatives:  21%|██        | 105994/502939 [00:36<02:12, 3004.43it/s]Loading hard negatives:  21%|██        | 106295/502939 [00:37<02:15, 2937.57it/s]Loading hard negatives:  21%|██        | 106598/502939 [00:37<02:13, 2963.63it/s]Loading hard negatives:  21%|██▏       | 106898/502939 [00:37<02:13, 2973.21it/s]Loading hard negatives:  21%|██▏       | 107204/502939 [00:37<02:12, 2997.19it/s]Loading hard negatives:  21%|██▏       | 107510/502939 [00:37<02:11, 3014.53it/s]Loading hard negatives:  21%|██▏       | 107812/502939 [00:37<02:14, 2945.58it/s]Loading hard negatives:  21%|██▏       | 108118/502939 [00:37<02:12, 2976.80it/s]Loading hard negatives:  22%|██▏       | 108423/502939 [00:37<02:11, 2998.05it/s]Loading hard negatives:  22%|██▏       | 108727/502939 [00:37<02:10, 3010.43it/s]Loading hard negatives:  22%|██▏       | 109031/502939 [00:37<02:10, 3016.60it/s]Loading hard negatives:  22%|██▏       | 109333/502939 [00:38<02:13, 2938.76it/s]Loading hard negatives:  22%|██▏       | 109636/502939 [00:38<02:12, 2965.49it/s]Loading hard negatives:  22%|██▏       | 109940/502939 [00:38<02:11, 2985.83it/s]Loading hard negatives:  22%|██▏       | 110246/502939 [00:38<02:10, 3006.73it/s]Loading hard negatives:  22%|██▏       | 110551/502939 [00:38<02:09, 3019.35it/s]Loading hard negatives:  22%|██▏       | 110854/502939 [00:38<02:12, 2967.84it/s]Loading hard negatives:  22%|██▏       | 111161/502939 [00:38<02:10, 2996.93it/s]Loading hard negatives:  22%|██▏       | 111462/502939 [00:38<02:10, 2999.18it/s]Loading hard negatives:  22%|██▏       | 111767/502939 [00:38<02:09, 3012.73it/s]Loading hard negatives:  22%|██▏       | 112069/502939 [00:39<02:09, 3012.10it/s]Loading hard negatives:  22%|██▏       | 112371/502939 [00:39<02:12, 2950.02it/s]Loading hard negatives:  22%|██▏       | 112675/502939 [00:39<02:11, 2975.04it/s]Loading hard negatives:  22%|██▏       | 112973/502939 [00:39<02:11, 2954.38it/s]Loading hard negatives:  23%|██▎       | 113276/502939 [00:39<02:10, 2975.66it/s]Loading hard negatives:  23%|██▎       | 113577/502939 [00:39<02:10, 2983.76it/s]Loading hard negatives:  23%|██▎       | 113877/502939 [00:39<02:10, 2986.64it/s]Loading hard negatives:  23%|██▎       | 114176/502939 [00:39<02:12, 2926.97it/s]Loading hard negatives:  23%|██▎       | 114480/502939 [00:39<02:11, 2960.10it/s]Loading hard negatives:  23%|██▎       | 114786/502939 [00:39<02:09, 2988.68it/s]Loading hard negatives:  23%|██▎       | 115093/502939 [00:40<02:08, 3010.54it/s]Loading hard negatives:  23%|██▎       | 115399/502939 [00:40<02:08, 3023.37it/s]Loading hard negatives:  23%|██▎       | 115702/502939 [00:40<02:10, 2957.92it/s]Loading hard negatives:  23%|██▎       | 116007/502939 [00:40<02:09, 2982.58it/s]Loading hard negatives:  23%|██▎       | 116310/502939 [00:40<02:09, 2995.21it/s]Loading hard negatives:  23%|██▎       | 116615/502939 [00:40<02:08, 3010.23it/s]Loading hard negatives:  23%|██▎       | 116921/502939 [00:40<02:07, 3024.28it/s]Loading hard negatives:  23%|██▎       | 117224/502939 [00:40<02:11, 2937.55it/s]Loading hard negatives:  23%|██▎       | 117528/502939 [00:40<02:09, 2967.43it/s]Loading hard negatives:  23%|██▎       | 117834/502939 [00:40<02:08, 2992.04it/s]Loading hard negatives:  23%|██▎       | 118139/502939 [00:41<02:07, 3007.38it/s]Loading hard negatives:  24%|██▎       | 118444/502939 [00:41<02:07, 3018.70it/s]Loading hard negatives:  24%|██▎       | 118747/502939 [00:41<02:10, 2946.63it/s]Loading hard negatives:  24%|██▎       | 119048/502939 [00:41<02:09, 2962.34it/s]Loading hard negatives:  24%|██▎       | 119353/502939 [00:41<02:08, 2987.52it/s]Loading hard negatives:  24%|██▍       | 119659/502939 [00:41<02:07, 3007.41it/s]Loading hard negatives:  24%|██▍       | 119961/502939 [00:41<02:07, 3010.19it/s]Loading hard negatives:  24%|██▍       | 120263/502939 [00:41<02:09, 2948.22it/s]Loading hard negatives:  24%|██▍       | 120566/502939 [00:41<02:08, 2971.38it/s]Loading hard negatives:  24%|██▍       | 120865/502939 [00:41<02:08, 2975.63it/s]Loading hard negatives:  24%|██▍       | 121171/502939 [00:42<02:07, 2999.64it/s]Loading hard negatives:  24%|██▍       | 121474/502939 [00:42<02:06, 3007.94it/s]Loading hard negatives:  24%|██▍       | 121775/502939 [00:42<02:09, 2946.65it/s]Loading hard negatives:  24%|██▍       | 122081/502939 [00:42<02:07, 2977.72it/s]Loading hard negatives:  24%|██▍       | 122387/502939 [00:42<02:06, 3001.38it/s]Loading hard negatives:  24%|██▍       | 122690/502939 [00:42<02:06, 3009.70it/s]Loading hard negatives:  24%|██▍       | 122994/502939 [00:42<02:05, 3017.37it/s]Loading hard negatives:  25%|██▍       | 123296/502939 [00:42<02:09, 2937.56it/s]Loading hard negatives:  25%|██▍       | 123602/502939 [00:42<02:07, 2971.99it/s]Loading hard negatives:  25%|██▍       | 123906/502939 [00:42<02:06, 2991.10it/s]Loading hard negatives:  25%|██▍       | 124211/502939 [00:43<02:05, 3006.24it/s]Loading hard negatives:  25%|██▍       | 124512/502939 [00:43<02:06, 3002.30it/s]Loading hard negatives:  25%|██▍       | 124813/502939 [00:43<02:08, 2952.04it/s]Loading hard negatives:  25%|██▍       | 125116/502939 [00:43<02:07, 2974.28it/s]Loading hard negatives:  25%|██▍       | 125414/502939 [00:43<02:07, 2962.90it/s]Loading hard negatives:  25%|██▍       | 125713/502939 [00:43<02:07, 2968.91it/s]Loading hard negatives:  25%|██▌       | 126020/502939 [00:43<02:05, 2996.34it/s]Loading hard negatives:  25%|██▌       | 126320/502939 [00:43<02:08, 2921.16it/s]Loading hard negatives:  25%|██▌       | 126623/502939 [00:43<02:07, 2951.57it/s]Loading hard negatives:  25%|██▌       | 126923/502939 [00:44<02:06, 2964.92it/s]Loading hard negatives:  25%|██▌       | 127228/502939 [00:44<02:05, 2987.90it/s]Loading hard negatives:  25%|██▌       | 127532/502939 [00:44<02:05, 3001.75it/s]Loading hard negatives:  25%|██▌       | 127833/502939 [00:44<02:07, 2946.29it/s]Loading hard negatives:  25%|██▌       | 128139/502939 [00:44<02:05, 2977.42it/s]Loading hard negatives:  26%|██▌       | 128444/502939 [00:44<02:04, 2998.14it/s]Loading hard negatives:  26%|██▌       | 128747/502939 [00:44<02:04, 3007.55it/s]Loading hard negatives:  26%|██▌       | 129050/502939 [00:44<02:04, 3013.63it/s]Loading hard negatives:  26%|██▌       | 129352/502939 [00:44<02:07, 2923.22it/s]Loading hard negatives:  26%|██▌       | 129656/502939 [00:44<02:06, 2956.28it/s]Loading hard negatives:  26%|██▌       | 129958/502939 [00:45<02:05, 2973.63it/s]Loading hard negatives:  26%|██▌       | 130260/502939 [00:45<02:04, 2985.86it/s]Loading hard negatives:  26%|██▌       | 130563/502939 [00:45<02:04, 2996.79it/s]Loading hard negatives:  26%|██▌       | 130863/502939 [00:45<02:06, 2944.44it/s]Loading hard negatives:  26%|██▌       | 131168/502939 [00:45<02:04, 2975.13it/s]Loading hard negatives:  26%|██▌       | 131466/502939 [00:45<04:21, 1421.57it/s]Loading hard negatives:  26%|██▌       | 131769/502939 [00:45<03:39, 1691.50it/s]Loading hard negatives:  26%|██▋       | 132074/502939 [00:46<03:09, 1953.96it/s]Loading hard negatives:  26%|██▋       | 132367/502939 [00:46<02:52, 2145.48it/s]Loading hard negatives:  26%|██▋       | 132658/502939 [00:46<02:39, 2323.58it/s]Loading hard negatives:  26%|██▋       | 132960/502939 [00:46<02:28, 2497.43it/s]Loading hard negatives:  26%|██▋       | 133266/502939 [00:46<02:19, 2644.98it/s]Loading hard negatives:  27%|██▋       | 133568/502939 [00:46<02:14, 2746.11it/s]Loading hard negatives:  27%|██▋       | 133873/502939 [00:46<02:10, 2831.18it/s]Loading hard negatives:  27%|██▋       | 134171/502939 [00:46<02:10, 2829.38it/s]Loading hard negatives:  27%|██▋       | 134474/502939 [00:46<02:07, 2886.40it/s]Loading hard negatives:  27%|██▋       | 134779/502939 [00:47<02:05, 2932.96it/s]Loading hard negatives:  27%|██▋       | 135078/502939 [00:47<02:04, 2944.92it/s]Loading hard negatives:  27%|██▋       | 135377/502939 [00:47<02:04, 2957.53it/s]Loading hard negatives:  27%|██▋       | 135676/502939 [00:47<02:06, 2909.05it/s]Loading hard negatives:  27%|██▋       | 135979/502939 [00:47<02:04, 2944.05it/s]Loading hard negatives:  27%|██▋       | 136283/502939 [00:47<02:03, 2971.57it/s]Loading hard negatives:  27%|██▋       | 136587/502939 [00:47<02:02, 2991.60it/s]Loading hard negatives:  27%|██▋       | 136889/502939 [00:47<02:02, 2999.40it/s]Loading hard negatives:  27%|██▋       | 137190/502939 [00:47<02:05, 2918.19it/s]Loading hard negatives:  27%|██▋       | 137492/502939 [00:47<02:03, 2947.36it/s]Loading hard negatives:  27%|██▋       | 137795/502939 [00:48<02:02, 2971.27it/s]Loading hard negatives:  27%|██▋       | 138098/502939 [00:48<02:02, 2987.05it/s]Loading hard negatives:  28%|██▊       | 138403/502939 [00:48<02:01, 3005.69it/s]Loading hard negatives:  28%|██▊       | 138704/502939 [00:48<02:03, 2949.55it/s]Loading hard negatives:  28%|██▊       | 139011/502939 [00:48<02:02, 2982.91it/s]Loading hard negatives:  28%|██▊       | 139316/502939 [00:48<02:01, 3002.02it/s]Loading hard negatives:  28%|██▊       | 139617/502939 [00:48<02:01, 2996.44it/s]Loading hard negatives:  28%|██▊       | 139924/502939 [00:48<02:00, 3017.14it/s]Loading hard negatives:  28%|██▊       | 140226/502939 [00:48<02:03, 2945.72it/s]Loading hard negatives:  28%|██▊       | 140531/502939 [00:48<02:01, 2975.57it/s]Loading hard negatives:  28%|██▊       | 140837/502939 [00:49<02:00, 2998.60it/s]Loading hard negatives:  28%|██▊       | 141145/502939 [00:49<01:59, 3020.79it/s]Loading hard negatives:  28%|██▊       | 141448/502939 [00:49<01:59, 3019.71it/s]Loading hard negatives:  28%|██▊       | 141751/502939 [00:49<02:02, 2957.14it/s]Loading hard negatives:  28%|██▊       | 142050/502939 [00:49<02:01, 2964.52it/s]Loading hard negatives:  28%|██▊       | 142352/502939 [00:49<02:00, 2980.66it/s]Loading hard negatives:  28%|██▊       | 142655/502939 [00:49<02:00, 2992.79it/s]Loading hard negatives:  28%|██▊       | 142957/502939 [00:49<01:59, 3000.01it/s]Loading hard negatives:  28%|██▊       | 143258/502939 [00:49<02:02, 2932.97it/s]Loading hard negatives:  29%|██▊       | 143562/502939 [00:49<02:01, 2962.33it/s]Loading hard negatives:  29%|██▊       | 143864/502939 [00:50<02:00, 2979.25it/s]Loading hard negatives:  29%|██▊       | 144163/502939 [00:50<02:00, 2972.12it/s]Loading hard negatives:  29%|██▊       | 144465/502939 [00:50<02:00, 2985.08it/s]Loading hard negatives:  29%|██▉       | 144767/502939 [00:50<01:59, 2992.94it/s]Loading hard negatives:  29%|██▉       | 145067/502939 [00:50<02:01, 2938.36it/s]Loading hard negatives:  29%|██▉       | 145370/502939 [00:50<02:00, 2962.74it/s]Loading hard negatives:  29%|██▉       | 145677/502939 [00:50<01:59, 2993.80it/s]Loading hard negatives:  29%|██▉       | 145979/502939 [00:50<01:58, 2999.97it/s]Loading hard negatives:  29%|██▉       | 146283/502939 [00:50<01:58, 3010.17it/s]Loading hard negatives:  29%|██▉       | 146585/502939 [00:50<02:01, 2939.28it/s]Loading hard negatives:  29%|██▉       | 146880/502939 [00:51<05:14, 1130.99it/s]Loading hard negatives:  29%|██▉       | 147180/502939 [00:51<04:15, 1390.16it/s]Loading hard negatives:  29%|██▉       | 147481/502939 [00:51<03:34, 1657.66it/s]Loading hard negatives:  29%|██▉       | 147785/502939 [00:51<03:04, 1921.41it/s]Loading hard negatives:  29%|██▉       | 148065/502939 [00:52<02:48, 2103.37it/s]Loading hard negatives:  30%|██▉       | 148369/502939 [00:52<02:32, 2322.11it/s]Loading hard negatives:  30%|██▉       | 148673/502939 [00:52<02:21, 2500.06it/s]Loading hard negatives:  30%|██▉       | 148980/502939 [00:52<02:13, 2648.29it/s]Loading hard negatives:  30%|██▉       | 149281/502939 [00:52<02:08, 2745.35it/s]Loading hard negatives:  30%|██▉       | 149581/502939 [00:52<02:07, 2766.98it/s]Loading hard negatives:  30%|██▉       | 149884/502939 [00:52<02:04, 2840.39it/s]Loading hard negatives:  30%|██▉       | 150189/502939 [00:52<02:01, 2900.47it/s]Loading hard negatives:  30%|██▉       | 150488/502939 [00:52<02:00, 2920.84it/s]Loading hard negatives:  30%|██▉       | 150793/502939 [00:52<01:59, 2956.45it/s]Loading hard negatives:  30%|███       | 151097/502939 [00:53<01:58, 2980.10it/s]Loading hard negatives:  30%|███       | 151398/502939 [00:53<02:00, 2915.89it/s]Loading hard negatives:  30%|███       | 151705/502939 [00:53<01:58, 2958.94it/s]Loading hard negatives:  30%|███       | 152008/502939 [00:53<01:57, 2978.41it/s]Loading hard negatives:  30%|███       | 152314/502939 [00:53<01:56, 3001.67it/s]Loading hard negatives:  30%|███       | 152618/502939 [00:53<01:56, 3012.19it/s]Loading hard negatives:  30%|███       | 152920/502939 [00:53<01:58, 2949.85it/s]Loading hard negatives:  30%|███       | 153223/502939 [00:53<01:57, 2972.17it/s]Loading hard negatives:  31%|███       | 153527/502939 [00:53<01:56, 2991.28it/s]Loading hard negatives:  31%|███       | 153828/502939 [00:53<01:56, 2994.48it/s]Loading hard negatives:  31%|███       | 154133/502939 [00:54<01:55, 3010.43it/s]Loading hard negatives:  31%|███       | 154435/502939 [00:54<01:57, 2957.74it/s]Loading hard negatives:  31%|███       | 154739/502939 [00:54<01:56, 2981.46it/s]Loading hard negatives:  31%|███       | 155043/502939 [00:54<01:56, 2997.49it/s]Loading hard negatives:  31%|███       | 155346/502939 [00:54<01:55, 3004.52it/s]Loading hard negatives:  31%|███       | 155647/502939 [00:54<01:55, 3004.02it/s]Loading hard negatives:  31%|███       | 155948/502939 [00:54<01:57, 2946.40it/s]Loading hard negatives:  31%|███       | 156247/502939 [00:54<01:57, 2958.97it/s]Loading hard negatives:  31%|███       | 156553/502939 [00:54<01:55, 2987.15it/s]Loading hard negatives:  31%|███       | 156859/502939 [00:54<01:55, 3006.11it/s]Loading hard negatives:  31%|███       | 157165/502939 [00:55<01:54, 3021.23it/s]Loading hard negatives:  31%|███▏      | 157468/502939 [00:55<01:56, 2965.61it/s]Loading hard negatives:  31%|███▏      | 157773/502939 [00:55<01:55, 2989.13it/s]Loading hard negatives:  31%|███▏      | 158077/502939 [00:55<01:54, 3002.12it/s]Loading hard negatives:  31%|███▏      | 158378/502939 [00:55<01:55, 2986.30it/s]Loading hard negatives:  32%|███▏      | 158683/502939 [00:55<01:54, 3002.73it/s]Loading hard negatives:  32%|███▏      | 158984/502939 [00:55<01:56, 2944.82it/s]Loading hard negatives:  32%|███▏      | 159286/502939 [00:55<01:55, 2965.68it/s]Loading hard negatives:  32%|███▏      | 159590/502939 [00:55<01:54, 2986.08it/s]Loading hard negatives:  32%|███▏      | 159893/502939 [00:55<01:54, 2997.33it/s]Loading hard negatives:  32%|███▏      | 160193/502939 [00:56<01:54, 2989.79it/s]Loading hard negatives:  32%|███▏      | 160493/502939 [00:56<01:56, 2934.08it/s]Loading hard negatives:  32%|███▏      | 160789/502939 [00:56<01:56, 2939.92it/s]Loading hard negatives:  32%|███▏      | 161095/502939 [00:56<01:54, 2972.92it/s]Loading hard negatives:  32%|███▏      | 161399/502939 [00:56<01:54, 2991.02it/s]Loading hard negatives:  32%|███▏      | 161701/502939 [00:56<01:53, 2998.36it/s]Loading hard negatives:  32%|███▏      | 162001/502939 [00:56<01:56, 2933.20it/s]Loading hard negatives:  32%|███▏      | 162306/502939 [00:56<01:54, 2966.32it/s]Loading hard negatives:  32%|███▏      | 162604/502939 [00:56<01:54, 2968.84it/s]Loading hard negatives:  32%|███▏      | 162907/502939 [00:56<01:53, 2986.44it/s]Loading hard negatives:  32%|███▏      | 163206/502939 [00:57<01:53, 2983.48it/s]Loading hard negatives:  33%|███▎      | 163505/502939 [00:57<01:55, 2934.10it/s]Loading hard negatives:  33%|███▎      | 163808/502939 [00:57<01:54, 2961.06it/s]Loading hard negatives:  33%|███▎      | 164112/502939 [00:57<01:53, 2983.99it/s]Loading hard negatives:  33%|███▎      | 164417/502939 [00:57<01:52, 3002.16it/s]Loading hard negatives:  33%|███▎      | 164722/502939 [00:57<01:52, 3015.67it/s]Loading hard negatives:  33%|███▎      | 165024/502939 [00:57<01:54, 2952.90it/s]Loading hard negatives:  33%|███▎      | 165326/502939 [00:57<01:53, 2971.89it/s]Loading hard negatives:  33%|███▎      | 165627/502939 [00:57<01:53, 2982.95it/s]Loading hard negatives:  33%|███▎      | 165932/502939 [00:58<01:52, 3001.44it/s]Loading hard negatives:  33%|███▎      | 166234/502939 [00:58<01:51, 3006.45it/s]Loading hard negatives:  33%|███▎      | 166536/502939 [00:58<01:53, 2952.42it/s]Loading hard negatives:  33%|███▎      | 166841/502939 [00:58<01:52, 2980.15it/s]Loading hard negatives:  33%|███▎      | 167143/502939 [00:58<01:52, 2990.18it/s]Loading hard negatives:  33%|███▎      | 167448/502939 [00:58<01:51, 3006.69it/s]Loading hard negatives:  33%|███▎      | 167749/502939 [00:58<01:51, 3003.81it/s]Loading hard negatives:  33%|███▎      | 168051/502939 [00:58<01:51, 3006.75it/s]Loading hard negatives:  33%|███▎      | 168352/502939 [00:58<01:54, 2929.13it/s]Loading hard negatives:  34%|███▎      | 168656/502939 [00:58<01:52, 2959.67it/s]Loading hard negatives:  34%|███▎      | 168959/502939 [00:59<01:52, 2979.08it/s]Loading hard negatives:  34%|███▎      | 169265/502939 [00:59<01:51, 3001.77it/s]Loading hard negatives:  34%|███▎      | 169568/502939 [00:59<01:50, 3009.68it/s]Loading hard negatives:  34%|███▍      | 169870/502939 [00:59<01:52, 2954.29it/s]Loading hard negatives:  34%|███▍      | 170169/502939 [00:59<01:52, 2964.72it/s]Loading hard negatives:  34%|███▍      | 170475/502939 [00:59<01:51, 2992.41it/s]Loading hard negatives:  34%|███▍      | 170781/502939 [00:59<01:50, 3011.66it/s]Loading hard negatives:  34%|███▍      | 171083/502939 [00:59<01:51, 2975.30it/s]Loading hard negatives:  34%|███▍      | 171381/502939 [00:59<01:53, 2925.07it/s]Loading hard negatives:  34%|███▍      | 171686/502939 [00:59<01:51, 2960.09it/s]Loading hard negatives:  34%|███▍      | 171991/502939 [01:00<01:50, 2984.77it/s]Loading hard negatives:  34%|███▍      | 172294/502939 [01:00<01:50, 2995.65it/s]Loading hard negatives:  34%|███▍      | 172597/502939 [01:00<01:49, 3003.67it/s]Loading hard negatives:  34%|███▍      | 172898/502939 [01:00<01:52, 2944.07it/s]Loading hard negatives:  34%|███▍      | 173203/502939 [01:00<01:50, 2972.72it/s]Loading hard negatives:  34%|███▍      | 173507/502939 [01:00<01:50, 2990.27it/s]Loading hard negatives:  35%|███▍      | 173807/502939 [01:00<01:50, 2991.68it/s]Loading hard negatives:  35%|███▍      | 174107/502939 [01:00<01:51, 2954.52it/s]Loading hard negatives:  35%|███▍      | 174403/502939 [01:00<01:53, 2900.28it/s]Loading hard negatives:  35%|███▍      | 174709/502939 [01:00<01:51, 2944.62it/s]Loading hard negatives:  35%|███▍      | 175007/502939 [01:01<01:51, 2952.38it/s]Loading hard negatives:  35%|███▍      | 175311/502939 [01:01<01:50, 2976.21it/s]Loading hard negatives:  35%|███▍      | 175613/502939 [01:01<01:49, 2987.99it/s]Loading hard negatives:  35%|███▍      | 175912/502939 [01:01<01:51, 2935.73it/s]Loading hard negatives:  35%|███▌      | 176206/502939 [01:01<01:51, 2923.09it/s]Loading hard negatives:  35%|███▌      | 176508/502939 [01:01<01:50, 2950.11it/s]Loading hard negatives:  35%|███▌      | 176813/502939 [01:01<01:49, 2979.63it/s]Loading hard negatives:  35%|███▌      | 177113/502939 [01:01<01:49, 2983.29it/s]Loading hard negatives:  35%|███▌      | 177414/502939 [01:01<01:48, 2989.09it/s]Loading hard negatives:  35%|███▌      | 177713/502939 [01:01<01:51, 2928.48it/s]Loading hard negatives:  35%|███▌      | 178017/502939 [01:02<01:49, 2959.70it/s]Loading hard negatives:  35%|███▌      | 178324/502939 [01:02<01:48, 2990.64it/s]Loading hard negatives:  36%|███▌      | 178628/502939 [01:02<01:47, 3004.40it/s]Loading hard negatives:  36%|███▌      | 178929/502939 [01:02<01:48, 2994.19it/s]Loading hard negatives:  36%|███▌      | 179229/502939 [01:02<01:49, 2945.85it/s]Loading hard negatives:  36%|███▌      | 179527/502939 [01:02<01:49, 2954.54it/s]Loading hard negatives:  36%|███▌      | 179832/502939 [01:02<01:48, 2981.83it/s]Loading hard negatives:  36%|███▌      | 180131/502939 [01:02<01:48, 2979.00it/s]Loading hard negatives:  36%|███▌      | 180434/502939 [01:02<01:47, 2992.69it/s]Loading hard negatives:  36%|███▌      | 180734/502939 [01:02<01:49, 2930.57it/s]Loading hard negatives:  36%|███▌      | 181037/502939 [01:03<01:48, 2957.35it/s]Loading hard negatives:  36%|███▌      | 181342/502939 [01:03<01:47, 2982.12it/s]Loading hard negatives:  36%|███▌      | 181645/502939 [01:03<01:47, 2993.80it/s]Loading hard negatives:  36%|███▌      | 181945/502939 [01:03<01:47, 2980.81it/s]Loading hard negatives:  36%|███▌      | 182244/502939 [01:03<01:49, 2927.12it/s]Loading hard negatives:  36%|███▋      | 182547/502939 [01:03<01:48, 2954.80it/s]Loading hard negatives:  36%|███▋      | 182853/502939 [01:03<01:47, 2984.23it/s]Loading hard negatives:  36%|███▋      | 183157/502939 [01:03<01:46, 2999.56it/s]Loading hard negatives:  36%|███▋      | 183461/502939 [01:03<01:46, 3010.85it/s]Loading hard negatives:  37%|███▋      | 183763/502939 [01:04<01:48, 2951.73it/s]Loading hard negatives:  37%|███▋      | 184068/502939 [01:04<01:47, 2978.28it/s]Loading hard negatives:  37%|███▋      | 184367/502939 [01:04<01:47, 2955.56it/s]Loading hard negatives:  37%|███▋      | 184672/502939 [01:04<01:46, 2983.00it/s]Loading hard negatives:  37%|███▋      | 184982/502939 [01:04<01:45, 3017.28it/s]Loading hard negatives:  37%|███▋      | 185284/502939 [01:04<01:47, 2967.84it/s]Loading hard negatives:  37%|███▋      | 185590/502939 [01:04<01:46, 2992.86it/s]Loading hard negatives:  37%|███▋      | 185892/502939 [01:04<01:45, 3000.70it/s]Loading hard negatives:  37%|███▋      | 186195/502939 [01:04<01:45, 3007.23it/s]Loading hard negatives:  37%|███▋      | 186496/502939 [01:04<01:45, 2997.82it/s]Loading hard negatives:  37%|███▋      | 186796/502939 [01:05<01:47, 2933.92it/s]Loading hard negatives:  37%|███▋      | 187100/502939 [01:05<01:46, 2963.02it/s]Loading hard negatives:  37%|███▋      | 187405/502939 [01:05<01:45, 2986.44it/s]Loading hard negatives:  37%|███▋      | 187707/502939 [01:05<01:45, 2996.33it/s]Loading hard negatives:  37%|███▋      | 188009/502939 [01:05<01:44, 3002.04it/s]Loading hard negatives:  37%|███▋      | 188310/502939 [01:05<01:47, 2934.30it/s]Loading hard negatives:  38%|███▊      | 188612/502939 [01:05<01:46, 2959.12it/s]Loading hard negatives:  38%|███▊      | 188913/502939 [01:05<01:45, 2972.18it/s]Loading hard negatives:  38%|███▊      | 189219/502939 [01:05<01:44, 2996.56it/s]Loading hard negatives:  38%|███▊      | 189524/502939 [01:05<01:44, 3011.72it/s]Loading hard negatives:  38%|███▊      | 189826/502939 [01:06<01:46, 2950.59it/s]Loading hard negatives:  38%|███▊      | 190131/502939 [01:06<01:44, 2979.43it/s]Loading hard negatives:  38%|███▊      | 190437/502939 [01:06<01:44, 3002.61it/s]Loading hard negatives:  38%|███▊      | 190741/502939 [01:06<01:43, 3013.00it/s]Loading hard negatives:  38%|███▊      | 191045/502939 [01:06<01:43, 3019.52it/s]Loading hard negatives:  38%|███▊      | 191348/502939 [01:06<01:45, 2944.51it/s]Loading hard negatives:  38%|███▊      | 191651/502939 [01:06<01:44, 2968.73it/s]Loading hard negatives:  38%|███▊      | 191956/502939 [01:06<01:43, 2991.91it/s]Loading hard negatives:  38%|███▊      | 192259/502939 [01:06<01:43, 3003.18it/s]Loading hard negatives:  38%|███▊      | 192562/502939 [01:06<01:43, 3009.93it/s]Loading hard negatives:  38%|███▊      | 192864/502939 [01:07<01:44, 2955.16it/s]Loading hard negatives:  38%|███▊      | 193169/502939 [01:07<01:43, 2981.11it/s]Loading hard negatives:  38%|███▊      | 193469/502939 [01:07<01:43, 2986.43it/s]Loading hard negatives:  39%|███▊      | 193773/502939 [01:07<01:42, 3002.32it/s]Loading hard negatives:  39%|███▊      | 194078/502939 [01:07<01:42, 3014.57it/s]Loading hard negatives:  39%|███▊      | 194380/502939 [01:07<01:44, 2954.58it/s]Loading hard negatives:  39%|███▊      | 194682/502939 [01:07<01:43, 2971.30it/s]Loading hard negatives:  39%|███▉      | 194984/502939 [01:07<01:43, 2983.84it/s]Loading hard negatives:  39%|███▉      | 195285/502939 [01:07<01:42, 2990.65it/s]Loading hard negatives:  39%|███▉      | 195588/502939 [01:07<01:42, 3001.38it/s]Loading hard negatives:  39%|███▉      | 195889/502939 [01:08<01:42, 2994.66it/s]Loading hard negatives:  39%|███▉      | 196189/502939 [01:08<01:44, 2947.67it/s]Loading hard negatives:  39%|███▉      | 196494/502939 [01:08<01:42, 2977.28it/s]Loading hard negatives:  39%|███▉      | 196798/502939 [01:08<01:42, 2994.16it/s]Loading hard negatives:  39%|███▉      | 197103/502939 [01:08<01:41, 3009.67it/s]Loading hard negatives:  39%|███▉      | 197408/502939 [01:08<01:41, 3021.23it/s]Loading hard negatives:  39%|███▉      | 197711/502939 [01:08<01:43, 2956.86it/s]Loading hard negatives:  39%|███▉      | 198017/502939 [01:08<01:42, 2986.76it/s]Loading hard negatives:  39%|███▉      | 198318/502939 [01:08<01:41, 2991.95it/s]Loading hard negatives:  39%|███▉      | 198620/502939 [01:08<01:41, 2998.38it/s]Loading hard negatives:  40%|███▉      | 198925/502939 [01:09<01:40, 3013.47it/s]Loading hard negatives:  40%|███▉      | 199227/502939 [01:09<01:42, 2961.09it/s]Loading hard negatives:  40%|███▉      | 199530/502939 [01:09<01:41, 2979.59it/s]Loading hard negatives:  40%|███▉      | 199834/502939 [01:09<01:41, 2996.18it/s]Loading hard negatives:  40%|███▉      | 200137/502939 [01:09<01:40, 3004.78it/s]Loading hard negatives:  40%|███▉      | 200438/502939 [01:09<01:41, 2983.01it/s]Loading hard negatives:  40%|███▉      | 200737/502939 [01:09<01:43, 2919.81it/s]Loading hard negatives:  40%|███▉      | 201042/502939 [01:09<01:42, 2957.85it/s]Loading hard negatives:  40%|████      | 201345/502939 [01:09<01:41, 2978.82it/s]Loading hard negatives:  40%|████      | 201646/502939 [01:10<01:40, 2986.78it/s]Loading hard negatives:  40%|████      | 201947/502939 [01:10<01:40, 2991.75it/s]Loading hard negatives:  40%|████      | 202247/502939 [01:10<01:42, 2932.61it/s]Loading hard negatives:  40%|████      | 202557/502939 [01:10<01:40, 2980.87it/s]Loading hard negatives:  40%|████      | 202856/502939 [01:10<01:41, 2954.41it/s]Loading hard negatives:  40%|████      | 203161/502939 [01:10<01:40, 2980.97it/s]Loading hard negatives:  40%|████      | 203466/502939 [01:10<01:39, 3001.08it/s]Loading hard negatives:  41%|████      | 203767/502939 [01:10<01:41, 2944.93it/s]Loading hard negatives:  41%|████      | 204071/502939 [01:10<01:40, 2971.04it/s]Loading hard negatives:  41%|████      | 204374/502939 [01:10<01:39, 2987.74it/s]Loading hard negatives:  41%|████      | 204675/502939 [01:11<01:39, 2992.14it/s]Loading hard negatives:  41%|████      | 204977/502939 [01:11<01:39, 2999.70it/s]Loading hard negatives:  41%|████      | 205278/502939 [01:11<01:41, 2938.14it/s]Loading hard negatives:  41%|████      | 205573/502939 [01:12<05:01, 987.37it/s] Loading hard negatives:  41%|████      | 205876/502939 [01:12<03:59, 1239.81it/s]Loading hard negatives:  41%|████      | 206179/502939 [01:12<03:16, 1507.95it/s]Loading hard negatives:  41%|████      | 206485/502939 [01:12<02:46, 1781.48it/s]Loading hard negatives:  41%|████      | 206778/502939 [01:12<02:28, 1988.24it/s]Loading hard negatives:  41%|████      | 207053/502939 [01:12<02:17, 2149.88it/s]Loading hard negatives:  41%|████      | 207357/502939 [01:12<02:05, 2363.10it/s]Loading hard negatives:  41%|████▏     | 207657/502939 [01:12<01:56, 2524.16it/s]Loading hard negatives:  41%|████▏     | 207962/502939 [01:12<01:50, 2664.03it/s]Loading hard negatives:  41%|████▏     | 208268/502939 [01:12<01:46, 2771.66it/s]Loading hard negatives:  41%|████▏     | 208565/502939 [01:13<01:46, 2760.76it/s]Loading hard negatives:  42%|████▏     | 208868/502939 [01:13<01:43, 2835.63it/s]Loading hard negatives:  42%|████▏     | 209173/502939 [01:13<01:41, 2896.42it/s]Loading hard negatives:  42%|████▏     | 209475/502939 [01:13<01:40, 2930.75it/s]Loading hard negatives:  42%|████▏     | 209783/502939 [01:13<01:38, 2972.59it/s]Loading hard negatives:  42%|████▏     | 210084/502939 [01:13<01:39, 2940.88it/s]Loading hard negatives:  42%|████▏     | 210388/502939 [01:13<01:38, 2969.04it/s]Loading hard negatives:  42%|████▏     | 210694/502939 [01:13<01:37, 2993.37it/s]Loading hard negatives:  42%|████▏     | 211001/502939 [01:13<01:36, 3014.16it/s]Loading hard negatives:  42%|████▏     | 211308/502939 [01:13<01:36, 3029.12it/s]Loading hard negatives:  42%|████▏     | 211612/502939 [01:14<01:38, 2972.15it/s]Loading hard negatives:  42%|████▏     | 211918/502939 [01:14<01:37, 2995.91it/s]Loading hard negatives:  42%|████▏     | 212223/502939 [01:14<01:36, 3011.65it/s]Loading hard negatives:  42%|████▏     | 212525/502939 [01:14<01:36, 3004.65it/s]Loading hard negatives:  42%|████▏     | 212831/502939 [01:14<01:36, 3020.28it/s]Loading hard negatives:  42%|████▏     | 213134/502939 [01:14<01:37, 2967.04it/s]Loading hard negatives:  42%|████▏     | 213441/502939 [01:14<01:36, 2996.56it/s]Loading hard negatives:  42%|████▏     | 213749/502939 [01:14<01:35, 3018.63it/s]Loading hard negatives:  43%|████▎     | 214057/502939 [01:14<01:35, 3035.71it/s]Loading hard negatives:  43%|████▎     | 214363/502939 [01:14<01:34, 3040.95it/s]Loading hard negatives:  43%|████▎     | 214668/502939 [01:15<01:37, 2961.13it/s]Loading hard negatives:  43%|████▎     | 214973/502939 [01:15<01:36, 2986.90it/s]Loading hard negatives:  43%|████▎     | 215277/502939 [01:15<01:35, 3001.43it/s]Loading hard negatives:  43%|████▎     | 215579/502939 [01:15<01:35, 3005.03it/s]Loading hard negatives:  43%|████▎     | 215886/502939 [01:15<01:36, 2970.48it/s]Loading hard negatives:  43%|████▎     | 216189/502939 [01:15<01:35, 2987.76it/s]Loading hard negatives:  43%|████▎     | 216493/502939 [01:15<01:35, 3001.16it/s]Loading hard negatives:  43%|████▎     | 216799/502939 [01:15<01:34, 3015.93it/s]Loading hard negatives:  43%|████▎     | 217101/502939 [01:15<01:34, 3011.46it/s]Loading hard negatives:  43%|████▎     | 217403/502939 [01:15<01:36, 2948.00it/s]Loading hard negatives:  43%|████▎     | 217709/502939 [01:16<01:35, 2978.47it/s]Loading hard negatives:  43%|████▎     | 218013/502939 [01:16<01:35, 2994.26it/s]Loading hard negatives:  43%|████▎     | 218315/502939 [01:16<01:34, 3001.35it/s]Loading hard negatives:  43%|████▎     | 218616/502939 [01:16<01:35, 2975.32it/s]Loading hard negatives:  44%|████▎     | 218922/502939 [01:16<01:36, 2932.04it/s]Loading hard negatives:  44%|████▎     | 219228/502939 [01:16<01:35, 2968.32it/s]Loading hard negatives:  44%|████▎     | 219529/502939 [01:16<01:35, 2979.97it/s]Loading hard negatives:  44%|████▎     | 219833/502939 [01:16<01:34, 2996.50it/s]Loading hard negatives:  44%|████▍     | 220140/502939 [01:16<01:33, 3016.53it/s]Loading hard negatives:  44%|████▍     | 220442/502939 [01:16<01:35, 2961.15it/s]Loading hard negatives:  44%|████▍     | 220739/502939 [01:17<01:35, 2962.04it/s]Loading hard negatives:  44%|████▍     | 221042/502939 [01:17<01:34, 2982.05it/s]Loading hard negatives:  44%|████▍     | 221345/502939 [01:17<01:33, 2995.85it/s]Loading hard negatives:  44%|████▍     | 221652/502939 [01:17<01:33, 3016.18it/s]Loading hard negatives:  44%|████▍     | 221954/502939 [01:17<01:33, 3017.09it/s]Loading hard negatives:  44%|████▍     | 222256/502939 [01:17<01:34, 2965.91it/s]Loading hard negatives:  44%|████▍     | 222561/502939 [01:17<01:33, 2989.24it/s]Loading hard negatives:  44%|████▍     | 222868/502939 [01:17<01:32, 3011.72it/s]Loading hard negatives:  44%|████▍     | 223175/502939 [01:17<01:32, 3026.78it/s]Loading hard negatives:  44%|████▍     | 223478/502939 [01:18<01:34, 2959.40it/s]Loading hard negatives:  44%|████▍     | 223781/502939 [01:18<01:33, 2978.05it/s]Loading hard negatives:  45%|████▍     | 224080/502939 [01:18<01:33, 2973.96it/s]Loading hard negatives:  45%|████▍     | 224385/502939 [01:18<01:33, 2994.50it/s]Loading hard negatives:  45%|████▍     | 224691/502939 [01:18<01:32, 3013.50it/s]Loading hard negatives:  45%|████▍     | 224997/502939 [01:18<01:33, 2961.23it/s]Loading hard negatives:  45%|████▍     | 225302/502939 [01:18<01:32, 2985.87it/s]Loading hard negatives:  45%|████▍     | 225609/502939 [01:18<01:32, 3008.61it/s]Loading hard negatives:  45%|████▍     | 225911/502939 [01:18<01:31, 3011.79it/s]Loading hard negatives:  45%|████▍     | 226215/502939 [01:18<01:31, 3019.75it/s]Loading hard negatives:  45%|████▌     | 226518/502939 [01:19<01:33, 2950.79it/s]Loading hard negatives:  45%|████▌     | 226822/502939 [01:19<01:32, 2976.45it/s]Loading hard negatives:  45%|████▌     | 227127/502939 [01:19<01:32, 2997.45it/s]Loading hard negatives:  45%|████▌     | 227431/502939 [01:19<01:31, 3009.01it/s]Loading hard negatives:  45%|████▌     | 227735/502939 [01:19<01:31, 3017.16it/s]Loading hard negatives:  45%|████▌     | 228037/502939 [01:19<01:32, 2958.74it/s]Loading hard negatives:  45%|████▌     | 228341/502939 [01:19<01:32, 2980.20it/s]Loading hard negatives:  45%|████▌     | 228645/502939 [01:19<01:31, 2996.44it/s]Loading hard negatives:  46%|████▌     | 228946/502939 [01:19<01:31, 2998.73it/s]Loading hard negatives:  46%|████▌     | 229251/502939 [01:19<01:30, 3012.06it/s]Loading hard negatives:  46%|████▌     | 229553/502939 [01:20<01:32, 2955.34it/s]Loading hard negatives:  46%|████▌     | 229856/502939 [01:20<01:31, 2975.43it/s]Loading hard negatives:  46%|████▌     | 230160/502939 [01:20<01:31, 2992.76it/s]Loading hard negatives:  46%|████▌     | 230468/502939 [01:20<01:30, 3016.13it/s]Loading hard negatives:  46%|████▌     | 230773/502939 [01:20<01:29, 3025.58it/s]Loading hard negatives:  46%|████▌     | 231076/502939 [01:20<01:31, 2964.74it/s]Loading hard negatives:  46%|████▌     | 231379/502939 [01:20<01:31, 2982.60it/s]Loading hard negatives:  46%|████▌     | 231678/502939 [01:20<01:30, 2982.68it/s]Loading hard negatives:  46%|████▌     | 231977/502939 [01:20<01:31, 2954.41it/s]Loading hard negatives:  46%|████▌     | 232282/502939 [01:20<01:30, 2980.16it/s]Loading hard negatives:  46%|████▌     | 232585/502939 [01:21<01:32, 2929.61it/s]Loading hard negatives:  46%|████▋     | 232881/502939 [01:21<01:31, 2936.34it/s]Loading hard negatives:  46%|████▋     | 233181/502939 [01:21<01:31, 2954.49it/s]Loading hard negatives:  46%|████▋     | 233477/502939 [01:21<01:31, 2955.91it/s]Loading hard negatives:  46%|████▋     | 233781/502939 [01:21<01:30, 2978.90it/s]Loading hard negatives:  47%|████▋     | 234089/502939 [01:21<01:29, 3008.49it/s]Loading hard negatives:  47%|████▋     | 234390/502939 [01:21<01:30, 2964.41it/s]Loading hard negatives:  47%|████▋     | 234695/502939 [01:21<01:29, 2989.21it/s]Loading hard negatives:  47%|████▋     | 235003/502939 [01:21<01:28, 3013.94it/s]Loading hard negatives:  47%|████▋     | 235306/502939 [01:21<01:28, 3017.53it/s]Loading hard negatives:  47%|████▋     | 235610/502939 [01:22<01:28, 3021.92it/s]Loading hard negatives:  47%|████▋     | 235913/502939 [01:22<01:30, 2936.27it/s]Loading hard negatives:  47%|████▋     | 236216/502939 [01:22<01:30, 2962.66it/s]Loading hard negatives:  47%|████▋     | 236523/502939 [01:22<01:29, 2992.30it/s]Loading hard negatives:  47%|████▋     | 236830/502939 [01:22<01:28, 3014.62it/s]Loading hard negatives:  47%|████▋     | 237137/502939 [01:22<01:27, 3030.26it/s]Loading hard negatives:  47%|████▋     | 237441/502939 [01:22<01:29, 2969.76it/s]Loading hard negatives:  47%|████▋     | 237747/502939 [01:22<01:28, 2996.03it/s]Loading hard negatives:  47%|████▋     | 238054/502939 [01:22<01:27, 3015.51it/s]Loading hard negatives:  47%|████▋     | 238356/502939 [01:22<01:27, 3013.56it/s]Loading hard negatives:  47%|████▋     | 238658/502939 [01:23<01:29, 2962.00it/s]Loading hard negatives:  48%|████▊     | 238964/502939 [01:23<01:28, 2990.52it/s]Loading hard negatives:  48%|████▊     | 239270/502939 [01:23<01:27, 3011.01it/s]Loading hard negatives:  48%|████▊     | 239575/502939 [01:23<01:27, 3021.93it/s]Loading hard negatives:  48%|████▊     | 239882/502939 [01:23<01:26, 3035.86it/s]Loading hard negatives:  48%|████▊     | 240186/502939 [01:23<01:28, 2982.76it/s]Loading hard negatives:  48%|████▊     | 240488/502939 [01:23<01:27, 2992.45it/s]Loading hard negatives:  48%|████▊     | 240793/502939 [01:23<01:27, 3008.49it/s]Loading hard negatives:  48%|████▊     | 241100/502939 [01:23<01:26, 3024.04it/s]Loading hard negatives:  48%|████▊     | 241404/502939 [01:23<01:26, 3026.61it/s]Loading hard negatives:  48%|████▊     | 241707/502939 [01:24<01:27, 2974.39it/s]Loading hard negatives:  48%|████▊     | 242011/502939 [01:24<01:27, 2991.27it/s]Loading hard negatives:  48%|████▊     | 242317/502939 [01:24<01:26, 3010.02it/s]Loading hard negatives:  48%|████▊     | 242620/502939 [01:24<01:26, 3014.78it/s]Loading hard negatives:  48%|████▊     | 242922/502939 [01:24<01:26, 2993.89it/s]Loading hard negatives:  48%|████▊     | 243222/502939 [01:24<01:27, 2955.69it/s]Loading hard negatives:  48%|████▊     | 243529/502939 [01:24<01:26, 2987.67it/s]Loading hard negatives:  48%|████▊     | 243837/502939 [01:24<01:26, 3012.48it/s]Loading hard negatives:  49%|████▊     | 244144/502939 [01:24<01:25, 3027.16it/s]Loading hard negatives:  49%|████▊     | 244449/502939 [01:25<01:25, 3032.07it/s]Loading hard negatives:  49%|████▊     | 244753/502939 [01:25<01:26, 2978.91it/s]Loading hard negatives:  49%|████▊     | 245057/502939 [01:25<01:26, 2995.43it/s]Loading hard negatives:  49%|████▉     | 245360/502939 [01:25<01:25, 3004.55it/s]Loading hard negatives:  49%|████▉     | 245666/502939 [01:25<01:25, 3020.18it/s]Loading hard negatives:  49%|████▉     | 245972/502939 [01:25<01:24, 3031.84it/s]Loading hard negatives:  49%|████▉     | 246276/502939 [01:25<01:26, 2981.12it/s]Loading hard negatives:  49%|████▉     | 246582/502939 [01:25<01:25, 3004.05it/s]Loading hard negatives:  49%|████▉     | 246887/502939 [01:25<01:24, 3017.45it/s]Loading hard negatives:  49%|████▉     | 247190/502939 [01:25<01:24, 3018.82it/s]Loading hard negatives:  49%|████▉     | 247492/502939 [01:26<01:24, 3006.81it/s]Loading hard negatives:  49%|████▉     | 247793/502939 [01:26<01:26, 2953.06it/s]Loading hard negatives:  49%|████▉     | 248099/502939 [01:26<01:25, 2982.65it/s]Loading hard negatives:  49%|████▉     | 248405/502939 [01:26<01:24, 3004.51it/s]Loading hard negatives:  49%|████▉     | 248711/502939 [01:26<01:24, 3019.16it/s]Loading hard negatives:  50%|████▉     | 249016/502939 [01:26<01:23, 3027.36it/s]Loading hard negatives:  50%|████▉     | 249319/502939 [01:26<01:25, 2974.28it/s]Loading hard negatives:  50%|████▉     | 249623/502939 [01:26<01:24, 2992.60it/s]Loading hard negatives:  50%|████▉     | 249923/502939 [01:26<01:24, 2982.70it/s]Loading hard negatives:  50%|████▉     | 250229/502939 [01:26<01:24, 3004.71it/s]Loading hard negatives:  50%|████▉     | 250534/502939 [01:27<01:23, 3017.89it/s]Loading hard negatives:  50%|████▉     | 250836/502939 [01:27<01:24, 2977.45it/s]Loading hard negatives:  50%|████▉     | 251139/502939 [01:27<01:24, 2992.30it/s]Loading hard negatives:  50%|████▉     | 251443/502939 [01:27<01:23, 3003.75it/s]Loading hard negatives:  50%|█████     | 251750/502939 [01:27<01:23, 3022.18it/s]Loading hard negatives:  50%|█████     | 252056/502939 [01:27<01:22, 3033.10it/s]Loading hard negatives:  50%|█████     | 252360/502939 [01:27<01:24, 2963.76it/s]Loading hard negatives:  50%|█████     | 252663/502939 [01:27<01:23, 2981.15it/s]Loading hard negatives:  50%|█████     | 252969/502939 [01:27<01:23, 3003.01it/s]Loading hard negatives:  50%|█████     | 253276/502939 [01:27<01:22, 3021.71it/s]Loading hard negatives:  50%|█████     | 253579/502939 [01:28<01:22, 3021.60it/s]Loading hard negatives:  50%|█████     | 253882/502939 [01:28<01:24, 2962.46it/s]Loading hard negatives:  51%|█████     | 254187/502939 [01:28<01:23, 2988.10it/s]Loading hard negatives:  51%|█████     | 254488/502939 [01:28<01:23, 2992.80it/s]Loading hard negatives:  51%|█████     | 254793/502939 [01:28<01:22, 3007.91it/s]Loading hard negatives:  51%|█████     | 255098/502939 [01:28<01:22, 3019.29it/s]Loading hard negatives:  51%|█████     | 255401/502939 [01:28<01:23, 2958.82it/s]Loading hard negatives:  51%|█████     | 255707/502939 [01:28<01:22, 2985.90it/s]Loading hard negatives:  51%|█████     | 256012/502939 [01:28<01:22, 3004.32it/s]Loading hard negatives:  51%|█████     | 256317/502939 [01:28<01:21, 3017.57it/s]Loading hard negatives:  51%|█████     | 256624/502939 [01:29<01:23, 2964.42it/s]Loading hard negatives:  51%|█████     | 256922/502939 [01:29<01:22, 2968.68it/s]Loading hard negatives:  51%|█████     | 257220/502939 [01:29<01:22, 2969.29it/s]Loading hard negatives:  51%|█████     | 257525/502939 [01:29<01:22, 2992.78it/s]Loading hard negatives:  51%|█████▏    | 257834/502939 [01:29<01:21, 3018.82it/s]Loading hard negatives:  51%|█████▏    | 258141/502939 [01:29<01:22, 2973.36it/s]Loading hard negatives:  51%|█████▏    | 258447/502939 [01:29<01:21, 2997.21it/s]Loading hard negatives:  51%|█████▏    | 258753/502939 [01:29<01:20, 3015.23it/s]Loading hard negatives:  52%|█████▏    | 259059/502939 [01:29<01:20, 3027.70it/s]Loading hard negatives:  52%|█████▏    | 259362/502939 [01:29<01:20, 3027.30it/s]Loading hard negatives:  52%|█████▏    | 259665/502939 [01:30<01:21, 2971.02it/s]Loading hard negatives:  52%|█████▏    | 259972/502939 [01:30<01:21, 2997.71it/s]Loading hard negatives:  52%|█████▏    | 260274/502939 [01:30<01:20, 3004.26it/s]Loading hard negatives:  52%|█████▏    | 260580/502939 [01:30<01:20, 3019.14it/s]Loading hard negatives:  52%|█████▏    | 260884/502939 [01:30<01:20, 3023.46it/s]Loading hard negatives:  52%|█████▏    | 261187/502939 [01:30<01:21, 2963.27it/s]Loading hard negatives:  52%|█████▏    | 261487/502939 [01:30<01:21, 2972.34it/s]Loading hard negatives:  52%|█████▏    | 261790/502939 [01:30<01:20, 2988.40it/s]Loading hard negatives:  52%|█████▏    | 262091/502939 [01:30<01:20, 2994.29it/s]Loading hard negatives:  52%|█████▏    | 262397/502939 [01:30<01:19, 3012.86it/s]Loading hard negatives:  52%|█████▏    | 262699/502939 [01:31<01:21, 2961.95it/s]Loading hard negatives:  52%|█████▏    | 262996/502939 [01:31<01:21, 2959.91it/s]Loading hard negatives:  52%|█████▏    | 263300/502939 [01:31<01:20, 2982.80it/s]Loading hard negatives:  52%|█████▏    | 263601/502939 [01:31<01:20, 2988.78it/s]Loading hard negatives:  52%|█████▏    | 263900/502939 [01:31<01:20, 2980.30it/s]Loading hard negatives:  53%|█████▎    | 264206/502939 [01:31<01:19, 3002.56it/s]Loading hard negatives:  53%|█████▎    | 264507/502939 [01:31<01:20, 2953.51it/s]Loading hard negatives:  53%|█████▎    | 264807/502939 [01:31<01:20, 2966.41it/s]Loading hard negatives:  53%|█████▎    | 265112/502939 [01:31<01:19, 2989.25it/s]Loading hard negatives:  53%|█████▎    | 265417/502939 [01:32<01:18, 3006.88it/s]Loading hard negatives:  53%|█████▎    | 265722/502939 [01:32<01:18, 3018.08it/s]Loading hard negatives:  53%|█████▎    | 266024/502939 [01:32<01:20, 2941.31it/s]Loading hard negatives:  53%|█████▎    | 266319/502939 [01:32<01:20, 2938.29it/s]Loading hard negatives:  53%|█████▎    | 266625/502939 [01:32<01:19, 2971.71it/s]Loading hard negatives:  53%|█████▎    | 266930/502939 [01:32<01:18, 2994.14it/s]Loading hard negatives:  53%|█████▎    | 267234/502939 [01:32<01:18, 3006.67it/s]Loading hard negatives:  53%|█████▎    | 267535/502939 [01:32<01:19, 2951.69it/s]Loading hard negatives:  53%|█████▎    | 267840/502939 [01:32<01:18, 2978.47it/s]Loading hard negatives:  53%|█████▎    | 268145/502939 [01:32<01:18, 2998.51it/s]Loading hard negatives:  53%|█████▎    | 268446/502939 [01:33<01:18, 2993.89it/s]Loading hard negatives:  53%|█████▎    | 268753/502939 [01:33<01:17, 3014.26it/s]Loading hard negatives:  53%|█████▎    | 269055/502939 [01:33<01:18, 2964.13it/s]Loading hard negatives:  54%|█████▎    | 269354/502939 [01:33<01:18, 2971.61it/s]Loading hard negatives:  54%|█████▎    | 269659/502939 [01:33<01:17, 2992.68it/s]Loading hard negatives:  54%|█████▎    | 269967/502939 [01:33<01:17, 3018.52it/s]Loading hard negatives:  54%|█████▎    | 270273/502939 [01:33<01:16, 3030.38it/s]Loading hard negatives:  54%|█████▍    | 270577/502939 [01:33<01:18, 2969.72it/s]Loading hard negatives:  54%|█████▍    | 270880/502939 [01:33<01:17, 2986.42it/s]Loading hard negatives:  54%|█████▍    | 271182/502939 [01:33<01:17, 2994.21it/s]Loading hard negatives:  54%|█████▍    | 271486/502939 [01:34<01:16, 3006.28it/s]Loading hard negatives:  54%|█████▍    | 271793/502939 [01:34<01:16, 3024.20it/s]Loading hard negatives:  54%|█████▍    | 272096/502939 [01:34<01:17, 2971.62it/s]Loading hard negatives:  54%|█████▍    | 272397/502939 [01:34<01:17, 2981.21it/s]Loading hard negatives:  54%|█████▍    | 272705/502939 [01:34<01:16, 3009.94it/s]Loading hard negatives:  54%|█████▍    | 273012/502939 [01:34<01:15, 3026.53it/s]Loading hard negatives:  54%|█████▍    | 273316/502939 [01:34<01:15, 3028.25it/s]Loading hard negatives:  54%|█████▍    | 273619/502939 [01:34<01:17, 2977.44it/s]Loading hard negatives:  54%|█████▍    | 273924/502939 [01:34<01:16, 2998.62it/s]Loading hard negatives:  55%|█████▍    | 274231/502939 [01:34<01:15, 3017.35it/s]Loading hard negatives:  55%|█████▍    | 274536/502939 [01:35<01:15, 3025.05it/s]Loading hard negatives:  55%|█████▍    | 274840/502939 [01:35<01:16, 2967.06it/s]Loading hard negatives:  55%|█████▍    | 275146/502939 [01:35<01:16, 2991.84it/s]Loading hard negatives:  55%|█████▍    | 275448/502939 [01:35<01:15, 2998.82it/s]Loading hard negatives:  55%|█████▍    | 275756/502939 [01:35<01:15, 3020.67it/s]Loading hard negatives:  55%|█████▍    | 276064/502939 [01:35<01:14, 3037.00it/s]Loading hard negatives:  55%|█████▍    | 276368/502939 [01:35<01:16, 2981.07it/s]Loading hard negatives:  55%|█████▌    | 276673/502939 [01:35<01:15, 3001.33it/s]Loading hard negatives:  55%|█████▌    | 276978/502939 [01:35<01:14, 3015.34it/s]Loading hard negatives:  55%|█████▌    | 277280/502939 [01:35<01:14, 3014.92it/s]Loading hard negatives:  55%|█████▌    | 277586/502939 [01:36<01:14, 3026.71it/s]Loading hard negatives:  55%|█████▌    | 277889/502939 [01:36<01:16, 2958.38it/s]Loading hard negatives:  55%|█████▌    | 278195/502939 [01:36<01:15, 2986.21it/s]Loading hard negatives:  55%|█████▌    | 278501/502939 [01:36<01:14, 3005.54it/s]Loading hard negatives:  55%|█████▌    | 278806/502939 [01:36<01:14, 3016.91it/s]Loading hard negatives:  55%|█████▌    | 279112/502939 [01:36<01:13, 3027.68it/s]Loading hard negatives:  56%|█████▌    | 279415/502939 [01:36<01:15, 2975.54it/s]Loading hard negatives:  56%|█████▌    | 279720/502939 [01:36<01:14, 2997.42it/s]Loading hard negatives:  56%|█████▌    | 280024/502939 [01:36<01:14, 3009.85it/s]Loading hard negatives:  56%|█████▌    | 280326/502939 [01:37<04:23, 846.27it/s] Loading hard negatives:  56%|█████▌    | 280625/502939 [01:37<03:26, 1074.71it/s]Loading hard negatives:  56%|█████▌    | 280917/502939 [01:38<02:49, 1313.34it/s]Loading hard negatives:  56%|█████▌    | 281218/502939 [01:38<02:20, 1581.76it/s]Loading hard negatives:  56%|█████▌    | 281526/502939 [01:38<01:59, 1857.54it/s]Loading hard negatives:  56%|█████▌    | 281834/502939 [01:38<01:44, 2111.94it/s]Loading hard negatives:  56%|█████▌    | 282140/502939 [01:38<01:34, 2327.99it/s]Loading hard negatives:  56%|█████▌    | 282436/502939 [01:38<01:29, 2460.76it/s]Loading hard negatives:  56%|█████▌    | 282740/502939 [01:38<01:24, 2609.37it/s]Loading hard negatives:  56%|█████▋    | 283045/502939 [01:38<01:20, 2726.88it/s]Loading hard negatives:  56%|█████▋    | 283351/502939 [01:38<01:17, 2818.34it/s]Loading hard negatives:  56%|█████▋    | 283654/502939 [01:38<01:16, 2878.20it/s]Loading hard negatives:  56%|█████▋    | 283955/502939 [01:39<01:16, 2847.39it/s]Loading hard negatives:  57%|█████▋    | 284258/502939 [01:39<01:15, 2898.37it/s]Loading hard negatives:  57%|█████▋    | 284563/502939 [01:39<01:14, 2942.35it/s]Loading hard negatives:  57%|█████▋    | 284869/502939 [01:39<01:13, 2976.73it/s]Loading hard negatives:  57%|█████▋    | 285171/502939 [01:39<01:13, 2982.46it/s]Loading hard negatives:  57%|█████▋    | 285473/502939 [01:39<01:13, 2942.86it/s]Loading hard negatives:  57%|█████▋    | 285770/502939 [01:39<01:13, 2950.35it/s]Loading hard negatives:  57%|█████▋    | 286070/502939 [01:39<01:13, 2963.95it/s]Loading hard negatives:  57%|█████▋    | 286374/502939 [01:39<01:12, 2985.29it/s]Loading hard negatives:  57%|█████▋    | 286680/502939 [01:39<01:11, 3007.12it/s]Loading hard negatives:  57%|█████▋    | 286986/502939 [01:40<01:11, 3020.97it/s]Loading hard negatives:  57%|█████▋    | 287289/502939 [01:40<01:12, 2971.60it/s]Loading hard negatives:  57%|█████▋    | 287591/502939 [01:40<01:12, 2984.80it/s]Loading hard negatives:  57%|█████▋    | 287898/502939 [01:40<01:11, 3008.92it/s]Loading hard negatives:  57%|█████▋    | 288203/502939 [01:40<01:11, 3021.10it/s]Loading hard negatives:  57%|█████▋    | 288510/502939 [01:40<01:10, 3033.59it/s]Loading hard negatives:  57%|█████▋    | 288814/502939 [01:40<01:12, 2972.22it/s]Loading hard negatives:  57%|█████▋    | 289119/502939 [01:40<01:11, 2992.89it/s]Loading hard negatives:  58%|█████▊    | 289426/502939 [01:40<01:10, 3013.81it/s]Loading hard negatives:  58%|█████▊    | 289728/502939 [01:40<01:12, 2955.66it/s]Loading hard negatives:  58%|█████▊    | 290028/502939 [01:41<01:12, 2919.11it/s]Loading hard negatives:  58%|█████▊    | 290332/502939 [01:41<01:12, 2952.78it/s]Loading hard negatives:  58%|█████▊    | 290639/502939 [01:41<01:11, 2983.79it/s]Loading hard negatives:  58%|█████▊    | 290943/502939 [01:41<01:10, 3000.31it/s]Loading hard negatives:  58%|█████▊    | 291254/502939 [01:41<01:09, 3032.19it/s]Loading hard negatives:  58%|█████▊    | 291558/502939 [01:41<01:11, 2965.14it/s]Loading hard negatives:  58%|█████▊    | 291862/502939 [01:41<01:10, 2985.02it/s]Loading hard negatives:  58%|█████▊    | 292161/502939 [01:41<01:11, 2929.58it/s]Loading hard negatives:  58%|█████▊    | 292464/502939 [01:41<01:11, 2957.95it/s]Loading hard negatives:  58%|█████▊    | 292763/502939 [01:42<01:10, 2967.12it/s]Loading hard negatives:  58%|█████▊    | 293064/502939 [01:42<01:11, 2919.99it/s]Loading hard negatives:  58%|█████▊    | 293369/502939 [01:42<01:10, 2956.22it/s]Loading hard negatives:  58%|█████▊    | 293673/502939 [01:42<01:10, 2980.70it/s]Loading hard negatives:  58%|█████▊    | 293972/502939 [01:42<01:10, 2982.36it/s]Loading hard negatives:  59%|█████▊    | 294276/502939 [01:42<01:09, 2998.57it/s]Loading hard negatives:  59%|█████▊    | 294576/502939 [01:42<01:09, 2980.78it/s]Loading hard negatives:  59%|█████▊    | 294875/502939 [01:42<01:10, 2932.10it/s]Loading hard negatives:  59%|█████▊    | 295170/502939 [01:42<01:10, 2936.42it/s]Loading hard negatives:  59%|█████▊    | 295472/502939 [01:42<01:10, 2958.72it/s]Loading hard negatives:  59%|█████▉    | 295779/502939 [01:43<01:09, 2991.10it/s]Loading hard negatives:  59%|█████▉    | 296083/502939 [01:43<01:08, 3003.80it/s]Loading hard negatives:  59%|█████▉    | 296384/502939 [01:43<01:10, 2950.43it/s]Loading hard negatives:  59%|█████▉    | 296680/502939 [01:43<01:09, 2948.09it/s]Loading hard negatives:  59%|█████▉    | 296992/502939 [01:43<01:08, 2997.47it/s]Loading hard negatives:  59%|█████▉    | 297293/502939 [01:43<01:08, 2998.35it/s]Loading hard negatives:  59%|█████▉    | 297600/502939 [01:43<01:08, 3017.13it/s]Loading hard negatives:  59%|█████▉    | 297902/502939 [01:43<01:09, 2958.40it/s]Loading hard negatives:  59%|█████▉    | 298208/502939 [01:43<01:08, 2987.51it/s]Loading hard negatives:  59%|█████▉    | 298511/502939 [01:43<01:08, 2999.53it/s]Loading hard negatives:  59%|█████▉    | 298816/502939 [01:44<01:07, 3014.21it/s]Loading hard negatives:  59%|█████▉    | 299118/502939 [01:44<01:07, 2999.00it/s]Loading hard negatives:  60%|█████▉    | 299419/502939 [01:44<01:09, 2942.16it/s]Loading hard negatives:  60%|█████▉    | 299721/502939 [01:44<01:08, 2962.82it/s]Loading hard negatives:  60%|█████▉    | 300021/502939 [01:44<01:08, 2971.96it/s]Loading hard negatives:  60%|█████▉    | 300325/502939 [01:44<01:07, 2989.70it/s]Loading hard negatives:  60%|█████▉    | 300629/502939 [01:44<01:07, 3004.58it/s]Loading hard negatives:  60%|█████▉    | 300930/502939 [01:44<01:08, 2938.40it/s]Loading hard negatives:  60%|█████▉    | 301235/502939 [01:44<01:07, 2968.63it/s]Loading hard negatives:  60%|█████▉    | 301533/502939 [01:44<01:07, 2967.96it/s]Loading hard negatives:  60%|██████    | 301837/502939 [01:45<01:07, 2987.12it/s]Loading hard negatives:  60%|██████    | 302139/502939 [01:45<01:07, 2996.02it/s]Loading hard negatives:  60%|██████    | 302439/502939 [01:45<01:08, 2943.47it/s]Loading hard negatives:  60%|██████    | 302740/502939 [01:45<01:07, 2962.54it/s]Loading hard negatives:  60%|██████    | 303051/502939 [01:45<01:06, 3005.30it/s]Loading hard negatives:  60%|██████    | 303355/502939 [01:45<01:06, 3012.75it/s]Loading hard negatives:  60%|██████    | 303661/502939 [01:45<01:05, 3025.12it/s]Loading hard negatives:  60%|██████    | 303964/502939 [01:45<01:07, 2937.72it/s]Loading hard negatives:  60%|██████    | 304267/502939 [01:45<01:07, 2963.25it/s]Loading hard negatives:  61%|██████    | 304574/502939 [01:45<01:06, 2993.96it/s]Loading hard negatives:  61%|██████    | 304879/502939 [01:46<01:05, 3008.94it/s]Loading hard negatives:  61%|██████    | 305185/502939 [01:46<01:05, 3023.42it/s]Loading hard negatives:  61%|██████    | 305488/502939 [01:46<01:06, 2966.77it/s]Loading hard negatives:  61%|██████    | 305786/502939 [01:46<01:06, 2964.18it/s]Loading hard negatives:  61%|██████    | 306083/502939 [01:46<01:06, 2947.36it/s]Loading hard negatives:  61%|██████    | 306388/502939 [01:46<01:06, 2975.30it/s]Loading hard negatives:  61%|██████    | 306696/502939 [01:46<01:05, 3004.36it/s]Loading hard negatives:  61%|██████    | 306997/502939 [01:46<01:06, 2947.40it/s]Loading hard negatives:  61%|██████    | 307302/502939 [01:46<01:05, 2975.61it/s]Loading hard negatives:  61%|██████    | 307607/502939 [01:46<01:05, 2997.30it/s]Loading hard negatives:  61%|██████    | 307913/502939 [01:47<01:04, 3014.48it/s]Loading hard negatives:  61%|██████▏   | 308219/502939 [01:47<01:04, 3027.20it/s]Loading hard negatives:  61%|██████▏   | 308522/502939 [01:47<01:05, 2954.39it/s]Loading hard negatives:  61%|██████▏   | 308825/502939 [01:47<01:05, 2974.25it/s]Loading hard negatives:  61%|██████▏   | 309127/502939 [01:47<01:04, 2987.21it/s]Loading hard negatives:  62%|██████▏   | 309433/502939 [01:47<01:04, 3006.74it/s]Loading hard negatives:  62%|██████▏   | 309739/502939 [01:47<01:03, 3020.39it/s]Loading hard negatives:  62%|██████▏   | 310042/502939 [01:47<01:05, 2943.57it/s]Loading hard negatives:  62%|██████▏   | 310342/502939 [01:47<01:05, 2959.42it/s]Loading hard negatives:  62%|██████▏   | 310645/502939 [01:48<01:04, 2980.07it/s]Loading hard negatives:  62%|██████▏   | 310944/502939 [01:48<01:04, 2976.53it/s]Loading hard negatives:  62%|██████▏   | 311249/502939 [01:48<01:03, 2996.67it/s]Loading hard negatives:  62%|██████▏   | 311549/502939 [01:48<01:05, 2942.38it/s]Loading hard negatives:  62%|██████▏   | 311853/502939 [01:48<01:04, 2968.13it/s]Loading hard negatives:  62%|██████▏   | 312155/502939 [01:48<01:03, 2983.10it/s]Loading hard negatives:  62%|██████▏   | 312458/502939 [01:48<01:03, 2995.14it/s]Loading hard negatives:  62%|██████▏   | 312765/502939 [01:48<01:03, 3015.60it/s]Loading hard negatives:  62%|██████▏   | 313067/502939 [01:48<01:04, 2951.51it/s]Loading hard negatives:  62%|██████▏   | 313372/502939 [01:48<01:03, 2978.62it/s]Loading hard negatives:  62%|██████▏   | 313678/502939 [01:49<01:03, 3000.04it/s]Loading hard negatives:  62%|██████▏   | 313986/502939 [01:49<01:02, 3021.31it/s]Loading hard negatives:  62%|██████▏   | 314292/502939 [01:49<01:02, 3030.11it/s]Loading hard negatives:  63%|██████▎   | 314596/502939 [01:49<01:03, 2970.05it/s]Loading hard negatives:  63%|██████▎   | 314901/502939 [01:49<01:02, 2991.58it/s]Loading hard negatives:  63%|██████▎   | 315204/502939 [01:49<01:02, 3001.94it/s]Loading hard negatives:  63%|██████▎   | 315505/502939 [01:49<01:02, 2998.90it/s]Loading hard negatives:  63%|██████▎   | 315809/502939 [01:49<01:02, 3010.78it/s]Loading hard negatives:  63%|██████▎   | 316111/502939 [01:49<01:03, 2955.29it/s]Loading hard negatives:  63%|██████▎   | 316415/502939 [01:49<01:02, 2979.48it/s]Loading hard negatives:  63%|██████▎   | 316714/502939 [01:50<01:02, 2977.07it/s]Loading hard negatives:  63%|██████▎   | 317018/502939 [01:50<01:02, 2994.03it/s]Loading hard negatives:  63%|██████▎   | 317325/502939 [01:50<01:01, 3014.80it/s]Loading hard negatives:  63%|██████▎   | 317627/502939 [01:50<01:02, 2957.55it/s]Loading hard negatives:  63%|██████▎   | 317924/502939 [01:50<01:02, 2961.20it/s]Loading hard negatives:  63%|██████▎   | 318226/502939 [01:50<01:02, 2977.67it/s]Loading hard negatives:  63%|██████▎   | 318532/502939 [01:50<01:01, 3001.67it/s]Loading hard negatives:  63%|██████▎   | 318838/502939 [01:50<01:01, 3016.87it/s]Loading hard negatives:  63%|██████▎   | 319140/502939 [01:50<01:02, 2963.73it/s]Loading hard negatives:  64%|██████▎   | 319445/502939 [01:50<01:01, 2987.85it/s]Loading hard negatives:  64%|██████▎   | 319752/502939 [01:51<01:00, 3011.63it/s]Loading hard negatives:  64%|██████▎   | 320054/502939 [01:51<01:01, 2987.25it/s]Loading hard negatives:  64%|██████▎   | 320355/502939 [01:51<01:00, 2993.68it/s]Loading hard negatives:  64%|██████▍   | 320655/502939 [01:51<01:02, 2931.78it/s]Loading hard negatives:  64%|██████▍   | 320958/502939 [01:51<01:01, 2959.58it/s]Loading hard negatives:  64%|██████▍   | 321259/502939 [01:51<01:01, 2973.95it/s]Loading hard negatives:  64%|██████▍   | 321564/502939 [01:51<01:00, 2995.50it/s]Loading hard negatives:  64%|██████▍   | 321874/502939 [01:51<00:59, 3025.76it/s]Loading hard negatives:  64%|██████▍   | 322177/502939 [01:51<01:00, 2967.28it/s]Loading hard negatives:  64%|██████▍   | 322477/502939 [01:51<01:00, 2975.44it/s]Loading hard negatives:  64%|██████▍   | 322784/502939 [01:52<01:00, 3002.06it/s]Loading hard negatives:  64%|██████▍   | 323089/502939 [01:52<00:59, 3013.64it/s]Loading hard negatives:  64%|██████▍   | 323394/502939 [01:52<00:59, 3021.88it/s]Loading hard negatives:  64%|██████▍   | 323697/502939 [01:52<01:00, 2961.30it/s]Loading hard negatives:  64%|██████▍   | 323999/502939 [01:52<01:00, 2977.69it/s]Loading hard negatives:  64%|██████▍   | 324304/502939 [01:52<00:59, 2998.80it/s]Loading hard negatives:  65%|██████▍   | 324616/502939 [01:52<00:58, 3032.85it/s]Loading hard negatives:  65%|██████▍   | 324920/502939 [01:52<00:58, 3020.10it/s]Loading hard negatives:  65%|██████▍   | 325223/502939 [01:52<00:59, 2981.50it/s]Loading hard negatives:  65%|██████▍   | 325527/502939 [01:52<00:59, 2996.40it/s]Loading hard negatives:  65%|██████▍   | 325832/502939 [01:53<00:58, 3010.21it/s]Loading hard negatives:  65%|██████▍   | 326134/502939 [01:53<00:59, 2971.04it/s]Loading hard negatives:  65%|██████▍   | 326436/502939 [01:53<00:59, 2983.92it/s]Loading hard negatives:  65%|██████▍   | 326735/502939 [01:53<01:00, 2930.97it/s]Loading hard negatives:  65%|██████▌   | 327038/502939 [01:53<00:59, 2957.98it/s]Loading hard negatives:  65%|██████▌   | 327335/502939 [01:53<00:59, 2953.23it/s]Loading hard negatives:  65%|██████▌   | 327637/502939 [01:53<00:58, 2971.43it/s]Loading hard negatives:  65%|██████▌   | 327940/502939 [01:53<00:58, 2987.16it/s]Loading hard negatives:  65%|██████▌   | 328239/502939 [01:53<00:59, 2934.89it/s]Loading hard negatives:  65%|██████▌   | 328543/502939 [01:54<00:58, 2965.78it/s]Loading hard negatives:  65%|██████▌   | 328840/502939 [01:54<00:58, 2963.09it/s]Loading hard negatives:  65%|██████▌   | 329143/502939 [01:54<00:58, 2982.70it/s]Loading hard negatives:  66%|██████▌   | 329442/502939 [01:54<00:58, 2979.05it/s]Loading hard negatives:  66%|██████▌   | 329746/502939 [01:54<00:57, 2996.86it/s]Loading hard negatives:  66%|██████▌   | 330046/502939 [01:54<00:58, 2947.48it/s]Loading hard negatives:  66%|██████▌   | 330352/502939 [01:54<00:57, 2978.25it/s]Loading hard negatives:  66%|██████▌   | 330655/502939 [01:54<00:57, 2992.21it/s]Loading hard negatives:  66%|██████▌   | 330958/502939 [01:54<00:57, 3002.54it/s]Loading hard negatives:  66%|██████▌   | 331261/502939 [01:54<00:57, 3008.36it/s]Loading hard negatives:  66%|██████▌   | 331562/502939 [01:55<00:58, 2939.96it/s]Loading hard negatives:  66%|██████▌   | 331860/502939 [01:55<00:58, 2949.20it/s]Loading hard negatives:  66%|██████▌   | 332164/502939 [01:55<00:57, 2973.61it/s]Loading hard negatives:  66%|██████▌   | 332468/502939 [01:55<00:56, 2990.88it/s]Loading hard negatives:  66%|██████▌   | 332771/502939 [01:55<00:56, 3001.11it/s]Loading hard negatives:  66%|██████▌   | 333072/502939 [01:55<00:57, 2968.39it/s]Loading hard negatives:  66%|██████▋   | 333382/502939 [01:55<00:56, 3006.55it/s]Loading hard negatives:  66%|██████▋   | 333688/502939 [01:55<00:56, 3022.24it/s]Loading hard negatives:  66%|██████▋   | 333995/502939 [01:55<00:55, 3035.21it/s]Loading hard negatives:  66%|██████▋   | 334300/502939 [01:55<00:55, 3037.08it/s]Loading hard negatives:  67%|██████▋   | 334604/502939 [01:56<00:56, 2976.65it/s]Loading hard negatives:  67%|██████▋   | 334911/502939 [01:56<00:55, 3003.26it/s]Loading hard negatives:  67%|██████▋   | 335216/502939 [01:56<00:55, 3015.70it/s]Loading hard negatives:  67%|██████▋   | 335521/502939 [01:56<00:55, 3024.11it/s]Loading hard negatives:  67%|██████▋   | 335825/502939 [01:56<00:55, 3028.07it/s]Loading hard negatives:  67%|██████▋   | 336128/502939 [01:56<00:56, 2966.86it/s]Loading hard negatives:  67%|██████▋   | 336426/502939 [01:56<00:56, 2970.50it/s]Loading hard negatives:  67%|██████▋   | 336728/502939 [01:56<00:55, 2983.86it/s]Loading hard negatives:  67%|██████▋   | 337031/502939 [01:56<00:55, 2996.42it/s]Loading hard negatives:  67%|██████▋   | 337334/502939 [01:56<00:55, 3005.26it/s]Loading hard negatives:  67%|██████▋   | 337635/502939 [01:57<00:56, 2949.26it/s]Loading hard negatives:  67%|██████▋   | 337940/502939 [01:57<00:55, 2977.79it/s]Loading hard negatives:  67%|██████▋   | 338246/502939 [01:57<00:54, 3001.01it/s]Loading hard negatives:  67%|██████▋   | 338550/502939 [01:57<00:54, 3010.73it/s]Loading hard negatives:  67%|██████▋   | 338852/502939 [01:57<00:54, 2994.63it/s]Loading hard negatives:  67%|██████▋   | 339152/502939 [01:57<00:55, 2945.67it/s]Loading hard negatives:  67%|██████▋   | 339461/502939 [01:57<00:54, 2985.84it/s]Loading hard negatives:  68%|██████▊   | 339764/502939 [01:57<00:54, 2998.73it/s]Loading hard negatives:  68%|██████▊   | 340071/502939 [01:57<00:53, 3017.19it/s]Loading hard negatives:  68%|██████▊   | 340378/502939 [01:57<00:53, 3030.53it/s]Loading hard negatives:  68%|██████▊   | 340682/502939 [01:58<00:54, 2974.04it/s]Loading hard negatives:  68%|██████▊   | 340984/502939 [01:58<00:54, 2986.15it/s]Loading hard negatives:  68%|██████▊   | 341283/502939 [01:58<00:54, 2987.17it/s]Loading hard negatives:  68%|██████▊   | 341589/502939 [01:58<00:53, 3006.68it/s]Loading hard negatives:  68%|██████▊   | 341893/502939 [01:58<00:53, 3015.79it/s]Loading hard negatives:  68%|██████▊   | 342195/502939 [01:58<00:54, 2951.47it/s]Loading hard negatives:  68%|██████▊   | 342500/502939 [01:58<00:53, 2979.16it/s]Loading hard negatives:  68%|██████▊   | 342806/502939 [01:58<00:53, 3002.28it/s]Loading hard negatives:  68%|██████▊   | 343112/502939 [01:58<00:52, 3018.82it/s]Loading hard negatives:  68%|██████▊   | 343415/502939 [01:58<00:52, 3012.15it/s]Loading hard negatives:  68%|██████▊   | 343717/502939 [01:59<00:53, 2966.36it/s]Loading hard negatives:  68%|██████▊   | 344023/502939 [01:59<00:53, 2992.29it/s]Loading hard negatives:  68%|██████▊   | 344330/502939 [01:59<00:52, 3013.10it/s]Loading hard negatives:  69%|██████▊   | 344638/502939 [01:59<00:52, 3030.39it/s]Loading hard negatives:  69%|██████▊   | 344942/502939 [01:59<00:53, 2963.26it/s]Loading hard negatives:  69%|██████▊   | 345253/502939 [01:59<00:52, 3004.35it/s]Loading hard negatives:  69%|██████▊   | 345560/502939 [01:59<00:52, 3021.14it/s]Loading hard negatives:  69%|██████▉   | 345863/502939 [01:59<00:52, 3004.52it/s]Loading hard negatives:  69%|██████▉   | 346168/502939 [01:59<00:51, 3017.37it/s]Loading hard negatives:  69%|██████▉   | 346470/502939 [02:00<00:52, 2962.39it/s]Loading hard negatives:  69%|██████▉   | 346775/502939 [02:00<00:52, 2987.48it/s]Loading hard negatives:  69%|██████▉   | 347078/502939 [02:00<00:51, 2997.80it/s]Loading hard negatives:  69%|██████▉   | 347387/502939 [02:00<00:51, 3023.73it/s]Loading hard negatives:  69%|██████▉   | 347692/502939 [02:00<00:51, 3029.47it/s]Loading hard negatives:  69%|██████▉   | 347996/502939 [02:00<00:52, 2965.68it/s]Loading hard negatives:  69%|██████▉   | 348293/502939 [02:00<00:52, 2963.04it/s]Loading hard negatives:  69%|██████▉   | 348599/502939 [02:00<00:51, 2991.02it/s]Loading hard negatives:  69%|██████▉   | 348899/502939 [02:00<00:51, 2993.52it/s]Loading hard negatives:  69%|██████▉   | 349209/502939 [02:00<00:50, 3023.47it/s]Loading hard negatives:  69%|██████▉   | 349512/502939 [02:01<00:51, 2964.38it/s]Loading hard negatives:  70%|██████▉   | 349819/502939 [02:01<00:51, 2993.51it/s]Loading hard negatives:  70%|██████▉   | 350126/502939 [02:01<00:50, 3016.03it/s]Loading hard negatives:  70%|██████▉   | 350431/502939 [02:01<00:50, 3024.13it/s]Loading hard negatives:  70%|██████▉   | 350734/502939 [02:01<00:50, 3003.86it/s]Loading hard negatives:  70%|██████▉   | 351035/502939 [02:01<00:51, 2958.65it/s]Loading hard negatives:  70%|██████▉   | 351340/502939 [02:01<00:50, 2985.15it/s]Loading hard negatives:  70%|██████▉   | 351647/502939 [02:01<00:50, 3008.98it/s]Loading hard negatives:  70%|██████▉   | 351955/502939 [02:01<00:49, 3027.67it/s]Loading hard negatives:  70%|███████   | 352261/502939 [02:01<00:49, 3036.05it/s]Loading hard negatives:  70%|███████   | 352565/502939 [02:02<00:51, 2909.86it/s]Loading hard negatives:  70%|███████   | 352871/502939 [02:02<00:50, 2951.99it/s]Loading hard negatives:  70%|███████   | 353168/502939 [02:02<00:50, 2951.81it/s]Loading hard negatives:  70%|███████   | 353473/502939 [02:02<00:50, 2979.02it/s]Loading hard negatives:  70%|███████   | 353778/502939 [02:02<00:49, 2998.63it/s]Loading hard negatives:  70%|███████   | 354079/502939 [02:02<00:50, 2960.15it/s]Loading hard negatives:  70%|███████   | 354387/502939 [02:02<00:49, 2994.50it/s]Loading hard negatives:  71%|███████   | 354701/502939 [02:02<00:48, 3035.90it/s]Loading hard negatives:  71%|███████   | 355007/502939 [02:02<00:48, 3042.41it/s]Loading hard negatives:  71%|███████   | 355312/502939 [02:02<00:49, 2987.74it/s]Loading hard negatives:  71%|███████   | 355618/502939 [02:03<00:48, 3007.74it/s]Loading hard negatives:  71%|███████   | 355920/502939 [02:03<00:48, 3010.61it/s]Loading hard negatives:  71%|███████   | 356226/502939 [02:03<00:48, 3023.69it/s]Loading hard negatives:  71%|███████   | 356534/502939 [02:03<00:48, 3037.86it/s]Loading hard negatives:  71%|███████   | 356838/502939 [02:03<00:48, 2985.45it/s]Loading hard negatives:  71%|███████   | 357145/502939 [02:03<00:48, 3008.74it/s]Loading hard negatives:  71%|███████   | 357447/502939 [02:03<00:48, 2995.55it/s]Loading hard negatives:  71%|███████   | 357753/502939 [02:03<00:48, 3013.14it/s]Loading hard negatives:  71%|███████   | 358057/502939 [02:03<00:47, 3020.73it/s]Loading hard negatives:  71%|███████▏  | 358360/502939 [02:03<00:48, 2962.62it/s]Loading hard negatives:  71%|███████▏  | 358666/502939 [02:04<00:48, 2989.97it/s]Loading hard negatives:  71%|███████▏  | 358973/502939 [02:04<00:47, 3012.26it/s]Loading hard negatives:  71%|███████▏  | 359283/502939 [02:04<00:47, 3035.83it/s]Loading hard negatives:  71%|███████▏  | 359587/502939 [02:04<00:47, 3033.39it/s]Loading hard negatives:  72%|███████▏  | 359891/502939 [02:04<00:48, 2958.83it/s]Loading hard negatives:  72%|███████▏  | 360194/502939 [02:04<00:47, 2979.28it/s]Loading hard negatives:  72%|███████▏  | 360500/502939 [02:04<00:47, 3000.66it/s]Loading hard negatives:  72%|███████▏  | 360806/502939 [02:04<00:47, 3018.19it/s]Loading hard negatives:  72%|███████▏  | 361110/502939 [02:04<00:46, 3024.54it/s]Loading hard negatives:  72%|███████▏  | 361413/502939 [02:04<00:47, 2975.34it/s]Loading hard negatives:  72%|███████▏  | 361720/502939 [02:05<00:47, 3003.00it/s]Loading hard negatives:  72%|███████▏  | 362028/502939 [02:05<00:46, 3024.12it/s]Loading hard negatives:  72%|███████▏  | 362331/502939 [02:05<00:46, 3009.78it/s]Loading hard negatives:  72%|███████▏  | 362638/502939 [02:05<00:46, 3025.86it/s]Loading hard negatives:  72%|███████▏  | 362941/502939 [02:05<00:47, 2961.89it/s]Loading hard negatives:  72%|███████▏  | 363247/502939 [02:05<00:46, 2988.88it/s]Loading hard negatives:  72%|███████▏  | 363549/502939 [02:05<00:46, 2995.86it/s]Loading hard negatives:  72%|███████▏  | 363856/502939 [02:05<00:46, 3015.95it/s]Loading hard negatives:  72%|███████▏  | 364159/502939 [02:05<00:45, 3019.82it/s]Loading hard negatives:  72%|███████▏  | 364462/502939 [02:06<00:47, 2942.85it/s]Loading hard negatives:  73%|███████▎  | 364767/502939 [02:06<00:46, 2972.95it/s]Loading hard negatives:  73%|███████▎  | 365070/502939 [02:06<00:46, 2988.68it/s]Loading hard negatives:  73%|███████▎  | 365371/502939 [02:06<00:45, 2993.83it/s]Loading hard negatives:  73%|███████▎  | 365675/502939 [02:06<00:45, 3007.06it/s]Loading hard negatives:  73%|███████▎  | 365976/502939 [02:06<00:46, 2955.38it/s]Loading hard negatives:  73%|███████▎  | 366279/502939 [02:06<00:45, 2974.89it/s]Loading hard negatives:  73%|███████▎  | 366582/502939 [02:06<00:45, 2989.89it/s]Loading hard negatives:  73%|███████▎  | 366882/502939 [02:06<00:45, 2981.96it/s]Loading hard negatives:  73%|███████▎  | 367184/502939 [02:06<00:45, 2992.34it/s]Loading hard negatives:  73%|███████▎  | 367484/502939 [02:07<00:45, 2946.10it/s]Loading hard negatives:  73%|███████▎  | 367789/502939 [02:07<00:45, 2976.31it/s]Loading hard negatives:  73%|███████▎  | 368088/502939 [02:07<00:45, 2979.41it/s]Loading hard negatives:  73%|███████▎  | 368387/502939 [02:07<00:45, 2976.17it/s]Loading hard negatives:  73%|███████▎  | 368692/502939 [02:07<00:44, 2996.13it/s]Loading hard negatives:  73%|███████▎  | 368992/502939 [02:07<00:45, 2959.48it/s]Loading hard negatives:  73%|███████▎  | 369289/502939 [02:07<00:45, 2959.47it/s]Loading hard negatives:  73%|███████▎  | 369592/502939 [02:07<00:44, 2978.61it/s]Loading hard negatives:  74%|███████▎  | 369897/502939 [02:07<00:44, 2999.71it/s]Loading hard negatives:  74%|███████▎  | 370206/502939 [02:07<00:43, 3025.92it/s]Loading hard negatives:  74%|███████▎  | 370509/502939 [02:08<00:44, 2948.52it/s]Loading hard negatives:  74%|███████▎  | 370813/502939 [02:08<00:44, 2973.12it/s]Loading hard negatives:  74%|███████▍  | 371118/502939 [02:08<00:44, 2993.75it/s]Loading hard negatives:  74%|███████▍  | 371423/502939 [02:08<00:43, 3007.81it/s]Loading hard negatives:  74%|███████▍  | 371724/502939 [02:09<03:01, 723.93it/s] Loading hard negatives:  74%|███████▍  | 372003/502939 [02:09<02:23, 915.52it/s]Loading hard negatives:  74%|███████▍  | 372305/502939 [02:09<01:52, 1161.50it/s]Loading hard negatives:  74%|███████▍  | 372608/502939 [02:09<01:31, 1428.75it/s]Loading hard negatives:  74%|███████▍  | 372910/502939 [02:09<01:16, 1698.08it/s]Loading hard negatives:  74%|███████▍  | 373206/502939 [02:09<01:06, 1943.40it/s]Loading hard negatives:  74%|███████▍  | 373500/502939 [02:10<00:59, 2160.35it/s]Loading hard negatives:  74%|███████▍  | 373788/502939 [02:10<00:55, 2309.08it/s]Loading hard negatives:  74%|███████▍  | 374078/502939 [02:10<00:52, 2457.19it/s]Loading hard negatives:  74%|███████▍  | 374380/502939 [02:10<00:49, 2604.82it/s]Loading hard negatives:  74%|███████▍  | 374678/502939 [02:10<00:47, 2707.13it/s]Loading hard negatives:  75%|███████▍  | 374982/502939 [02:10<00:45, 2800.10it/s]Loading hard negatives:  75%|███████▍  | 375287/502939 [02:10<00:45, 2828.43it/s]Loading hard negatives:  75%|███████▍  | 375591/502939 [02:10<00:44, 2887.79it/s]Loading hard negatives:  75%|███████▍  | 375895/502939 [02:10<00:43, 2931.18it/s]Loading hard negatives:  75%|███████▍  | 376199/502939 [02:11<00:42, 2962.86it/s]Loading hard negatives:  75%|███████▍  | 376500/502939 [02:11<00:42, 2966.53it/s]Loading hard negatives:  75%|███████▍  | 376803/502939 [02:11<00:42, 2984.72it/s]Loading hard negatives:  75%|███████▍  | 377104/502939 [02:11<00:42, 2950.75it/s]Loading hard negatives:  75%|███████▌  | 377407/502939 [02:11<00:42, 2971.68it/s]Loading hard negatives:  75%|███████▌  | 377710/502939 [02:11<00:41, 2988.35it/s]Loading hard negatives:  75%|███████▌  | 378013/502939 [02:11<00:41, 2998.75it/s]Loading hard negatives:  75%|███████▌  | 378316/502939 [02:11<00:41, 3005.51it/s]Loading hard negatives:  75%|███████▌  | 378617/502939 [02:11<00:42, 2947.38it/s]Loading hard negatives:  75%|███████▌  | 378914/502939 [02:11<00:42, 2952.18it/s]Loading hard negatives:  75%|███████▌  | 379218/502939 [02:12<00:41, 2976.09it/s]Loading hard negatives:  75%|███████▌  | 379524/502939 [02:12<00:41, 2999.45it/s]Loading hard negatives:  76%|███████▌  | 379829/502939 [02:12<00:40, 3011.83it/s]Loading hard negatives:  76%|███████▌  | 380131/502939 [02:12<00:41, 2956.14it/s]Loading hard negatives:  76%|███████▌  | 380439/502939 [02:12<00:40, 2991.08it/s]Loading hard negatives:  76%|███████▌  | 380743/502939 [02:12<00:40, 3005.31it/s]Loading hard negatives:  76%|███████▌  | 381044/502939 [02:12<00:40, 2993.57it/s]Loading hard negatives:  76%|███████▌  | 381348/502939 [02:12<00:40, 3006.83it/s]Loading hard negatives:  76%|███████▌  | 381649/502939 [02:12<00:41, 2946.33it/s]Loading hard negatives:  76%|███████▌  | 381952/502939 [02:12<00:40, 2968.58it/s]Loading hard negatives:  76%|███████▌  | 382258/502939 [02:13<00:40, 2993.79it/s]Loading hard negatives:  76%|███████▌  | 382561/502939 [02:13<00:40, 3003.17it/s]Loading hard negatives:  76%|███████▌  | 382866/502939 [02:13<00:39, 3016.60it/s]Loading hard negatives:  76%|███████▌  | 383168/502939 [02:13<00:40, 2954.05it/s]Loading hard negatives:  76%|███████▌  | 383468/502939 [02:13<00:40, 2965.76it/s]Loading hard negatives:  76%|███████▋  | 383773/502939 [02:13<00:39, 2988.33it/s]Loading hard negatives:  76%|███████▋  | 384082/502939 [02:13<00:39, 3018.22it/s]Loading hard negatives:  76%|███████▋  | 384386/502939 [02:13<00:39, 3023.93it/s]Loading hard negatives:  76%|███████▋  | 384689/502939 [02:13<00:39, 2961.80it/s]Loading hard negatives:  77%|███████▋  | 384994/502939 [02:13<00:39, 2985.95it/s]Loading hard negatives:  77%|███████▋  | 385299/502939 [02:14<00:39, 3002.46it/s]Loading hard negatives:  77%|███████▋  | 385604/502939 [02:14<00:38, 3015.02it/s]Loading hard negatives:  77%|███████▋  | 385906/502939 [02:14<00:38, 3012.11it/s]Loading hard negatives:  77%|███████▋  | 386208/502939 [02:14<00:39, 2943.90it/s]Loading hard negatives:  77%|███████▋  | 386511/502939 [02:14<00:39, 2967.54it/s]Loading hard negatives:  77%|███████▋  | 386815/502939 [02:14<00:38, 2988.16it/s]Loading hard negatives:  77%|███████▋  | 387115/502939 [02:14<00:38, 2990.49it/s]Loading hard negatives:  77%|███████▋  | 387417/502939 [02:14<00:38, 2997.65it/s]Loading hard negatives:  77%|███████▋  | 387717/502939 [02:14<00:38, 2957.36it/s]Loading hard negatives:  77%|███████▋  | 388019/502939 [02:14<00:38, 2953.65it/s]Loading hard negatives:  77%|███████▋  | 388323/502939 [02:15<00:38, 2978.84it/s]Loading hard negatives:  77%|███████▋  | 388627/502939 [02:15<00:38, 2994.68it/s]Loading hard negatives:  77%|███████▋  | 388929/502939 [02:15<00:37, 3002.15it/s]Loading hard negatives:  77%|███████▋  | 389230/502939 [02:15<00:38, 2935.45it/s]Loading hard negatives:  77%|███████▋  | 389533/502939 [02:15<00:38, 2961.06it/s]Loading hard negatives:  78%|███████▊  | 389837/502939 [02:15<00:37, 2982.74it/s]Loading hard negatives:  78%|███████▊  | 390142/502939 [02:15<00:37, 3002.01it/s]Loading hard negatives:  78%|███████▊  | 390443/502939 [02:15<00:37, 2992.97it/s]Loading hard negatives:  78%|███████▊  | 390743/502939 [02:15<00:38, 2941.60it/s]Loading hard negatives:  78%|███████▊  | 391047/502939 [02:15<00:37, 2970.08it/s]Loading hard negatives:  78%|███████▊  | 391353/502939 [02:16<00:37, 2996.59it/s]Loading hard negatives:  78%|███████▊  | 391656/502939 [02:16<00:37, 3006.15it/s]Loading hard negatives:  78%|███████▊  | 391959/502939 [02:16<00:36, 3012.52it/s]Loading hard negatives:  78%|███████▊  | 392261/502939 [02:16<00:37, 2939.82it/s]Loading hard negatives:  78%|███████▊  | 392558/502939 [02:16<00:37, 2948.47it/s]Loading hard negatives:  78%|███████▊  | 392854/502939 [02:16<00:37, 2931.28it/s]Loading hard negatives:  78%|███████▊  | 393154/502939 [02:16<00:37, 2949.57it/s]Loading hard negatives:  78%|███████▊  | 393456/502939 [02:16<00:36, 2968.80it/s]Loading hard negatives:  78%|███████▊  | 393756/502939 [02:16<00:37, 2930.90it/s]Loading hard negatives:  78%|███████▊  | 394061/502939 [02:17<00:36, 2965.83it/s]Loading hard negatives:  78%|███████▊  | 394360/502939 [02:17<00:36, 2972.64it/s]Loading hard negatives:  78%|███████▊  | 394666/502939 [02:17<00:36, 2997.58it/s]Loading hard negatives:  79%|███████▊  | 394969/502939 [02:17<00:35, 3005.91it/s]Loading hard negatives:  79%|███████▊  | 395271/502939 [02:17<00:35, 3007.49it/s]Loading hard negatives:  79%|███████▊  | 395572/502939 [02:17<00:36, 2958.38it/s]Loading hard negatives:  79%|███████▊  | 395875/502939 [02:17<00:35, 2979.30it/s]Loading hard negatives:  79%|███████▉  | 396179/502939 [02:17<00:35, 2996.96it/s]Loading hard negatives:  79%|███████▉  | 396485/502939 [02:17<00:35, 3013.01it/s]Loading hard negatives:  79%|███████▉  | 396787/502939 [02:17<00:35, 3012.18it/s]Loading hard negatives:  79%|███████▉  | 397089/502939 [02:18<00:35, 2955.25it/s]Loading hard negatives:  79%|███████▉  | 397385/502939 [02:18<00:35, 2953.52it/s]Loading hard negatives:  79%|███████▉  | 397687/502939 [02:18<00:35, 2972.98it/s]Loading hard negatives:  79%|███████▉  | 397997/502939 [02:18<00:34, 3010.49it/s]Loading hard negatives:  79%|███████▉  | 398299/502939 [02:18<00:34, 3011.42it/s]Loading hard negatives:  79%|███████▉  | 398601/502939 [02:18<00:35, 2953.50it/s]Loading hard negatives:  79%|███████▉  | 398907/502939 [02:18<00:34, 2982.49it/s]Loading hard negatives:  79%|███████▉  | 399212/502939 [02:18<00:34, 3001.22it/s]Loading hard negatives:  79%|███████▉  | 399518/502939 [02:18<00:34, 3017.64it/s]Loading hard negatives:  79%|███████▉  | 399820/502939 [02:18<00:34, 3012.54it/s]Loading hard negatives:  80%|███████▉  | 400122/502939 [02:19<00:34, 2949.33it/s]Loading hard negatives:  80%|███████▉  | 400427/502939 [02:19<00:34, 2977.17it/s]Loading hard negatives:  80%|███████▉  | 400732/502939 [02:19<00:34, 2996.78it/s]Loading hard negatives:  80%|███████▉  | 401037/502939 [02:19<00:33, 3011.21it/s]Loading hard negatives:  80%|███████▉  | 401344/502939 [02:19<00:33, 3026.70it/s]Loading hard negatives:  80%|███████▉  | 401647/502939 [02:19<00:34, 2966.73it/s]Loading hard negatives:  80%|███████▉  | 401951/502939 [02:19<00:33, 2988.15it/s]Loading hard negatives:  80%|███████▉  | 402251/502939 [02:19<00:33, 2981.41it/s]Loading hard negatives:  80%|████████  | 402558/502939 [02:19<00:33, 3006.90it/s]Loading hard negatives:  80%|████████  | 402869/502939 [02:19<00:32, 3037.32it/s]Loading hard negatives:  80%|████████  | 403173/502939 [02:20<00:33, 2972.98it/s]Loading hard negatives:  80%|████████  | 403481/502939 [02:20<00:33, 3003.93it/s]Loading hard negatives:  80%|████████  | 403787/502939 [02:20<00:32, 3020.04it/s]Loading hard negatives:  80%|████████  | 404093/502939 [02:20<00:32, 3030.75it/s]Loading hard negatives:  80%|████████  | 404397/502939 [02:20<00:33, 2956.46it/s]Loading hard negatives:  80%|████████  | 404703/502939 [02:20<00:32, 2984.69it/s]Loading hard negatives:  81%|████████  | 405009/502939 [02:20<00:32, 3005.16it/s]Loading hard negatives:  81%|████████  | 405321/502939 [02:20<00:32, 3038.13it/s]Loading hard negatives:  81%|████████  | 405626/502939 [02:20<00:32, 3031.36it/s]Loading hard negatives:  81%|████████  | 405930/502939 [02:20<00:32, 2974.07it/s]Loading hard negatives:  81%|████████  | 406237/502939 [02:21<00:32, 2999.71it/s]Loading hard negatives:  81%|████████  | 406540/502939 [02:21<00:32, 3006.59it/s]Loading hard negatives:  81%|████████  | 406841/502939 [02:21<00:32, 2998.22it/s]Loading hard negatives:  81%|████████  | 407147/502939 [02:21<00:31, 3015.34it/s]Loading hard negatives:  81%|████████  | 407449/502939 [02:21<00:32, 2976.79it/s]Loading hard negatives:  81%|████████  | 407755/502939 [02:21<00:31, 3000.02it/s]Loading hard negatives:  81%|████████  | 408062/502939 [02:21<00:31, 3020.56it/s]Loading hard negatives:  81%|████████  | 408373/502939 [02:21<00:31, 3046.16it/s]Loading hard negatives:  81%|████████▏ | 408678/502939 [02:21<00:30, 3043.41it/s]Loading hard negatives:  81%|████████▏ | 408983/502939 [02:21<00:31, 2976.43it/s]Loading hard negatives:  81%|████████▏ | 409282/502939 [02:22<00:31, 2974.49it/s]Loading hard negatives:  81%|████████▏ | 409587/502939 [02:22<00:31, 2995.56it/s]Loading hard negatives:  81%|████████▏ | 409895/502939 [02:22<00:30, 3018.72it/s]Loading hard negatives:  82%|████████▏ | 410203/502939 [02:22<00:30, 3036.23it/s]Loading hard negatives:  82%|████████▏ | 410507/502939 [02:22<00:31, 2954.98it/s]Loading hard negatives:  82%|████████▏ | 410814/502939 [02:22<00:30, 2986.98it/s]Loading hard negatives:  82%|████████▏ | 411122/502939 [02:22<00:30, 3013.72it/s]Loading hard negatives:  82%|████████▏ | 411424/502939 [02:22<00:30, 3007.74it/s]Loading hard negatives:  82%|████████▏ | 411727/502939 [02:22<00:30, 2963.96it/s]Loading hard negatives:  82%|████████▏ | 412031/502939 [02:22<00:30, 2984.30it/s]Loading hard negatives:  82%|████████▏ | 412335/502939 [02:23<00:30, 2997.67it/s]Loading hard negatives:  82%|████████▏ | 412637/502939 [02:23<00:30, 3004.04it/s]Loading hard negatives:  82%|████████▏ | 412941/502939 [02:23<00:29, 3013.05it/s]Loading hard negatives:  82%|████████▏ | 413243/502939 [02:23<00:29, 3008.98it/s]Loading hard negatives:  82%|████████▏ | 413544/502939 [02:23<00:30, 2950.42it/s]Loading hard negatives:  82%|████████▏ | 413843/502939 [02:23<00:30, 2959.38it/s]Loading hard negatives:  82%|████████▏ | 414150/502939 [02:23<00:29, 2991.35it/s]Loading hard negatives:  82%|████████▏ | 414456/502939 [02:23<00:29, 3010.91it/s]Loading hard negatives:  82%|████████▏ | 414762/502939 [02:23<00:29, 2963.44it/s]Loading hard negatives:  83%|████████▎ | 415068/502939 [02:24<00:29, 2990.58it/s]Loading hard negatives:  83%|████████▎ | 415370/502939 [02:24<00:29, 2997.85it/s]Loading hard negatives:  83%|████████▎ | 415676/502939 [02:24<00:28, 3014.60it/s]Loading hard negatives:  83%|████████▎ | 415980/502939 [02:24<00:28, 3020.86it/s]Loading hard negatives:  83%|████████▎ | 416283/502939 [02:24<00:29, 2939.87it/s]Loading hard negatives:  83%|████████▎ | 416590/502939 [02:24<00:29, 2976.32it/s]Loading hard negatives:  83%|████████▎ | 416895/502939 [02:24<00:28, 2995.36it/s]Loading hard negatives:  83%|████████▎ | 417199/502939 [02:24<00:28, 3007.05it/s]Loading hard negatives:  83%|████████▎ | 417503/502939 [02:24<00:28, 3015.58it/s]Loading hard negatives:  83%|████████▎ | 417805/502939 [02:24<00:28, 2970.24it/s]Loading hard negatives:  83%|████████▎ | 418108/502939 [02:25<00:28, 2985.72it/s]Loading hard negatives:  83%|████████▎ | 418407/502939 [02:25<00:28, 2979.49it/s]Loading hard negatives:  83%|████████▎ | 418712/502939 [02:25<00:28, 2999.26it/s]Loading hard negatives:  83%|████████▎ | 419016/502939 [02:25<00:27, 3009.74it/s]Loading hard negatives:  83%|████████▎ | 419318/502939 [02:25<00:28, 2957.20it/s]Loading hard negatives:  83%|████████▎ | 419622/502939 [02:25<00:27, 2980.08it/s]Loading hard negatives:  83%|████████▎ | 419924/502939 [02:25<00:27, 2991.09it/s]Loading hard negatives:  84%|████████▎ | 420228/502939 [02:25<00:27, 3004.86it/s]Loading hard negatives:  84%|████████▎ | 420533/502939 [02:25<00:27, 3017.12it/s]Loading hard negatives:  84%|████████▎ | 420835/502939 [02:25<00:27, 3008.03it/s]Loading hard negatives:  84%|████████▎ | 421136/502939 [02:26<00:27, 2948.30it/s]Loading hard negatives:  84%|████████▍ | 421440/502939 [02:26<00:27, 2972.90it/s]Loading hard negatives:  84%|████████▍ | 421739/502939 [02:26<00:27, 2975.73it/s]Loading hard negatives:  84%|████████▍ | 422037/502939 [02:26<00:27, 2961.26it/s]Loading hard negatives:  84%|████████▍ | 422334/502939 [02:26<00:27, 2946.61it/s]Loading hard negatives:  84%|████████▍ | 422629/502939 [02:26<00:28, 2863.02it/s]Loading hard negatives:  84%|████████▍ | 422933/502939 [02:26<00:27, 2912.97it/s]Loading hard negatives:  84%|████████▍ | 423233/502939 [02:26<00:27, 2936.80it/s]Loading hard negatives:  84%|████████▍ | 423539/502939 [02:26<00:26, 2970.65it/s]Loading hard negatives:  84%|████████▍ | 423843/502939 [02:26<00:26, 2989.27it/s]Loading hard negatives:  84%|████████▍ | 424143/502939 [02:27<00:26, 2951.85it/s]Loading hard negatives:  84%|████████▍ | 424448/502939 [02:27<00:26, 2978.00it/s]Loading hard negatives:  84%|████████▍ | 424753/502939 [02:27<00:26, 2999.13it/s]Loading hard negatives:  85%|████████▍ | 425056/502939 [02:27<00:25, 3007.21it/s]Loading hard negatives:  85%|████████▍ | 425359/502939 [02:27<00:25, 3013.41it/s]Loading hard negatives:  85%|████████▍ | 425661/502939 [02:27<00:26, 2944.90it/s]Loading hard negatives:  85%|████████▍ | 425965/502939 [02:27<00:25, 2972.20it/s]Loading hard negatives:  85%|████████▍ | 426269/502939 [02:27<00:25, 2991.61it/s]Loading hard negatives:  85%|████████▍ | 426574/502939 [02:27<00:25, 3007.69it/s]Loading hard negatives:  85%|████████▍ | 426880/502939 [02:27<00:25, 3021.04it/s]Loading hard negatives:  85%|████████▍ | 427183/502939 [02:28<00:25, 2959.97it/s]Loading hard negatives:  85%|████████▍ | 427486/502939 [02:28<00:25, 2979.46it/s]Loading hard negatives:  85%|████████▌ | 427785/502939 [02:28<00:25, 2971.53it/s]Loading hard negatives:  85%|████████▌ | 428087/502939 [02:28<00:25, 2984.69it/s]Loading hard negatives:  85%|████████▌ | 428393/502939 [02:28<00:24, 3006.81it/s]Loading hard negatives:  85%|████████▌ | 428694/502939 [02:28<00:25, 2956.76it/s]Loading hard negatives:  85%|████████▌ | 428998/502939 [02:28<00:24, 2980.06it/s]Loading hard negatives:  85%|████████▌ | 429301/502939 [02:28<00:24, 2992.62it/s]Loading hard negatives:  85%|████████▌ | 429605/502939 [02:28<00:24, 3004.93it/s]Loading hard negatives:  85%|████████▌ | 429911/502939 [02:28<00:24, 3019.93it/s]Loading hard negatives:  86%|████████▌ | 430214/502939 [02:29<00:24, 2948.61it/s]Loading hard negatives:  86%|████████▌ | 430519/502939 [02:29<00:24, 2976.70it/s]Loading hard negatives:  86%|████████▌ | 430822/502939 [02:29<00:24, 2991.40it/s]Loading hard negatives:  86%|████████▌ | 431122/502939 [02:29<00:23, 2993.46it/s]Loading hard negatives:  86%|████████▌ | 431428/502939 [02:29<00:23, 3012.35it/s]Loading hard negatives:  86%|████████▌ | 431730/502939 [02:29<00:23, 2969.58it/s]Loading hard negatives:  86%|████████▌ | 432035/502939 [02:29<00:23, 2992.37it/s]Loading hard negatives:  86%|████████▌ | 432342/502939 [02:29<00:23, 3013.20it/s]Loading hard negatives:  86%|████████▌ | 432644/502939 [02:29<00:23, 3007.14it/s]Loading hard negatives:  86%|████████▌ | 432951/502939 [02:30<00:23, 3024.84it/s]Loading hard negatives:  86%|████████▌ | 433254/502939 [02:30<00:23, 2966.28it/s]Loading hard negatives:  86%|████████▌ | 433559/502939 [02:30<00:23, 2988.88it/s]Loading hard negatives:  86%|████████▋ | 433866/502939 [02:30<00:22, 3011.56it/s]Loading hard negatives:  86%|████████▋ | 434173/502939 [02:30<00:22, 3028.72it/s]Loading hard negatives:  86%|████████▋ | 434483/502939 [02:30<00:22, 3047.65it/s]Loading hard negatives:  86%|████████▋ | 434788/502939 [02:30<00:23, 2956.98it/s]Loading hard negatives:  87%|████████▋ | 435100/502939 [02:30<00:22, 3003.28it/s]Loading hard negatives:  87%|████████▋ | 435401/502939 [02:30<00:22, 3004.52it/s]Loading hard negatives:  87%|████████▋ | 435704/502939 [02:30<00:22, 3011.10it/s]Loading hard negatives:  87%|████████▋ | 436008/502939 [02:31<00:22, 3017.85it/s]Loading hard negatives:  87%|████████▋ | 436310/502939 [02:31<00:22, 2964.89it/s]Loading hard negatives:  87%|████████▋ | 436615/502939 [02:31<00:22, 2987.55it/s]Loading hard negatives:  87%|████████▋ | 436918/502939 [02:31<00:22, 2998.92it/s]Loading hard negatives:  87%|████████▋ | 437219/502939 [02:31<00:22, 2986.50it/s]Loading hard negatives:  87%|████████▋ | 437523/502939 [02:31<00:21, 3002.00it/s]Loading hard negatives:  87%|████████▋ | 437824/502939 [02:31<00:22, 2942.90it/s]Loading hard negatives:  87%|████████▋ | 438127/502939 [02:31<00:21, 2966.10it/s]Loading hard negatives:  87%|████████▋ | 438433/502939 [02:31<00:21, 2993.14it/s]Loading hard negatives:  87%|████████▋ | 438733/502939 [02:31<00:21, 2991.85it/s]Loading hard negatives:  87%|████████▋ | 439038/502939 [02:32<00:21, 3007.74it/s]Loading hard negatives:  87%|████████▋ | 439339/502939 [02:32<00:21, 2950.56it/s]Loading hard negatives:  87%|████████▋ | 439635/502939 [02:32<00:21, 2949.26it/s]Loading hard negatives:  87%|████████▋ | 439940/502939 [02:32<00:21, 2978.45it/s]Loading hard negatives:  88%|████████▊ | 440246/502939 [02:32<00:20, 3002.00it/s]Loading hard negatives:  88%|████████▊ | 440552/502939 [02:32<00:20, 3018.08it/s]Loading hard negatives:  88%|████████▊ | 440854/502939 [02:32<00:20, 2975.69it/s]Loading hard negatives:  88%|████████▊ | 441159/502939 [02:32<00:20, 2995.83it/s]Loading hard negatives:  88%|████████▊ | 441464/502939 [02:32<00:20, 3011.31it/s]Loading hard negatives:  88%|████████▊ | 441770/502939 [02:32<00:20, 3025.18it/s]Loading hard negatives:  88%|████████▊ | 442073/502939 [02:33<00:20, 3005.43it/s]Loading hard negatives:  88%|████████▊ | 442374/502939 [02:33<00:20, 2952.68it/s]Loading hard negatives:  88%|████████▊ | 442680/502939 [02:33<00:20, 2982.00it/s]Loading hard negatives:  88%|████████▊ | 442987/502939 [02:33<00:19, 3007.04it/s]Loading hard negatives:  88%|████████▊ | 443291/502939 [02:33<00:19, 3014.55it/s]Loading hard negatives:  88%|████████▊ | 443604/502939 [02:33<00:19, 3047.49it/s]Loading hard negatives:  88%|████████▊ | 443909/502939 [02:33<00:19, 2975.33it/s]Loading hard negatives:  88%|████████▊ | 444207/502939 [02:33<00:19, 2968.29it/s]Loading hard negatives:  88%|████████▊ | 444511/502939 [02:33<00:19, 2988.88it/s]Loading hard negatives:  88%|████████▊ | 444816/502939 [02:33<00:19, 3004.57it/s]Loading hard negatives:  89%|████████▊ | 445121/502939 [02:34<00:19, 3017.63it/s]Loading hard negatives:  89%|████████▊ | 445423/502939 [02:34<00:19, 2955.28it/s]Loading hard negatives:  89%|████████▊ | 445725/502939 [02:34<00:19, 2973.05it/s]Loading hard negatives:  89%|████████▊ | 446030/502939 [02:34<00:19, 2993.54it/s]Loading hard negatives:  89%|████████▊ | 446335/502939 [02:34<00:18, 3008.08it/s]Loading hard negatives:  89%|████████▉ | 446636/502939 [02:34<00:18, 3000.31it/s]Loading hard negatives:  89%|████████▉ | 446937/502939 [02:34<00:19, 2942.95it/s]Loading hard negatives:  89%|████████▉ | 447238/502939 [02:34<00:18, 2961.17it/s]Loading hard negatives:  89%|████████▉ | 447539/502939 [02:34<00:18, 2974.96it/s]Loading hard negatives:  89%|████████▉ | 447843/502939 [02:34<00:18, 2992.95it/s]Loading hard negatives:  89%|████████▉ | 448150/502939 [02:35<00:18, 3015.30it/s]Loading hard negatives:  89%|████████▉ | 448452/502939 [02:35<00:18, 2961.04it/s]Loading hard negatives:  89%|████████▉ | 448753/502939 [02:35<00:18, 2973.52it/s]Loading hard negatives:  89%|████████▉ | 449051/502939 [02:35<00:18, 2915.81it/s]Loading hard negatives:  89%|████████▉ | 449355/502939 [02:35<00:18, 2950.25it/s]Loading hard negatives:  89%|████████▉ | 449658/502939 [02:35<00:17, 2972.90it/s]Loading hard negatives:  89%|████████▉ | 449956/502939 [02:35<00:18, 2933.41it/s]Loading hard negatives:  90%|████████▉ | 450250/502939 [02:35<00:18, 2903.27it/s]Loading hard negatives:  90%|████████▉ | 450553/502939 [02:35<00:17, 2940.40it/s]Loading hard negatives:  90%|████████▉ | 450856/502939 [02:36<00:17, 2965.65it/s]Loading hard negatives:  90%|████████▉ | 451153/502939 [02:36<00:17, 2959.04it/s]Loading hard negatives:  90%|████████▉ | 451450/502939 [02:36<00:17, 2945.21it/s]Loading hard negatives:  90%|████████▉ | 451745/502939 [02:36<00:17, 2902.47it/s]Loading hard negatives:  90%|████████▉ | 452048/502939 [02:36<00:17, 2938.05it/s]Loading hard negatives:  90%|████████▉ | 452354/502939 [02:36<00:17, 2971.78it/s]Loading hard negatives:  90%|█████████ | 452658/502939 [02:36<00:16, 2990.98it/s]Loading hard negatives:  90%|█████████ | 452964/502939 [02:36<00:16, 3009.41it/s]Loading hard negatives:  90%|█████████ | 453266/502939 [02:36<00:16, 2949.17it/s]Loading hard negatives:  90%|█████████ | 453562/502939 [02:36<00:16, 2944.13it/s]Loading hard negatives:  90%|█████████ | 453857/502939 [02:37<00:16, 2918.18it/s]Loading hard negatives:  90%|█████████ | 454159/502939 [02:37<00:16, 2946.58it/s]Loading hard negatives:  90%|█████████ | 454464/502939 [02:37<00:16, 2975.77it/s]Loading hard negatives:  90%|█████████ | 454762/502939 [02:37<00:16, 2925.49it/s]Loading hard negatives:  90%|█████████ | 455071/502939 [02:37<00:16, 2971.50it/s]Loading hard negatives:  91%|█████████ | 455378/502939 [02:37<00:15, 2999.13it/s]Loading hard negatives:  91%|█████████ | 455679/502939 [02:37<00:15, 3000.04it/s]Loading hard negatives:  91%|█████████ | 455980/502939 [02:37<00:16, 2932.59it/s]Loading hard negatives:  91%|█████████ | 456274/502939 [02:37<00:16, 2898.05it/s]Loading hard negatives:  91%|█████████ | 456576/502939 [02:37<00:15, 2933.47it/s]Loading hard negatives:  91%|█████████ | 456880/502939 [02:38<00:15, 2963.95it/s]Loading hard negatives:  91%|█████████ | 457181/502939 [02:38<00:15, 2976.66it/s]Loading hard negatives:  91%|█████████ | 457485/502939 [02:38<00:15, 2994.94it/s]Loading hard negatives:  91%|█████████ | 457787/502939 [02:38<00:15, 2941.53it/s]Loading hard negatives:  91%|█████████ | 458088/502939 [02:38<00:15, 2960.17it/s]Loading hard negatives:  91%|█████████ | 458385/502939 [02:38<00:15, 2961.53it/s]Loading hard negatives:  91%|█████████ | 458682/502939 [02:38<00:14, 2958.11it/s]Loading hard negatives:  91%|█████████▏| 458986/502939 [02:38<00:14, 2980.76it/s]Loading hard negatives:  91%|█████████▏| 459289/502939 [02:38<00:14, 2993.61it/s]Loading hard negatives:  91%|█████████▏| 459589/502939 [02:38<00:14, 2950.18it/s]Loading hard negatives:  91%|█████████▏| 459895/502939 [02:39<00:14, 2981.88it/s]Loading hard negatives:  92%|█████████▏| 460202/502939 [02:39<00:14, 3007.82it/s]Loading hard negatives:  92%|█████████▏| 460503/502939 [02:39<00:14, 3001.37it/s]Loading hard negatives:  92%|█████████▏| 460808/502939 [02:39<00:13, 3015.08it/s]Loading hard negatives:  92%|█████████▏| 461110/502939 [02:39<00:14, 2939.72it/s]Loading hard negatives:  92%|█████████▏| 461410/502939 [02:39<00:14, 2955.82it/s]Loading hard negatives:  92%|█████████▏| 461713/502939 [02:39<00:13, 2976.37it/s]Loading hard negatives:  92%|█████████▏| 462018/502939 [02:39<00:13, 2998.05it/s]Loading hard negatives:  92%|█████████▏| 462323/502939 [02:39<00:13, 3011.55it/s]Loading hard negatives:  92%|█████████▏| 462625/502939 [02:39<00:13, 2949.47it/s]Loading hard negatives:  92%|█████████▏| 462921/502939 [02:40<00:13, 2949.78it/s]Loading hard negatives:  92%|█████████▏| 463225/502939 [02:40<00:13, 2974.32it/s]Loading hard negatives:  92%|█████████▏| 463531/502939 [02:40<00:13, 2997.83it/s]Loading hard negatives:  92%|█████████▏| 463831/502939 [02:40<00:13, 2960.16it/s]Loading hard negatives:  92%|█████████▏| 464128/502939 [02:40<00:13, 2923.02it/s]Loading hard negatives:  92%|█████████▏| 464432/502939 [02:40<00:13, 2956.86it/s]Loading hard negatives:  92%|█████████▏| 464738/502939 [02:40<00:12, 2985.18it/s]Loading hard negatives:  92%|█████████▏| 465037/502939 [02:40<00:12, 2970.99it/s]Loading hard negatives:  93%|█████████▎| 465339/502939 [02:40<00:12, 2983.62it/s]Loading hard negatives:  93%|█████████▎| 465638/502939 [02:40<00:12, 2923.66it/s]Loading hard negatives:  93%|█████████▎| 465938/502939 [02:41<00:12, 2943.52it/s]Loading hard negatives:  93%|█████████▎| 466244/502939 [02:41<00:12, 2975.43it/s]Loading hard negatives:  93%|█████████▎| 466550/502939 [02:41<00:12, 2998.55it/s]Loading hard negatives:  93%|█████████▎| 466857/502939 [02:41<00:11, 3017.60it/s]Loading hard negatives:  93%|█████████▎| 467159/502939 [02:41<00:11, 2994.76it/s]Loading hard negatives:  93%|█████████▎| 467465/502939 [02:41<00:11, 3013.55it/s]Loading hard negatives:  93%|█████████▎| 467767/502939 [02:41<00:11, 3003.46it/s]Loading hard negatives:  93%|█████████▎| 468069/502939 [02:41<00:11, 3008.10it/s]Loading hard negatives:  93%|█████████▎| 468371/502939 [02:41<00:11, 3009.78it/s]Loading hard negatives:  93%|█████████▎| 468673/502939 [02:42<00:11, 2947.90it/s]Loading hard negatives:  93%|█████████▎| 468983/502939 [02:42<00:11, 2991.70it/s]Loading hard negatives:  93%|█████████▎| 469285/502939 [02:42<00:11, 3000.00it/s]Loading hard negatives:  93%|█████████▎| 469586/502939 [02:42<00:11, 2998.40it/s]Loading hard negatives:  93%|█████████▎| 469886/502939 [02:42<00:11, 2982.02it/s]Loading hard negatives:  93%|█████████▎| 470185/502939 [02:42<00:11, 2931.98it/s]Loading hard negatives:  94%|█████████▎| 470491/502939 [02:42<00:10, 2968.48it/s]Loading hard negatives:  94%|█████████▎| 470789/502939 [02:42<00:10, 2966.92it/s]Loading hard negatives:  94%|█████████▎| 471095/502939 [02:42<00:10, 2994.05it/s]Loading hard negatives:  94%|█████████▎| 471402/502939 [02:42<00:10, 3015.57it/s]Loading hard negatives:  94%|█████████▍| 471704/502939 [02:43<00:10, 2961.87it/s]Loading hard negatives:  94%|█████████▍| 472010/502939 [02:43<00:10, 2990.23it/s]Loading hard negatives:  94%|█████████▍| 472313/502939 [02:43<00:10, 3000.95it/s]Loading hard negatives:  94%|█████████▍| 472619/502939 [02:43<00:10, 3017.46it/s]Loading hard negatives:  94%|█████████▍| 472924/502939 [02:43<00:09, 3024.45it/s]Loading hard negatives:  94%|█████████▍| 473227/502939 [02:43<00:10, 2968.23it/s]Loading hard negatives:  94%|█████████▍| 473525/502939 [02:43<00:09, 2961.03it/s]Loading hard negatives:  94%|█████████▍| 473822/502939 [02:43<00:09, 2941.65it/s]Loading hard negatives:  94%|█████████▍| 474124/502939 [02:43<00:09, 2962.32it/s]Loading hard negatives:  94%|█████████▍| 474423/502939 [02:43<00:09, 2969.14it/s]Loading hard negatives:  94%|█████████▍| 474721/502939 [02:44<00:09, 2960.69it/s]Loading hard negatives:  94%|█████████▍| 475018/502939 [02:44<00:09, 2914.13it/s]Loading hard negatives:  95%|█████████▍| 475323/502939 [02:44<00:09, 2951.74it/s]Loading hard negatives:  95%|█████████▍| 475626/502939 [02:44<00:09, 2974.26it/s]Loading hard negatives:  95%|█████████▍| 475933/502939 [02:44<00:08, 3001.34it/s]Loading hard negatives:  95%|█████████▍| 476235/502939 [02:44<00:08, 3005.93it/s]Loading hard negatives:  95%|█████████▍| 476536/502939 [02:44<00:08, 2949.03it/s]Loading hard negatives:  95%|█████████▍| 476837/502939 [02:44<00:08, 2965.26it/s]Loading hard negatives:  95%|█████████▍| 477143/502939 [02:44<00:08, 2991.97it/s]Loading hard negatives:  95%|█████████▍| 477447/502939 [02:44<00:08, 3003.89it/s]Loading hard negatives:  95%|█████████▍| 477752/502939 [02:45<00:08, 3016.01it/s]Loading hard negatives:  95%|█████████▌| 478054/502939 [02:45<00:08, 2956.30it/s]Loading hard negatives:  95%|█████████▌| 478358/502939 [02:45<00:08, 2979.98it/s]Loading hard negatives:  95%|█████████▌| 478662/502939 [02:45<00:08, 2995.89it/s]Loading hard negatives:  95%|█████████▌| 478965/502939 [02:45<00:07, 3005.58it/s]Loading hard negatives:  95%|█████████▌| 479266/502939 [02:45<00:07, 2996.50it/s]Loading hard negatives:  95%|█████████▌| 479566/502939 [02:45<00:07, 2941.85it/s]Loading hard negatives:  95%|█████████▌| 479871/502939 [02:45<00:07, 2971.22it/s]Loading hard negatives:  95%|█████████▌| 480182/502939 [02:45<00:07, 3011.31it/s]Loading hard negatives:  96%|█████████▌| 480486/502939 [02:45<00:07, 3019.63it/s]Loading hard negatives:  96%|█████████▌| 480789/502939 [02:46<00:07, 3019.46it/s]Loading hard negatives:  96%|█████████▌| 481092/502939 [02:46<00:07, 2973.89it/s]Loading hard negatives:  96%|█████████▌| 481396/502939 [02:46<00:07, 2991.02it/s]Loading hard negatives:  96%|█████████▌| 481696/502939 [02:46<00:07, 2991.00it/s]Loading hard negatives:  96%|█████████▌| 482001/502939 [02:46<00:06, 3006.84it/s]Loading hard negatives:  96%|█████████▌| 482302/502939 [02:46<00:06, 2990.38it/s]Loading hard negatives:  96%|█████████▌| 482602/502939 [02:46<00:06, 2914.03it/s]Loading hard negatives:  96%|█████████▌| 482906/502939 [02:46<00:06, 2948.46it/s]Loading hard negatives:  96%|█████████▌| 483211/502939 [02:46<00:06, 2977.56it/s]Loading hard negatives:  96%|█████████▌| 483517/502939 [02:46<00:06, 2999.56it/s]Loading hard negatives:  96%|█████████▌| 483823/502939 [02:47<00:06, 3017.42it/s]Loading hard negatives:  96%|█████████▋| 484125/502939 [02:47<00:06, 2949.62it/s]Loading hard negatives:  96%|█████████▋| 484430/502939 [02:47<00:06, 2977.28it/s]Loading hard negatives:  96%|█████████▋| 484737/502939 [02:47<00:06, 3003.16it/s]Loading hard negatives:  96%|█████████▋| 485048/502939 [02:47<00:05, 3033.10it/s]Loading hard negatives:  97%|█████████▋| 485354/502939 [02:47<00:05, 3040.43it/s]Loading hard negatives:  97%|█████████▋| 485659/502939 [02:47<00:05, 2978.14it/s]Loading hard negatives:  97%|█████████▋| 485966/502939 [02:47<00:05, 3004.25it/s]Loading hard negatives:  97%|█████████▋| 486267/502939 [02:47<00:05, 3001.71it/s]Loading hard negatives:  97%|█████████▋| 486568/502939 [02:49<00:26, 616.66it/s] Loading hard negatives:  97%|█████████▋| 486869/502939 [02:49<00:19, 808.22it/s]Loading hard negatives:  97%|█████████▋| 487148/502939 [02:49<00:15, 1011.61it/s]Loading hard negatives:  97%|█████████▋| 487453/502939 [02:49<00:12, 1272.22it/s]Loading hard negatives:  97%|█████████▋| 487761/502939 [02:49<00:09, 1550.97it/s]Loading hard negatives:  97%|█████████▋| 488065/502939 [02:49<00:08, 1819.24it/s]Loading hard negatives:  97%|█████████▋| 488370/502939 [02:49<00:07, 2071.11it/s]Loading hard negatives:  97%|█████████▋| 488663/502939 [02:50<00:06, 2245.14it/s]Loading hard negatives:  97%|█████████▋| 488964/502939 [02:50<00:05, 2430.41it/s]Loading hard negatives:  97%|█████████▋| 489273/502939 [02:50<00:05, 2598.89it/s]Loading hard negatives:  97%|█████████▋| 489575/502939 [02:50<00:04, 2710.55it/s]Loading hard negatives:  97%|█████████▋| 489878/502939 [02:50<00:04, 2798.04it/s]Loading hard negatives:  97%|█████████▋| 490178/502939 [02:50<00:04, 2804.06it/s]Loading hard negatives:  98%|█████████▊| 490482/502939 [02:50<00:04, 2871.14it/s]Loading hard negatives:  98%|█████████▊| 490783/502939 [02:50<00:04, 2909.15it/s]Loading hard negatives:  98%|█████████▊| 491082/502939 [02:50<00:04, 2926.11it/s]Loading hard negatives:  98%|█████████▊| 491387/502939 [02:50<00:03, 2960.92it/s]Loading hard negatives:  98%|█████████▊| 491691/502939 [02:51<00:03, 2931.27it/s]Loading hard negatives:  98%|█████████▊| 491992/502939 [02:51<00:03, 2953.43it/s]Loading hard negatives:  98%|█████████▊| 492297/502939 [02:51<00:03, 2981.10it/s]Loading hard negatives:  98%|█████████▊| 492602/502939 [02:51<00:03, 3001.20it/s]Loading hard negatives:  98%|█████████▊| 492907/502939 [02:51<00:03, 3013.71it/s]Loading hard negatives:  98%|█████████▊| 493210/502939 [02:51<00:03, 2968.64it/s]Loading hard negatives:  98%|█████████▊| 493513/502939 [02:51<00:03, 2985.85it/s]Loading hard negatives:  98%|█████████▊| 493819/502939 [02:51<00:03, 3007.13it/s]Loading hard negatives:  98%|█████████▊| 494127/502939 [02:51<00:02, 3027.54it/s]Loading hard negatives:  98%|█████████▊| 494436/502939 [02:51<00:02, 3045.63it/s]Loading hard negatives:  98%|█████████▊| 494741/502939 [02:52<00:02, 2985.34it/s]Loading hard negatives:  98%|█████████▊| 495046/502939 [02:52<00:02, 3001.58it/s]Loading hard negatives:  98%|█████████▊| 495351/502939 [02:52<00:02, 3015.31it/s]Loading hard negatives:  99%|█████████▊| 495656/502939 [02:52<00:02, 3023.78it/s]Loading hard negatives:  99%|█████████▊| 495959/502939 [02:52<00:02, 3014.77it/s]Loading hard negatives:  99%|█████████▊| 496261/502939 [02:52<00:02, 2944.98it/s]Loading hard negatives:  99%|█████████▊| 496566/502939 [02:52<00:02, 2973.41it/s]Loading hard negatives:  99%|█████████▉| 496872/502939 [02:52<00:02, 2997.83it/s]Loading hard negatives:  99%|█████████▉| 497177/502939 [02:52<00:01, 3012.35it/s]Loading hard negatives:  99%|█████████▉| 497481/502939 [02:52<00:01, 3018.68it/s]Loading hard negatives:  99%|█████████▉| 497784/502939 [02:53<00:01, 2956.86it/s]Loading hard negatives:  99%|█████████▉| 498081/502939 [02:53<00:01, 2956.64it/s]Loading hard negatives:  99%|█████████▉| 498384/502939 [02:53<00:01, 2978.14it/s]Loading hard negatives:  99%|█████████▉| 498685/502939 [02:53<00:01, 2986.78it/s]Loading hard negatives:  99%|█████████▉| 498989/502939 [02:53<00:01, 3001.22it/s]Loading hard negatives:  99%|█████████▉| 499290/502939 [02:53<00:01, 2970.95it/s]Loading hard negatives:  99%|█████████▉| 499595/502939 [02:53<00:01, 2992.52it/s]Loading hard negatives:  99%|█████████▉| 499898/502939 [02:53<00:01, 3003.60it/s]Loading hard negatives:  99%|█████████▉| 500199/502939 [02:53<00:00, 3003.76it/s]Loading hard negatives: 100%|█████████▉| 500500/502939 [02:53<00:00, 3005.03it/s]Loading hard negatives: 100%|█████████▉| 500802/502939 [02:54<00:00, 3007.27it/s]Loading hard negatives: 100%|█████████▉| 501103/502939 [02:54<00:00, 2948.61it/s]Loading hard negatives: 100%|█████████▉| 501407/502939 [02:54<00:00, 2974.87it/s]Loading hard negatives: 100%|█████████▉| 501708/502939 [02:54<00:00, 2984.31it/s]Loading hard negatives: 100%|█████████▉| 502010/502939 [02:54<00:00, 2992.92it/s]Loading hard negatives: 100%|█████████▉| 502312/502939 [02:54<00:00, 2998.44it/s]Loading hard negatives: 100%|█████████▉| 502612/502939 [02:54<00:00, 2946.73it/s]Loading hard negatives: 100%|█████████▉| 502920/502939 [02:54<00:00, 2983.91it/s]Loading hard negatives: 100%|██████████| 502939/502939 [02:54<00:00, 2877.49it/s]
2025-05-12 01:37:50.546105: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1747013870.567362  252925 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1747013870.574902  252925 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-12 01:37:40 - Prepared 498970 queries with hard negatives
2025-05-12 01:37:46 - Created dataloader with 498970 samples
2025-05-12 01:37:46 - Initializing shared MultiLayerReranker model for all layers
2025-05-12 01:37:46 - Using device: cuda with dtype: torch.float16
2025-05-12 01:37:46 - Using GPU-accelerated FastDTWEmbeddingSimilarity
2025-05-12 01:37:46 - Loading tokenizer: meta-llama/Llama-2-7b-hf
2025-05-12 01:37:46 - Set pad_token to eos_token since it was not defined
2025-05-12 01:37:46 - Loading model: meta-llama/Llama-2-7b-hf
2025-05-12 01:37:52 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]
2025-05-12 01:37:55 - Initializing weighters with vocab size: 32000
2025-05-12 01:37:55 - Model loaded successfully with weight_normalization=linear, weighting_mode=full
2025-05-12 01:37:55 - Successfully loaded shared model with layers: [0, 3, 6, 9, 12, 15, 18, 21]
2025-05-12 01:37:55 - Creating models for all configurations...
2025-05-12 01:37:55 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:55 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:55 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:55 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:55 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:55 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:55 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:55 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:55 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:55 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:55 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:55 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:56 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:56 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:56 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:57 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:57 - Initializing token weights from token_frequency_data/llama2_token_freq_weights.pkl using log_weights
2025-05-12 01:37:57 - Loaded 31569 token weights ('log_weights') from token_frequency_data/llama2_token_freq_weights.pkl
2025-05-12 01:37:58 - Initialized 31569 token weights (431 tokens used fallback weight 21.5481)
2025-05-12 01:37:58 - Created 72 models for all configurations
2025-05-12 01:37:58 - Loss log will be saved to ./model_checkpoints/msmarco_rerankers/logs/all_models_loss_log_20250512_013758.csv
2025-05-12 01:37:58 - Training all models in parallel for 500 steps
2025-05-12 01:37:58 - Starting epoch 1
Epoch 1:   0%|          | 0/31186 [00:00<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:00<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  2025-05-12 01:37:58 - Step 0: Processing batch with 16 examples
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in backward pass for layer_0_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:37:59 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in backward pass for layer_0_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  2025-05-12 01:37:59 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in backward pass for layer_0_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:37:59 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:37:59 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:37:59 - Error computing loss for layer_0_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:01<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_0_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 14: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_0_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_0_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 14: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 14: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 13: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_3_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_3_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 10: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_6_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in backward pass for layer_6_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:00 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:00 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:00 - Error computing loss for layer_9_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 10: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:02<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 10: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 9: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_9_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_9_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 8: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 7: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_12_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_12_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 6: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_15_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_15_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_15_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_15_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 5: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in backward pass for layer_15_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:01 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:01 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:01 - Error computing loss for layer_15_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_15_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 4: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:03<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_15_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_15_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 3: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_15_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_15_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 3: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 1: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_18_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_18_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 0: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_query_only, batch item 15: name 'global_step' is not defined
Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]                                                  Epoch 1:   0%|          | 0/31186 [00:04<?, ?it/s]Epoch 1:   0%|          | 1/31186 [00:04<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:04<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:02 - Error in backward pass for layer_21_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:02 - Error computing loss for layer_21_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:02 - Error in backward pass for layer_21_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:02 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:02 - Step 0: Processing batch with 16 examples
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error computing loss for layer_0_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error computing loss for layer_0_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 13: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_0_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_0_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_3_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_3_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in backward pass for layer_3_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:05 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:05 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:05 - Error computing loss for layer_3_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:07<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 11: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_3_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_3_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 11: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 11: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 10: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_6_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_6_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 8: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:08<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in backward pass for layer_9_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:06 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:06 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:06 - Error computing loss for layer_9_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_9_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_9_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 7: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_12_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_12_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 4: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 3: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_15_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_15_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_18_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_18_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 1: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in backward pass for layer_18_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:07 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:07 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:07 - Error computing loss for layer_18_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:09<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_18_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_18_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_token_full: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_token_none, batch item 15: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Error in backward pass for layer_21_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 14: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]                                                             Epoch 1:   0%|          | 1/31186 [00:10<42:02:33,  4.85s/it]Epoch 1:   0%|          | 2/31186 [00:10<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:10<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:08 - Error computing loss for layer_21_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:08 - Error in backward pass for layer_21_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:08 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:08 - Step 0: Processing batch with 16 examples
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 12: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 12: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 11: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_0_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_0_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 9: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_3_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_3_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in backward pass for layer_6_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:09 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:09 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:09 - Error computing loss for layer_6_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:11<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 7: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_6_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_6_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 6: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_9_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_9_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 4: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 3: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in backward pass for layer_12_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:10 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:12<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:10 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:10 - Error computing loss for layer_12_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_12_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_12_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_12_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 1: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_15_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_15_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_token_full: element 0 of tensors does not require grad and does not have a grad_fn
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_token_none, batch item 15: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error in backward pass for layer_18_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 14: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_18_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_18_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_21_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 12: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_21_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_21_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_21_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in backward pass for layer_21_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:11 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:11 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:11 - Error computing loss for layer_21_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:13<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_21_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_21_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]                                                             Epoch 1:   0%|          | 2/31186 [00:14<46:36:43,  5.38s/it]Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_21_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_21_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_21_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Step 0: Processing batch with 16 examples
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 8: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:14<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:12 - Error computing loss for layer_0_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:12 - Error in backward pass for layer_0_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:12 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_0_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_0_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_0_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_surprise_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 5: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_surprise_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_3_surprise_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_3_surprise_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_token_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_token_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_token_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_token_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_full, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_positional_full: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 4: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_query_only, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_positional_query_only: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 4: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 5: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 6: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 7: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 8: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 9: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 10: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 11: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 12: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 13: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 14: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_positional_none, batch item 15: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in backward pass for layer_6_positional_none: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-12 01:38:13 - Traceback (most recent call last):
  File "/home/ubuntu/a100-storage/projects/semantic-trajectory/parallel_train.py", line 698, in train_all_weighters_parallel
    batch_loss.backward()
  File "/usr/lib/python3/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/lib/python3/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_surprise_full, batch item 0: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_surprise_full, batch item 1: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_surprise_full, batch item 2: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
2025-05-12 01:38:13 - Error computing loss for layer_6_surprise_full, batch item 3: name 'global_step' is not defined
2025-05-12 01:38:13 - Error in loss calculation: name 'global_step' is not defined
Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             Epoch 1:   0%|          | 3/31186 [00:15<39:11:39,  4.52s/it]                                                             